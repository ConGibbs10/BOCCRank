> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.417445+0.001017	test-rmse:0.417447+0.004057 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.417177+0.001015	test-rmse:0.417206+0.004060 
[3]	train-rmse:0.416911+0.001014	test-rmse:0.416964+0.004060 
[4]	train-rmse:0.416646+0.001014	test-rmse:0.416723+0.004055 
[5]	train-rmse:0.416378+0.001013	test-rmse:0.416477+0.004051 
[6]	train-rmse:0.416112+0.001011	test-rmse:0.416235+0.004049 
[7]	train-rmse:0.415845+0.001010	test-rmse:0.415994+0.004046 
[8]	train-rmse:0.415582+0.001007	test-rmse:0.415751+0.004040 
[9]	train-rmse:0.415317+0.001006	test-rmse:0.415515+0.004033 
[10]	train-rmse:0.415053+0.001007	test-rmse:0.415273+0.004032 
[11]	train-rmse:0.414788+0.001007	test-rmse:0.415030+0.004026 
[12]	train-rmse:0.414523+0.001006	test-rmse:0.414788+0.004021 
[13]	train-rmse:0.414260+0.001007	test-rmse:0.414544+0.004019 
[14]	train-rmse:0.413995+0.001007	test-rmse:0.414304+0.004016 
[15]	train-rmse:0.413732+0.001005	test-rmse:0.414065+0.004015 
[16]	train-rmse:0.413467+0.001004	test-rmse:0.413825+0.004014 
[17]	train-rmse:0.413204+0.001001	test-rmse:0.413586+0.004009 
[18]	train-rmse:0.412942+0.000999	test-rmse:0.413344+0.004005 
[19]	train-rmse:0.412678+0.001000	test-rmse:0.413105+0.004002 
[20]	train-rmse:0.412416+0.000998	test-rmse:0.412864+0.003997 
[21]	train-rmse:0.412154+0.000998	test-rmse:0.412628+0.003995 
[22]	train-rmse:0.411892+0.000997	test-rmse:0.412390+0.003990 
[23]	train-rmse:0.411630+0.000998	test-rmse:0.412154+0.003985 
[24]	train-rmse:0.411368+0.001000	test-rmse:0.411918+0.003986 
[25]	train-rmse:0.411109+0.000998	test-rmse:0.411681+0.003987 
[26]	train-rmse:0.410849+0.000998	test-rmse:0.411443+0.003984 
[27]	train-rmse:0.410587+0.000997	test-rmse:0.411208+0.003981 
[28]	train-rmse:0.410326+0.000996	test-rmse:0.410973+0.003975 
[29]	train-rmse:0.410068+0.000994	test-rmse:0.410736+0.003971 
[30]	train-rmse:0.409808+0.000992	test-rmse:0.410495+0.003965 
[31]	train-rmse:0.409547+0.000991	test-rmse:0.410260+0.003963 
[32]	train-rmse:0.409289+0.000989	test-rmse:0.410025+0.003964 
[33]	train-rmse:0.409029+0.000988	test-rmse:0.409791+0.003961 
[34]	train-rmse:0.408767+0.000990	test-rmse:0.409557+0.003957 
[35]	train-rmse:0.408509+0.000989	test-rmse:0.409322+0.003954 
[36]	train-rmse:0.408249+0.000989	test-rmse:0.409091+0.003950 
[37]	train-rmse:0.407991+0.000989	test-rmse:0.408857+0.003950 
[38]	train-rmse:0.407732+0.000989	test-rmse:0.408622+0.003948 
[39]	train-rmse:0.407475+0.000988	test-rmse:0.408389+0.003943 
[40]	train-rmse:0.407216+0.000986	test-rmse:0.408159+0.003939 
[41]	train-rmse:0.406957+0.000985	test-rmse:0.407924+0.003939 
[42]	train-rmse:0.406699+0.000985	test-rmse:0.407695+0.003940 
[43]	train-rmse:0.406443+0.000984	test-rmse:0.407465+0.003940 
[44]	train-rmse:0.406185+0.000984	test-rmse:0.407234+0.003937 
[45]	train-rmse:0.405931+0.000982	test-rmse:0.407001+0.003935 
[46]	train-rmse:0.405676+0.000981	test-rmse:0.406771+0.003932 
[47]	train-rmse:0.405417+0.000977	test-rmse:0.406539+0.003930 
[48]	train-rmse:0.405161+0.000975	test-rmse:0.406309+0.003932 
[49]	train-rmse:0.404906+0.000975	test-rmse:0.406078+0.003927 
[50]	train-rmse:0.404650+0.000974	test-rmse:0.405843+0.003923 
[51]	train-rmse:0.404394+0.000974	test-rmse:0.405612+0.003921 
[52]	train-rmse:0.404141+0.000972	test-rmse:0.405378+0.003921 
[53]	train-rmse:0.403887+0.000971	test-rmse:0.405148+0.003918 
[54]	train-rmse:0.403634+0.000972	test-rmse:0.404921+0.003921 
[55]	train-rmse:0.403404+0.000978	test-rmse:0.404716+0.003913 
[56]	train-rmse:0.403149+0.000977	test-rmse:0.404486+0.003913 
[57]	train-rmse:0.402896+0.000975	test-rmse:0.404256+0.003911 
[58]	train-rmse:0.402641+0.000975	test-rmse:0.404029+0.003910 
[59]	train-rmse:0.402386+0.000975	test-rmse:0.403800+0.003911 
[60]	train-rmse:0.402133+0.000975	test-rmse:0.403573+0.003908 
[61]	train-rmse:0.401879+0.000975	test-rmse:0.403345+0.003905 
[62]	train-rmse:0.401625+0.000975	test-rmse:0.403118+0.003903 
[63]	train-rmse:0.401374+0.000975	test-rmse:0.402892+0.003903 
[64]	train-rmse:0.401119+0.000972	test-rmse:0.402668+0.003899 
[65]	train-rmse:0.400867+0.000972	test-rmse:0.402446+0.003895 
[66]	train-rmse:0.400616+0.000973	test-rmse:0.402217+0.003893 
[67]	train-rmse:0.400364+0.000974	test-rmse:0.401989+0.003893 
[68]	train-rmse:0.400112+0.000973	test-rmse:0.401761+0.003892 
[69]	train-rmse:0.399863+0.000973	test-rmse:0.401534+0.003892 
[70]	train-rmse:0.399662+0.000953	test-rmse:0.401352+0.003912 
[71]	train-rmse:0.399410+0.000953	test-rmse:0.401120+0.003913 
[72]	train-rmse:0.399158+0.000951	test-rmse:0.400895+0.003912 
[73]	train-rmse:0.398934+0.000961	test-rmse:0.400695+0.003905 
[74]	train-rmse:0.398683+0.000961	test-rmse:0.400471+0.003907 
[75]	train-rmse:0.398467+0.000905	test-rmse:0.400279+0.003958 
[76]	train-rmse:0.398217+0.000904	test-rmse:0.400057+0.003959 
[77]	train-rmse:0.397966+0.000902	test-rmse:0.399836+0.003955 
[78]	train-rmse:0.397713+0.000902	test-rmse:0.399607+0.003957 
[79]	train-rmse:0.397461+0.000902	test-rmse:0.399384+0.003953 
[80]	train-rmse:0.397213+0.000902	test-rmse:0.399163+0.003953 
[81]	train-rmse:0.396964+0.000902	test-rmse:0.398943+0.003953 
[82]	train-rmse:0.396714+0.000900	test-rmse:0.398719+0.003949 
[83]	train-rmse:0.396498+0.000935	test-rmse:0.398526+0.003924 
[84]	train-rmse:0.396248+0.000933	test-rmse:0.398304+0.003925 
[85]	train-rmse:0.395999+0.000934	test-rmse:0.398087+0.003924 
[86]	train-rmse:0.395753+0.000933	test-rmse:0.397863+0.003923 
[87]	train-rmse:0.395531+0.000960	test-rmse:0.397660+0.003900 
[88]	train-rmse:0.395284+0.000960	test-rmse:0.397438+0.003897 
[89]	train-rmse:0.395036+0.000959	test-rmse:0.397215+0.003897 
[90]	train-rmse:0.394791+0.000961	test-rmse:0.396995+0.003894 
[91]	train-rmse:0.394543+0.000959	test-rmse:0.396772+0.003894 
[92]	train-rmse:0.394297+0.000958	test-rmse:0.396550+0.003892 
[93]	train-rmse:0.394050+0.000960	test-rmse:0.396328+0.003889 
[94]	train-rmse:0.393828+0.000944	test-rmse:0.396126+0.003895 
[95]	train-rmse:0.393609+0.000932	test-rmse:0.395928+0.003902 
[96]	train-rmse:0.393364+0.000929	test-rmse:0.395708+0.003901 
[97]	train-rmse:0.393119+0.000929	test-rmse:0.395491+0.003899 
[98]	train-rmse:0.392876+0.000928	test-rmse:0.395270+0.003897 
[99]	train-rmse:0.392653+0.000920	test-rmse:0.395069+0.003906 
[100]	train-rmse:0.392408+0.000919	test-rmse:0.394851+0.003907 
[101]	train-rmse:0.392164+0.000919	test-rmse:0.394636+0.003902 
[102]	train-rmse:0.391944+0.000875	test-rmse:0.394439+0.003943 
[103]	train-rmse:0.391701+0.000876	test-rmse:0.394228+0.003941 
[104]	train-rmse:0.391482+0.000903	test-rmse:0.394027+0.003924 
[105]	train-rmse:0.391238+0.000901	test-rmse:0.393807+0.003925 
[106]	train-rmse:0.390995+0.000901	test-rmse:0.393591+0.003924 
[107]	train-rmse:0.390779+0.000892	test-rmse:0.393397+0.003932 
[108]	train-rmse:0.390536+0.000891	test-rmse:0.393179+0.003933 
[109]	train-rmse:0.390292+0.000890	test-rmse:0.392959+0.003934 
[110]	train-rmse:0.390083+0.000929	test-rmse:0.392771+0.003908 
[111]	train-rmse:0.389842+0.000928	test-rmse:0.392552+0.003909 
[112]	train-rmse:0.389600+0.000927	test-rmse:0.392339+0.003911 
[113]	train-rmse:0.389382+0.000957	test-rmse:0.392145+0.003888 
[114]	train-rmse:0.389163+0.000964	test-rmse:0.391953+0.003878 
[115]	train-rmse:0.388952+0.000910	test-rmse:0.391766+0.003927 
[116]	train-rmse:0.388734+0.000918	test-rmse:0.391575+0.003916 
[117]	train-rmse:0.388495+0.000916	test-rmse:0.391364+0.003921 
[118]	train-rmse:0.388253+0.000913	test-rmse:0.391147+0.003927 
[119]	train-rmse:0.388013+0.000910	test-rmse:0.390932+0.003930 
[120]	train-rmse:0.387774+0.000909	test-rmse:0.390719+0.003926 
[121]	train-rmse:0.387569+0.000948	test-rmse:0.390532+0.003903 
[122]	train-rmse:0.387327+0.000947	test-rmse:0.390317+0.003902 
[123]	train-rmse:0.387087+0.000947	test-rmse:0.390103+0.003900 
[124]	train-rmse:0.386872+0.000979	test-rmse:0.389911+0.003881 
[125]	train-rmse:0.386632+0.000978	test-rmse:0.389697+0.003879 
[126]	train-rmse:0.386394+0.000979	test-rmse:0.389484+0.003873 
[127]	train-rmse:0.386154+0.000977	test-rmse:0.389272+0.003874 
[128]	train-rmse:0.385913+0.000976	test-rmse:0.389058+0.003878 
[129]	train-rmse:0.385674+0.000975	test-rmse:0.388846+0.003877 
[130]	train-rmse:0.385459+0.000962	test-rmse:0.388654+0.003884 
[131]	train-rmse:0.385222+0.000961	test-rmse:0.388443+0.003886 
[132]	train-rmse:0.384983+0.000958	test-rmse:0.388233+0.003892 
[133]	train-rmse:0.384745+0.000957	test-rmse:0.388020+0.003895 
[134]	train-rmse:0.384508+0.000958	test-rmse:0.387810+0.003898 
[135]	train-rmse:0.384271+0.000958	test-rmse:0.387596+0.003894 
[136]	train-rmse:0.384056+0.000968	test-rmse:0.387408+0.003888 
[137]	train-rmse:0.383844+0.000976	test-rmse:0.387217+0.003868 
[138]	train-rmse:0.383607+0.000976	test-rmse:0.387007+0.003866 
[139]	train-rmse:0.383369+0.000975	test-rmse:0.386800+0.003869 
[140]	train-rmse:0.383133+0.000974	test-rmse:0.386589+0.003867 
[141]	train-rmse:0.382893+0.000975	test-rmse:0.386377+0.003867 
[142]	train-rmse:0.382657+0.000975	test-rmse:0.386169+0.003870 
[143]	train-rmse:0.382482+0.000988	test-rmse:0.386007+0.003867 
[144]	train-rmse:0.382276+0.000934	test-rmse:0.385826+0.003917 
[145]	train-rmse:0.382062+0.000943	test-rmse:0.385635+0.003897 
[146]	train-rmse:0.381824+0.000943	test-rmse:0.385424+0.003900 
[147]	train-rmse:0.381589+0.000943	test-rmse:0.385212+0.003902 
[148]	train-rmse:0.381355+0.000941	test-rmse:0.385001+0.003903 
[149]	train-rmse:0.381121+0.000940	test-rmse:0.384793+0.003900 
[150]	train-rmse:0.380887+0.000941	test-rmse:0.384581+0.003903 
[151]	train-rmse:0.380675+0.000902	test-rmse:0.384394+0.003939 
[152]	train-rmse:0.380441+0.000898	test-rmse:0.384187+0.003939 
[153]	train-rmse:0.380205+0.000898	test-rmse:0.383980+0.003944 
[154]	train-rmse:0.379971+0.000897	test-rmse:0.383773+0.003943 
[155]	train-rmse:0.379736+0.000897	test-rmse:0.383562+0.003943 
[156]	train-rmse:0.379503+0.000897	test-rmse:0.383355+0.003950 
[157]	train-rmse:0.379300+0.000914	test-rmse:0.383181+0.003934 
[158]	train-rmse:0.379067+0.000915	test-rmse:0.382972+0.003932 
[159]	train-rmse:0.378834+0.000913	test-rmse:0.382767+0.003934 
[160]	train-rmse:0.378602+0.000912	test-rmse:0.382558+0.003937 
[161]	train-rmse:0.378370+0.000914	test-rmse:0.382355+0.003935 
[162]	train-rmse:0.378138+0.000914	test-rmse:0.382157+0.003935 
[163]	train-rmse:0.377907+0.000915	test-rmse:0.381954+0.003935 
[164]	train-rmse:0.377673+0.000914	test-rmse:0.381746+0.003932 
[165]	train-rmse:0.377504+0.000844	test-rmse:0.381596+0.003994 
[166]	train-rmse:0.377294+0.000856	test-rmse:0.381413+0.003973 
[167]	train-rmse:0.377064+0.000856	test-rmse:0.381207+0.003973 
[168]	train-rmse:0.376831+0.000854	test-rmse:0.381005+0.003974 
[169]	train-rmse:0.376623+0.000871	test-rmse:0.380821+0.003963 
[170]	train-rmse:0.376392+0.000869	test-rmse:0.380622+0.003965 
[171]	train-rmse:0.376159+0.000868	test-rmse:0.380415+0.003965 
[172]	train-rmse:0.375929+0.000868	test-rmse:0.380210+0.003962 
[173]	train-rmse:0.375732+0.000851	test-rmse:0.380031+0.003981 
[174]	train-rmse:0.375503+0.000852	test-rmse:0.379830+0.003979 
[175]	train-rmse:0.375296+0.000840	test-rmse:0.379648+0.003991 
[176]	train-rmse:0.375088+0.000830	test-rmse:0.379463+0.004004 
[177]	train-rmse:0.374860+0.000828	test-rmse:0.379261+0.004006 
[178]	train-rmse:0.374661+0.000841	test-rmse:0.379085+0.003978 
[179]	train-rmse:0.374431+0.000841	test-rmse:0.378880+0.003979 
[180]	train-rmse:0.374204+0.000840	test-rmse:0.378679+0.003985 
[181]	train-rmse:0.373975+0.000838	test-rmse:0.378475+0.003984 
[182]	train-rmse:0.373746+0.000837	test-rmse:0.378270+0.003984 
[183]	train-rmse:0.373520+0.000838	test-rmse:0.378067+0.003982 
[184]	train-rmse:0.373293+0.000837	test-rmse:0.377867+0.003984 
[185]	train-rmse:0.373065+0.000836	test-rmse:0.377664+0.003982 
[186]	train-rmse:0.372836+0.000834	test-rmse:0.377466+0.003985 
[187]	train-rmse:0.372642+0.000859	test-rmse:0.377299+0.003968 
[188]	train-rmse:0.372415+0.000858	test-rmse:0.377097+0.003970 
[189]	train-rmse:0.372221+0.000889	test-rmse:0.376930+0.003952 
[190]	train-rmse:0.372018+0.000909	test-rmse:0.376746+0.003936 
[191]	train-rmse:0.371823+0.000924	test-rmse:0.376573+0.003914 
[192]	train-rmse:0.371630+0.000943	test-rmse:0.376404+0.003888 
[193]	train-rmse:0.371433+0.000966	test-rmse:0.376232+0.003862 
[194]	train-rmse:0.371228+0.000955	test-rmse:0.376052+0.003874 
[195]	train-rmse:0.371001+0.000954	test-rmse:0.375852+0.003877 
[196]	train-rmse:0.370804+0.000979	test-rmse:0.375680+0.003863 
[197]	train-rmse:0.370608+0.000966	test-rmse:0.375506+0.003883 
[198]	train-rmse:0.370382+0.000965	test-rmse:0.375310+0.003889 
[199]	train-rmse:0.370157+0.000964	test-rmse:0.375115+0.003887 
[200]	train-rmse:0.369932+0.000962	test-rmse:0.374916+0.003892 
[201]	train-rmse:0.369707+0.000961	test-rmse:0.374714+0.003896 
[202]	train-rmse:0.369515+0.000984	test-rmse:0.374545+0.003869 
[203]	train-rmse:0.369320+0.001008	test-rmse:0.374376+0.003852 
[204]	train-rmse:0.369094+0.001007	test-rmse:0.374176+0.003860 
[205]	train-rmse:0.368921+0.001051	test-rmse:0.374026+0.003824 
[206]	train-rmse:0.368724+0.001082	test-rmse:0.373856+0.003811 
[207]	train-rmse:0.368499+0.001085	test-rmse:0.373656+0.003813 
[208]	train-rmse:0.368321+0.001036	test-rmse:0.373498+0.003862 
[209]	train-rmse:0.368119+0.000999	test-rmse:0.373323+0.003893 
[210]	train-rmse:0.367897+0.000998	test-rmse:0.373126+0.003897 
[211]	train-rmse:0.367674+0.000998	test-rmse:0.372928+0.003900 
[212]	train-rmse:0.367448+0.000997	test-rmse:0.372730+0.003898 
[213]	train-rmse:0.367224+0.000997	test-rmse:0.372531+0.003900 
[214]	train-rmse:0.367002+0.000997	test-rmse:0.372336+0.003905 
[215]	train-rmse:0.366779+0.000998	test-rmse:0.372140+0.003905 
[216]	train-rmse:0.366558+0.001000	test-rmse:0.371944+0.003903 
[217]	train-rmse:0.366371+0.000983	test-rmse:0.371779+0.003923 
[218]	train-rmse:0.366149+0.000983	test-rmse:0.371587+0.003920 
[219]	train-rmse:0.365956+0.000933	test-rmse:0.371419+0.003962 
[220]	train-rmse:0.365756+0.000897	test-rmse:0.371248+0.003992 
[221]	train-rmse:0.365533+0.000898	test-rmse:0.371049+0.003992 
[222]	train-rmse:0.365312+0.000897	test-rmse:0.370859+0.003998 
[223]	train-rmse:0.365121+0.000884	test-rmse:0.370690+0.004021 
[224]	train-rmse:0.364900+0.000886	test-rmse:0.370498+0.004023 
[225]	train-rmse:0.364679+0.000888	test-rmse:0.370305+0.004026 
[226]	train-rmse:0.364483+0.000883	test-rmse:0.370130+0.004037 
[227]	train-rmse:0.364261+0.000881	test-rmse:0.369935+0.004038 
[228]	train-rmse:0.364071+0.000915	test-rmse:0.369771+0.004021 
[229]	train-rmse:0.363873+0.000920	test-rmse:0.369598+0.004007 
[230]	train-rmse:0.363652+0.000919	test-rmse:0.369404+0.004008 
[231]	train-rmse:0.363434+0.000922	test-rmse:0.369212+0.004007 
[232]	train-rmse:0.363215+0.000923	test-rmse:0.369022+0.004012 
[233]	train-rmse:0.362998+0.000926	test-rmse:0.368833+0.004012 
[234]	train-rmse:0.362778+0.000928	test-rmse:0.368642+0.004007 
[235]	train-rmse:0.362557+0.000928	test-rmse:0.368451+0.004011 
[236]	train-rmse:0.362367+0.000936	test-rmse:0.368286+0.003996 
[237]	train-rmse:0.362171+0.000964	test-rmse:0.368113+0.003983 
[238]	train-rmse:0.361954+0.000964	test-rmse:0.367923+0.003987 
[239]	train-rmse:0.361735+0.000964	test-rmse:0.367734+0.003984 
[240]	train-rmse:0.361537+0.000929	test-rmse:0.367565+0.004013 
[241]	train-rmse:0.361351+0.000875	test-rmse:0.367403+0.004059 
[242]	train-rmse:0.361135+0.000877	test-rmse:0.367213+0.004062 
[243]	train-rmse:0.360917+0.000878	test-rmse:0.367023+0.004064 
[244]	train-rmse:0.360698+0.000880	test-rmse:0.366834+0.004064 
[245]	train-rmse:0.360480+0.000883	test-rmse:0.366646+0.004069 
[246]	train-rmse:0.360285+0.000873	test-rmse:0.366476+0.004083 
[247]	train-rmse:0.360098+0.000894	test-rmse:0.366311+0.004058 
[248]	train-rmse:0.359931+0.000916	test-rmse:0.366164+0.004023 
[249]	train-rmse:0.359714+0.000918	test-rmse:0.365979+0.004021 
[250]	train-rmse:0.359498+0.000920	test-rmse:0.365792+0.004023 
[251]	train-rmse:0.359282+0.000919	test-rmse:0.365606+0.004024 
[252]	train-rmse:0.359067+0.000920	test-rmse:0.365417+0.004026 
[253]	train-rmse:0.358922+0.000958	test-rmse:0.365293+0.004008 
[254]	train-rmse:0.358741+0.000903	test-rmse:0.365137+0.004057 
[255]	train-rmse:0.358558+0.000935	test-rmse:0.364983+0.004032 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
