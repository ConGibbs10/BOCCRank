> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.408835+0.000948	test-rmse:0.409334+0.003997 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.400334+0.000939	test-rmse:0.401423+0.003812 
[3]	train-rmse:0.392143+0.000864	test-rmse:0.393789+0.003791 
[4]	train-rmse:0.384161+0.000869	test-rmse:0.386455+0.003754 
[5]	train-rmse:0.376381+0.000904	test-rmse:0.379369+0.003692 
[6]	train-rmse:0.368810+0.000944	test-rmse:0.372538+0.003733 
[7]	train-rmse:0.361546+0.001040	test-rmse:0.365866+0.003904 
[8]	train-rmse:0.354589+0.001207	test-rmse:0.359473+0.003922 
[9]	train-rmse:0.347835+0.001218	test-rmse:0.353362+0.004126 
[10]	train-rmse:0.341285+0.001278	test-rmse:0.347629+0.004233 
[11]	train-rmse:0.334936+0.001354	test-rmse:0.341962+0.004397 
[12]	train-rmse:0.328828+0.001381	test-rmse:0.336482+0.004608 
[13]	train-rmse:0.324108+0.001212	test-rmse:0.332197+0.006098 
[14]	train-rmse:0.319198+0.002363	test-rmse:0.327888+0.007123 
[15]	train-rmse:0.313778+0.002333	test-rmse:0.323058+0.007304 
[16]	train-rmse:0.308292+0.002401	test-rmse:0.318376+0.007356 
[17]	train-rmse:0.303116+0.002404	test-rmse:0.314053+0.007437 
[18]	train-rmse:0.298042+0.002288	test-rmse:0.309634+0.007665 
[19]	train-rmse:0.294856+0.002399	test-rmse:0.306989+0.006503 
[20]	train-rmse:0.290589+0.001911	test-rmse:0.303283+0.007199 
[21]	train-rmse:0.286131+0.001968	test-rmse:0.299471+0.007517 
[22]	train-rmse:0.283253+0.003146	test-rmse:0.297092+0.007317 
[23]	train-rmse:0.279801+0.004006	test-rmse:0.294252+0.007584 
[24]	train-rmse:0.276432+0.004220	test-rmse:0.291400+0.008590 
[25]	train-rmse:0.272771+0.004414	test-rmse:0.288429+0.008517 
[26]	train-rmse:0.268778+0.004343	test-rmse:0.285075+0.008501 
[27]	train-rmse:0.264797+0.004229	test-rmse:0.281803+0.008673 
[28]	train-rmse:0.261030+0.004259	test-rmse:0.278837+0.008898 
[29]	train-rmse:0.258278+0.005078	test-rmse:0.276730+0.008610 
[30]	train-rmse:0.255320+0.005665	test-rmse:0.274257+0.009245 
[31]	train-rmse:0.251899+0.005507	test-rmse:0.271573+0.009194 
[32]	train-rmse:0.248962+0.004999	test-rmse:0.269261+0.009499 
[33]	train-rmse:0.246573+0.004718	test-rmse:0.267456+0.009589 
[34]	train-rmse:0.243936+0.004784	test-rmse:0.265484+0.009502 
[35]	train-rmse:0.241253+0.004149	test-rmse:0.263462+0.009762 
[36]	train-rmse:0.239365+0.003539	test-rmse:0.261969+0.009826 
[37]	train-rmse:0.236810+0.003903	test-rmse:0.260109+0.009600 
[38]	train-rmse:0.234362+0.004232	test-rmse:0.258207+0.009897 
[39]	train-rmse:0.232729+0.004617	test-rmse:0.257080+0.009890 
[40]	train-rmse:0.231073+0.004985	test-rmse:0.255843+0.010413 
[41]	train-rmse:0.228806+0.005181	test-rmse:0.254178+0.010318 
[42]	train-rmse:0.226581+0.004816	test-rmse:0.252545+0.010542 
[43]	train-rmse:0.224390+0.004903	test-rmse:0.250955+0.010550 
[44]	train-rmse:0.223003+0.005062	test-rmse:0.250037+0.010463 
[45]	train-rmse:0.221705+0.005263	test-rmse:0.249138+0.010553 
[46]	train-rmse:0.220395+0.005282	test-rmse:0.248146+0.010870 
[47]	train-rmse:0.218345+0.005537	test-rmse:0.246826+0.010739 
[48]	train-rmse:0.216556+0.005892	test-rmse:0.245524+0.011161 
[49]	train-rmse:0.215299+0.006170	test-rmse:0.244708+0.011235 
[50]	train-rmse:0.214145+0.006115	test-rmse:0.243818+0.011568 
[51]	train-rmse:0.212657+0.006312	test-rmse:0.242846+0.011930 
[52]	train-rmse:0.211505+0.006152	test-rmse:0.242054+0.011673 
[53]	train-rmse:0.210158+0.005566	test-rmse:0.241056+0.012068 
[54]	train-rmse:0.209018+0.005480	test-rmse:0.240377+0.011972 
[55]	train-rmse:0.207713+0.004858	test-rmse:0.239473+0.012341 
[56]	train-rmse:0.206890+0.005023	test-rmse:0.238950+0.012363 
[57]	train-rmse:0.205555+0.004968	test-rmse:0.238084+0.012475 
[58]	train-rmse:0.204322+0.005006	test-rmse:0.237357+0.012349 
[59]	train-rmse:0.203002+0.005159	test-rmse:0.236501+0.012549 
[60]	train-rmse:0.201957+0.005104	test-rmse:0.235908+0.012439 
[61]	train-rmse:0.201116+0.004516	test-rmse:0.235338+0.012710 
[62]	train-rmse:0.199881+0.004405	test-rmse:0.234601+0.012854 
[63]	train-rmse:0.198955+0.004226	test-rmse:0.233988+0.013288 
[64]	train-rmse:0.198050+0.004160	test-rmse:0.233461+0.013190 
[65]	train-rmse:0.197357+0.004067	test-rmse:0.232937+0.013522 
[66]	train-rmse:0.196420+0.004025	test-rmse:0.232434+0.013716 
[67]	train-rmse:0.195452+0.004071	test-rmse:0.231819+0.013918 
[68]	train-rmse:0.195185+0.004166	test-rmse:0.231666+0.013913 
[69]	train-rmse:0.194567+0.004221	test-rmse:0.231327+0.013909 
[70]	train-rmse:0.193677+0.004160	test-rmse:0.230878+0.013961 
[71]	train-rmse:0.193038+0.004132	test-rmse:0.230445+0.014052 
[72]	train-rmse:0.192575+0.003950	test-rmse:0.230154+0.014129 
[73]	train-rmse:0.192175+0.003947	test-rmse:0.229952+0.014073 
[74]	train-rmse:0.191418+0.003606	test-rmse:0.229528+0.014232 
[75]	train-rmse:0.190964+0.003370	test-rmse:0.229214+0.014240 
[76]	train-rmse:0.190422+0.003554	test-rmse:0.228920+0.014270 
[77]	train-rmse:0.189539+0.003907	test-rmse:0.228440+0.014153 
[78]	train-rmse:0.189069+0.004152	test-rmse:0.228196+0.013992 
[79]	train-rmse:0.188366+0.004207	test-rmse:0.227896+0.013928 
[80]	train-rmse:0.187780+0.004627	test-rmse:0.227713+0.013863 
[81]	train-rmse:0.186911+0.004642	test-rmse:0.227138+0.014071 
[82]	train-rmse:0.186289+0.004751	test-rmse:0.226835+0.014071 
[83]	train-rmse:0.185651+0.005033	test-rmse:0.226530+0.013959 
[84]	train-rmse:0.185169+0.005178	test-rmse:0.226321+0.013860 
[85]	train-rmse:0.184549+0.004831	test-rmse:0.226018+0.014043 
[86]	train-rmse:0.183621+0.004966	test-rmse:0.225555+0.014049 
[87]	train-rmse:0.183329+0.004963	test-rmse:0.225412+0.014052 
[88]	train-rmse:0.182451+0.004654	test-rmse:0.225039+0.014196 
[89]	train-rmse:0.181759+0.004573	test-rmse:0.224748+0.014348 
[90]	train-rmse:0.180833+0.004334	test-rmse:0.224306+0.014528 
[91]	train-rmse:0.180331+0.004425	test-rmse:0.224066+0.014468 
[92]	train-rmse:0.179504+0.004246	test-rmse:0.223622+0.014576 
[93]	train-rmse:0.179043+0.004406	test-rmse:0.223394+0.014469 
[94]	train-rmse:0.178565+0.004496	test-rmse:0.223227+0.014514 
[95]	train-rmse:0.178333+0.004484	test-rmse:0.223144+0.014528 
[96]	train-rmse:0.177881+0.004774	test-rmse:0.222851+0.014394 
[97]	train-rmse:0.177285+0.004965	test-rmse:0.222582+0.014407 
[98]	train-rmse:0.176887+0.005112	test-rmse:0.222443+0.014434 
[99]	train-rmse:0.176511+0.004935	test-rmse:0.222283+0.014584 
[100]	train-rmse:0.176213+0.004770	test-rmse:0.222135+0.014724 
[101]	train-rmse:0.175858+0.004766	test-rmse:0.222000+0.014809 
[102]	train-rmse:0.175621+0.004699	test-rmse:0.221926+0.014826 
[103]	train-rmse:0.175335+0.004552	test-rmse:0.221803+0.014838 
[104]	train-rmse:0.174980+0.004442	test-rmse:0.221665+0.014986 
[105]	train-rmse:0.174449+0.004555	test-rmse:0.221457+0.014966 
[106]	train-rmse:0.173928+0.004538	test-rmse:0.221201+0.014958 
[107]	train-rmse:0.173602+0.004709	test-rmse:0.221051+0.014920 
[108]	train-rmse:0.173136+0.004797	test-rmse:0.220871+0.014849 
[109]	train-rmse:0.172698+0.004805	test-rmse:0.220760+0.014827 
[110]	train-rmse:0.172213+0.004974	test-rmse:0.220585+0.014862 
[111]	train-rmse:0.172076+0.004988	test-rmse:0.220526+0.014869 
[112]	train-rmse:0.171626+0.005090	test-rmse:0.220409+0.014970 
[113]	train-rmse:0.171224+0.004891	test-rmse:0.220232+0.015065 
[114]	train-rmse:0.170864+0.004941	test-rmse:0.220067+0.015014 
[115]	train-rmse:0.170406+0.004858	test-rmse:0.219895+0.015161 
[116]	train-rmse:0.170100+0.004928	test-rmse:0.219739+0.015205 
[117]	train-rmse:0.170011+0.004923	test-rmse:0.219699+0.015205 
[118]	train-rmse:0.169829+0.004874	test-rmse:0.219643+0.015229 
[119]	train-rmse:0.169495+0.004928	test-rmse:0.219522+0.015290 
[120]	train-rmse:0.169363+0.004922	test-rmse:0.219480+0.015297 
[121]	train-rmse:0.169129+0.005088	test-rmse:0.219387+0.015254 
[122]	train-rmse:0.168899+0.005288	test-rmse:0.219363+0.015298 
[123]	train-rmse:0.168715+0.005315	test-rmse:0.219310+0.015293 
[124]	train-rmse:0.168553+0.005343	test-rmse:0.219279+0.015298 
[125]	train-rmse:0.168196+0.005353	test-rmse:0.219196+0.015288 
[126]	train-rmse:0.167812+0.005350	test-rmse:0.218992+0.015445 
[127]	train-rmse:0.167652+0.005319	test-rmse:0.218920+0.015472 
[128]	train-rmse:0.167465+0.005266	test-rmse:0.218846+0.015461 
[129]	train-rmse:0.167107+0.005248	test-rmse:0.218744+0.015438 
[130]	train-rmse:0.166886+0.005172	test-rmse:0.218652+0.015531 
[131]	train-rmse:0.166782+0.005157	test-rmse:0.218614+0.015556 
[132]	train-rmse:0.166470+0.005310	test-rmse:0.218514+0.015576 
[133]	train-rmse:0.166224+0.005183	test-rmse:0.218439+0.015570 
[134]	train-rmse:0.165841+0.005126	test-rmse:0.218354+0.015490 
[135]	train-rmse:0.165705+0.005103	test-rmse:0.218333+0.015520 
[136]	train-rmse:0.165518+0.005258	test-rmse:0.218297+0.015505 
[137]	train-rmse:0.165412+0.005232	test-rmse:0.218276+0.015521 
[138]	train-rmse:0.165162+0.005166	test-rmse:0.218217+0.015572 
[139]	train-rmse:0.164977+0.005079	test-rmse:0.218178+0.015629 
[140]	train-rmse:0.164732+0.004966	test-rmse:0.218116+0.015672 
[141]	train-rmse:0.164615+0.004999	test-rmse:0.218085+0.015676 
[142]	train-rmse:0.164363+0.004945	test-rmse:0.217983+0.015721 
[143]	train-rmse:0.164242+0.004943	test-rmse:0.217941+0.015733 
[144]	train-rmse:0.164020+0.004878	test-rmse:0.217856+0.015824 
[145]	train-rmse:0.163812+0.004817	test-rmse:0.217765+0.015948 
[146]	train-rmse:0.163717+0.004804	test-rmse:0.217750+0.015963 
[147]	train-rmse:0.163654+0.004807	test-rmse:0.217725+0.015961 
[148]	train-rmse:0.163377+0.005003	test-rmse:0.217707+0.015963 
[149]	train-rmse:0.163202+0.004940	test-rmse:0.217680+0.015987 
[150]	train-rmse:0.163132+0.004926	test-rmse:0.217664+0.015993 
[151]	train-rmse:0.162939+0.005062	test-rmse:0.217646+0.016003 
[152]	train-rmse:0.162839+0.005046	test-rmse:0.217639+0.016007 
[153]	train-rmse:0.162734+0.005042	test-rmse:0.217611+0.016005 
[154]	train-rmse:0.162483+0.005058	test-rmse:0.217553+0.016049 
[155]	train-rmse:0.162165+0.005049	test-rmse:0.217451+0.016001 
[156]	train-rmse:0.161809+0.005233	test-rmse:0.217373+0.015953 
[157]	train-rmse:0.161640+0.005172	test-rmse:0.217373+0.015940 
[158]	train-rmse:0.161245+0.005126	test-rmse:0.217273+0.016027 
[159]	train-rmse:0.161054+0.005235	test-rmse:0.217253+0.016033 
[160]	train-rmse:0.160826+0.005354	test-rmse:0.217279+0.016139 
[161]	train-rmse:0.160665+0.005298	test-rmse:0.217189+0.016264 
[162]	train-rmse:0.160588+0.005292	test-rmse:0.217189+0.016272 
[163]	train-rmse:0.160514+0.005303	test-rmse:0.217172+0.016267 
[164]	train-rmse:0.160241+0.005077	test-rmse:0.217106+0.016262 
[165]	train-rmse:0.159772+0.005117	test-rmse:0.217046+0.016269 
[166]	train-rmse:0.159700+0.005132	test-rmse:0.217034+0.016285 
[167]	train-rmse:0.159484+0.004949	test-rmse:0.216969+0.016298 
[168]	train-rmse:0.159330+0.004867	test-rmse:0.216959+0.016295 
[169]	train-rmse:0.159164+0.004995	test-rmse:0.216920+0.016261 
[170]	train-rmse:0.158978+0.005170	test-rmse:0.216903+0.016265 
[171]	train-rmse:0.158911+0.005170	test-rmse:0.216888+0.016285 
[172]	train-rmse:0.158666+0.005016	test-rmse:0.216779+0.016288 
[173]	train-rmse:0.158386+0.005058	test-rmse:0.216740+0.016329 
[174]	train-rmse:0.158158+0.005081	test-rmse:0.216683+0.016274 
[175]	train-rmse:0.157978+0.004999	test-rmse:0.216627+0.016270 
[176]	train-rmse:0.157897+0.005017	test-rmse:0.216600+0.016277 
[177]	train-rmse:0.157769+0.005143	test-rmse:0.216614+0.016314 
[178]	train-rmse:0.157614+0.005287	test-rmse:0.216570+0.016305 
[179]	train-rmse:0.157360+0.005283	test-rmse:0.216507+0.016328 
[180]	train-rmse:0.157298+0.005284	test-rmse:0.216490+0.016334 
[181]	train-rmse:0.157073+0.005176	test-rmse:0.216445+0.016376 
[182]	train-rmse:0.156721+0.005116	test-rmse:0.216468+0.016437 
[183]	train-rmse:0.156553+0.005046	test-rmse:0.216471+0.016425 
[184]	train-rmse:0.156407+0.005044	test-rmse:0.216472+0.016453 
[185]	train-rmse:0.156284+0.004977	test-rmse:0.216448+0.016468 
[186]	train-rmse:0.156146+0.005133	test-rmse:0.216419+0.016440 
[187]	train-rmse:0.155935+0.005118	test-rmse:0.216365+0.016518 
[188]	train-rmse:0.155777+0.005222	test-rmse:0.216364+0.016530 
[189]	train-rmse:0.155583+0.005283	test-rmse:0.216387+0.016567 
[190]	train-rmse:0.155384+0.005455	test-rmse:0.216385+0.016574 
[191]	train-rmse:0.155224+0.005419	test-rmse:0.216381+0.016584 
[192]	train-rmse:0.155101+0.005364	test-rmse:0.216342+0.016579 
[193]	train-rmse:0.154886+0.005537	test-rmse:0.216321+0.016587 
[194]	train-rmse:0.154769+0.005450	test-rmse:0.216280+0.016582 
[195]	train-rmse:0.154630+0.005400	test-rmse:0.216248+0.016601 
[196]	train-rmse:0.154475+0.005341	test-rmse:0.216179+0.016650 
[197]	train-rmse:0.154311+0.005408	test-rmse:0.216104+0.016599 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
