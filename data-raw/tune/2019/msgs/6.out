> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.410914+0.000972	test-rmse:0.411415+0.004023 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.404295+0.001036	test-rmse:0.405343+0.003840 
[3]	train-rmse:0.397817+0.001067	test-rmse:0.399335+0.003866 
[4]	train-rmse:0.391487+0.001103	test-rmse:0.393659+0.003851 
[5]	train-rmse:0.385319+0.001152	test-rmse:0.387964+0.003684 
[6]	train-rmse:0.379239+0.001080	test-rmse:0.382440+0.003785 
[7]	train-rmse:0.373340+0.001140	test-rmse:0.377191+0.003795 
[8]	train-rmse:0.367605+0.001117	test-rmse:0.372122+0.003814 
[9]	train-rmse:0.361943+0.001076	test-rmse:0.367120+0.003917 
[10]	train-rmse:0.356472+0.001103	test-rmse:0.362299+0.003969 
[11]	train-rmse:0.351084+0.001086	test-rmse:0.357639+0.003970 
[12]	train-rmse:0.345872+0.001065	test-rmse:0.353009+0.004038 
[13]	train-rmse:0.340754+0.001044	test-rmse:0.348463+0.004172 
[14]	train-rmse:0.335680+0.001030	test-rmse:0.344124+0.004214 
[15]	train-rmse:0.330853+0.001024	test-rmse:0.339998+0.004370 
[16]	train-rmse:0.326085+0.001029	test-rmse:0.335864+0.004436 
[17]	train-rmse:0.321393+0.001049	test-rmse:0.331810+0.004722 
[18]	train-rmse:0.316885+0.001141	test-rmse:0.327954+0.004944 
[19]	train-rmse:0.312499+0.001163	test-rmse:0.324263+0.005117 
[20]	train-rmse:0.308065+0.001246	test-rmse:0.320522+0.005292 
[21]	train-rmse:0.303798+0.001254	test-rmse:0.316983+0.005453 
[22]	train-rmse:0.299692+0.001314	test-rmse:0.313701+0.005657 
[23]	train-rmse:0.295634+0.001330	test-rmse:0.310446+0.005892 
[24]	train-rmse:0.291708+0.001372	test-rmse:0.307237+0.006110 
[25]	train-rmse:0.287804+0.001338	test-rmse:0.304008+0.006281 
[26]	train-rmse:0.283985+0.001342	test-rmse:0.300823+0.006460 
[27]	train-rmse:0.280301+0.001441	test-rmse:0.297894+0.006534 
[28]	train-rmse:0.276727+0.001481	test-rmse:0.295009+0.006737 
[29]	train-rmse:0.273226+0.001574	test-rmse:0.292254+0.006980 
[30]	train-rmse:0.269819+0.001655	test-rmse:0.289574+0.007250 
[31]	train-rmse:0.266433+0.001656	test-rmse:0.286954+0.007485 
[32]	train-rmse:0.263110+0.001675	test-rmse:0.284331+0.007695 
[33]	train-rmse:0.259933+0.001706	test-rmse:0.281945+0.007931 
[34]	train-rmse:0.256771+0.001738	test-rmse:0.279668+0.008183 
[35]	train-rmse:0.253665+0.001737	test-rmse:0.277386+0.008395 
[36]	train-rmse:0.250645+0.001746	test-rmse:0.275111+0.008689 
[37]	train-rmse:0.247727+0.001730	test-rmse:0.272937+0.008877 
[38]	train-rmse:0.244900+0.001708	test-rmse:0.270852+0.009103 
[39]	train-rmse:0.242066+0.001736	test-rmse:0.268889+0.009382 
[40]	train-rmse:0.239327+0.001742	test-rmse:0.266975+0.009563 
[41]	train-rmse:0.236692+0.001836	test-rmse:0.265075+0.009689 
[42]	train-rmse:0.234080+0.001837	test-rmse:0.263402+0.009887 
[43]	train-rmse:0.231559+0.001903	test-rmse:0.261733+0.009969 
[44]	train-rmse:0.229041+0.001936	test-rmse:0.260056+0.010125 
[45]	train-rmse:0.226645+0.001978	test-rmse:0.258438+0.010191 
[46]	train-rmse:0.224263+0.002010	test-rmse:0.256886+0.010300 
[47]	train-rmse:0.222408+0.002401	test-rmse:0.255641+0.010247 
[48]	train-rmse:0.220141+0.002385	test-rmse:0.254110+0.010433 
[49]	train-rmse:0.217923+0.002334	test-rmse:0.252738+0.010610 
[50]	train-rmse:0.215789+0.002326	test-rmse:0.251418+0.010811 
[51]	train-rmse:0.213681+0.002338	test-rmse:0.250083+0.010972 
[52]	train-rmse:0.211622+0.002302	test-rmse:0.248751+0.011215 
[53]	train-rmse:0.209582+0.002300	test-rmse:0.247377+0.011349 
[54]	train-rmse:0.207593+0.002329	test-rmse:0.246228+0.011529 
[55]	train-rmse:0.205650+0.002348	test-rmse:0.245071+0.011579 
[56]	train-rmse:0.203736+0.002332	test-rmse:0.243989+0.011719 
[57]	train-rmse:0.201884+0.002283	test-rmse:0.242974+0.011918 
[58]	train-rmse:0.200106+0.002277	test-rmse:0.241948+0.012037 
[59]	train-rmse:0.198670+0.001900	test-rmse:0.241098+0.012281 
[60]	train-rmse:0.196936+0.001917	test-rmse:0.240075+0.012341 
[61]	train-rmse:0.195273+0.001909	test-rmse:0.239193+0.012537 
[62]	train-rmse:0.193604+0.001904	test-rmse:0.238418+0.012582 
[63]	train-rmse:0.191975+0.001946	test-rmse:0.237540+0.012777 
[64]	train-rmse:0.190335+0.001931	test-rmse:0.236741+0.012931 
[65]	train-rmse:0.188775+0.001930	test-rmse:0.235919+0.013000 
[66]	train-rmse:0.187210+0.001871	test-rmse:0.235182+0.013054 
[67]	train-rmse:0.185688+0.001862	test-rmse:0.234423+0.013184 
[68]	train-rmse:0.184492+0.001723	test-rmse:0.233834+0.013300 
[69]	train-rmse:0.183038+0.001739	test-rmse:0.233042+0.013342 
[70]	train-rmse:0.181634+0.001736	test-rmse:0.232295+0.013300 
[71]	train-rmse:0.180284+0.001778	test-rmse:0.231631+0.013362 
[72]	train-rmse:0.179162+0.001787	test-rmse:0.231040+0.013598 
[73]	train-rmse:0.177856+0.001754	test-rmse:0.230386+0.013744 
[74]	train-rmse:0.176506+0.001757	test-rmse:0.229739+0.013732 
[75]	train-rmse:0.175174+0.001741	test-rmse:0.229160+0.013759 
[76]	train-rmse:0.173954+0.001704	test-rmse:0.228625+0.013772 
[77]	train-rmse:0.172738+0.001701	test-rmse:0.228101+0.013951 
[78]	train-rmse:0.171823+0.001288	test-rmse:0.227680+0.014111 
[79]	train-rmse:0.170562+0.001312	test-rmse:0.227192+0.014207 
[80]	train-rmse:0.169426+0.001366	test-rmse:0.226716+0.014251 
[81]	train-rmse:0.168294+0.001386	test-rmse:0.226346+0.014309 
[82]	train-rmse:0.167391+0.001697	test-rmse:0.225890+0.014178 
[83]	train-rmse:0.166262+0.001744	test-rmse:0.225393+0.014197 
[84]	train-rmse:0.165195+0.001743	test-rmse:0.224973+0.014212 
[85]	train-rmse:0.164150+0.001724	test-rmse:0.224505+0.014288 
[86]	train-rmse:0.163053+0.001736	test-rmse:0.224054+0.014331 
[87]	train-rmse:0.161993+0.001742	test-rmse:0.223719+0.014422 
[88]	train-rmse:0.160972+0.001724	test-rmse:0.223459+0.014423 
[89]	train-rmse:0.159944+0.001691	test-rmse:0.223026+0.014527 
[90]	train-rmse:0.158946+0.001705	test-rmse:0.222751+0.014629 
[91]	train-rmse:0.157934+0.001714	test-rmse:0.222470+0.014625 
[92]	train-rmse:0.156985+0.001681	test-rmse:0.222240+0.014685 
[93]	train-rmse:0.156087+0.001701	test-rmse:0.222021+0.014736 
[94]	train-rmse:0.155333+0.001831	test-rmse:0.221804+0.014727 
[95]	train-rmse:0.154398+0.001824	test-rmse:0.221485+0.014645 
[96]	train-rmse:0.153589+0.001809	test-rmse:0.221256+0.014697 
[97]	train-rmse:0.152698+0.001826	test-rmse:0.220934+0.014728 
[98]	train-rmse:0.151786+0.001784	test-rmse:0.220755+0.014732 
[99]	train-rmse:0.150902+0.001747	test-rmse:0.220511+0.014804 
[100]	train-rmse:0.150204+0.001539	test-rmse:0.220344+0.014905 
[101]	train-rmse:0.149383+0.001530	test-rmse:0.220062+0.014967 
[102]	train-rmse:0.148580+0.001508	test-rmse:0.219829+0.015100 
[103]	train-rmse:0.147781+0.001538	test-rmse:0.219633+0.015179 
[104]	train-rmse:0.147027+0.001570	test-rmse:0.219414+0.015214 
[105]	train-rmse:0.146210+0.001564	test-rmse:0.219276+0.015253 
[106]	train-rmse:0.145410+0.001543	test-rmse:0.219082+0.015340 
[107]	train-rmse:0.144609+0.001567	test-rmse:0.218954+0.015388 
[108]	train-rmse:0.143870+0.001558	test-rmse:0.218792+0.015472 
[109]	train-rmse:0.143105+0.001596	test-rmse:0.218677+0.015565 
[110]	train-rmse:0.142398+0.001620	test-rmse:0.218445+0.015643 
[111]	train-rmse:0.141689+0.001696	test-rmse:0.218347+0.015664 
[112]	train-rmse:0.141008+0.001734	test-rmse:0.218278+0.015683 
[113]	train-rmse:0.140302+0.001744	test-rmse:0.218116+0.015675 
[114]	train-rmse:0.139624+0.001767	test-rmse:0.217976+0.015694 
[115]	train-rmse:0.138894+0.001748	test-rmse:0.217937+0.015687 
[116]	train-rmse:0.138276+0.001731	test-rmse:0.217855+0.015728 
[117]	train-rmse:0.137655+0.001728	test-rmse:0.217649+0.015736 
[118]	train-rmse:0.137010+0.001713	test-rmse:0.217495+0.015753 
[119]	train-rmse:0.136375+0.001742	test-rmse:0.217384+0.015793 
[120]	train-rmse:0.135757+0.001769	test-rmse:0.217256+0.015828 
[121]	train-rmse:0.135107+0.001778	test-rmse:0.217148+0.015817 
[122]	train-rmse:0.134646+0.001627	test-rmse:0.217044+0.015900 
[123]	train-rmse:0.134006+0.001608	test-rmse:0.216916+0.015905 
[124]	train-rmse:0.133384+0.001626	test-rmse:0.216861+0.016007 
[125]	train-rmse:0.132776+0.001652	test-rmse:0.216741+0.016030 
[126]	train-rmse:0.132191+0.001624	test-rmse:0.216652+0.016030 
[127]	train-rmse:0.131528+0.001593	test-rmse:0.216572+0.016045 
[128]	train-rmse:0.130917+0.001583	test-rmse:0.216535+0.016100 
[129]	train-rmse:0.130349+0.001606	test-rmse:0.216444+0.016105 
[130]	train-rmse:0.129771+0.001630	test-rmse:0.216388+0.016100 
[131]	train-rmse:0.129181+0.001624	test-rmse:0.216295+0.016157 
[132]	train-rmse:0.128643+0.001642	test-rmse:0.216245+0.016130 
[133]	train-rmse:0.128083+0.001641	test-rmse:0.216066+0.016197 
[134]	train-rmse:0.127513+0.001650	test-rmse:0.215968+0.016211 
[135]	train-rmse:0.126999+0.001694	test-rmse:0.215897+0.016208 
[136]	train-rmse:0.126414+0.001666	test-rmse:0.215837+0.016189 
[137]	train-rmse:0.125917+0.001675	test-rmse:0.215804+0.016226 
[138]	train-rmse:0.125399+0.001662	test-rmse:0.215713+0.016276 
[139]	train-rmse:0.124856+0.001689	test-rmse:0.215620+0.016297 
[140]	train-rmse:0.124360+0.001728	test-rmse:0.215524+0.016272 
[141]	train-rmse:0.123861+0.001718	test-rmse:0.215471+0.016300 
[142]	train-rmse:0.123336+0.001736	test-rmse:0.215288+0.016273 
[143]	train-rmse:0.122833+0.001762	test-rmse:0.215228+0.016281 
[144]	train-rmse:0.122348+0.001778	test-rmse:0.215209+0.016344 
[145]	train-rmse:0.121848+0.001782	test-rmse:0.215120+0.016431 
[146]	train-rmse:0.121401+0.001841	test-rmse:0.214991+0.016487 
[147]	train-rmse:0.120918+0.001869	test-rmse:0.214982+0.016463 
[148]	train-rmse:0.120436+0.001879	test-rmse:0.214920+0.016466 
[149]	train-rmse:0.119994+0.001901	test-rmse:0.214929+0.016540 
[150]	train-rmse:0.119560+0.001866	test-rmse:0.214820+0.016536 
[151]	train-rmse:0.119056+0.001866	test-rmse:0.214754+0.016540 
[152]	train-rmse:0.118587+0.001893	test-rmse:0.214658+0.016547 
[153]	train-rmse:0.118148+0.001861	test-rmse:0.214595+0.016510 
[154]	train-rmse:0.117705+0.001836	test-rmse:0.214494+0.016465 
[155]	train-rmse:0.117281+0.001750	test-rmse:0.214458+0.016430 
[156]	train-rmse:0.116832+0.001721	test-rmse:0.214374+0.016485 
[157]	train-rmse:0.116469+0.001582	test-rmse:0.214363+0.016489 
[158]	train-rmse:0.116014+0.001576	test-rmse:0.214268+0.016492 
[159]	train-rmse:0.115562+0.001578	test-rmse:0.214279+0.016550 
[160]	train-rmse:0.115152+0.001612	test-rmse:0.214280+0.016553 
[161]	train-rmse:0.114739+0.001627	test-rmse:0.214259+0.016575 
[162]	train-rmse:0.114306+0.001611	test-rmse:0.214303+0.016605 
[163]	train-rmse:0.113870+0.001558	test-rmse:0.214271+0.016660 
[164]	train-rmse:0.113442+0.001527	test-rmse:0.214298+0.016681 
[165]	train-rmse:0.113041+0.001515	test-rmse:0.214256+0.016700 
[166]	train-rmse:0.112631+0.001556	test-rmse:0.214241+0.016681 
[167]	train-rmse:0.112215+0.001528	test-rmse:0.214218+0.016654 
[168]	train-rmse:0.111802+0.001549	test-rmse:0.214183+0.016699 
[169]	train-rmse:0.111412+0.001579	test-rmse:0.214213+0.016786 
[170]	train-rmse:0.111116+0.001550	test-rmse:0.214220+0.016765 
[171]	train-rmse:0.110719+0.001550	test-rmse:0.214250+0.016826 
[172]	train-rmse:0.110325+0.001547	test-rmse:0.214185+0.016801 
[173]	train-rmse:0.109893+0.001562	test-rmse:0.214155+0.016825 
[174]	train-rmse:0.109497+0.001562	test-rmse:0.214195+0.016814 
[175]	train-rmse:0.109000+0.001538	test-rmse:0.214078+0.016815 
[176]	train-rmse:0.108642+0.001541	test-rmse:0.214050+0.016793 
[177]	train-rmse:0.108347+0.001622	test-rmse:0.214040+0.016756 
[178]	train-rmse:0.107973+0.001667	test-rmse:0.214079+0.016796 
[179]	train-rmse:0.107556+0.001668	test-rmse:0.214107+0.016861 
[180]	train-rmse:0.107185+0.001687	test-rmse:0.214099+0.016849 
[181]	train-rmse:0.106894+0.001799	test-rmse:0.214133+0.016840 
[182]	train-rmse:0.106569+0.001884	test-rmse:0.214168+0.016821 
[183]	train-rmse:0.106188+0.001854	test-rmse:0.214167+0.016810 
[184]	train-rmse:0.105814+0.001848	test-rmse:0.214186+0.016795 
[185]	train-rmse:0.105477+0.001839	test-rmse:0.214205+0.016815 
[186]	train-rmse:0.105173+0.001951	test-rmse:0.214217+0.016834 
[187]	train-rmse:0.104805+0.001964	test-rmse:0.214205+0.016822 
[188]	train-rmse:0.104401+0.001966	test-rmse:0.214217+0.016838 
[189]	train-rmse:0.104060+0.001924	test-rmse:0.214210+0.016846 
[190]	train-rmse:0.103691+0.001902	test-rmse:0.214192+0.016866 
[191]	train-rmse:0.103392+0.001900	test-rmse:0.214191+0.016838 
[192]	train-rmse:0.103012+0.001901	test-rmse:0.214170+0.016847 
[193]	train-rmse:0.102748+0.001809	test-rmse:0.214173+0.016800 
[194]	train-rmse:0.102342+0.001783	test-rmse:0.214239+0.016802 
[195]	train-rmse:0.102035+0.001775	test-rmse:0.214236+0.016787 
[196]	train-rmse:0.101639+0.001739	test-rmse:0.214202+0.016786 
[197]	train-rmse:0.101278+0.001750	test-rmse:0.214204+0.016810 
[198]	train-rmse:0.100907+0.001768	test-rmse:0.214136+0.016882 
[199]	train-rmse:0.100606+0.001767	test-rmse:0.214114+0.016881 
[200]	train-rmse:0.100251+0.001729	test-rmse:0.214043+0.016852 
[201]	train-rmse:0.099901+0.001670	test-rmse:0.213988+0.016887 
[202]	train-rmse:0.099572+0.001663	test-rmse:0.213991+0.016860 
[203]	train-rmse:0.099249+0.001670	test-rmse:0.213919+0.016851 
[204]	train-rmse:0.098894+0.001699	test-rmse:0.213922+0.016929 
[205]	train-rmse:0.098528+0.001707	test-rmse:0.213998+0.016940 
[206]	train-rmse:0.098214+0.001705	test-rmse:0.213948+0.016989 
[207]	train-rmse:0.097936+0.001681	test-rmse:0.213944+0.016884 
[208]	train-rmse:0.097581+0.001656	test-rmse:0.213962+0.016937 
[209]	train-rmse:0.097359+0.001683	test-rmse:0.213945+0.016888 
[210]	train-rmse:0.097062+0.001638	test-rmse:0.213931+0.016873 
[211]	train-rmse:0.096740+0.001628	test-rmse:0.213954+0.016887 
[212]	train-rmse:0.096433+0.001649	test-rmse:0.213988+0.016964 
[213]	train-rmse:0.096079+0.001665	test-rmse:0.213984+0.017009 
[214]	train-rmse:0.095743+0.001641	test-rmse:0.214002+0.017008 
[215]	train-rmse:0.095456+0.001629	test-rmse:0.213955+0.017050 
[216]	train-rmse:0.095155+0.001603	test-rmse:0.213985+0.017067 
[217]	train-rmse:0.094806+0.001609	test-rmse:0.214040+0.017069 
[218]	train-rmse:0.094541+0.001600	test-rmse:0.214048+0.017072 
[219]	train-rmse:0.094252+0.001607	test-rmse:0.214047+0.017110 
[220]	train-rmse:0.093937+0.001591	test-rmse:0.214048+0.017066 
[221]	train-rmse:0.093627+0.001640	test-rmse:0.214105+0.017125 
[222]	train-rmse:0.093366+0.001669	test-rmse:0.214119+0.017153 
[223]	train-rmse:0.093065+0.001713	test-rmse:0.214046+0.017149 
[224]	train-rmse:0.092739+0.001713	test-rmse:0.214051+0.017152 
[225]	train-rmse:0.092430+0.001729	test-rmse:0.214088+0.017155 
[226]	train-rmse:0.092135+0.001747	test-rmse:0.214072+0.017193 
[227]	train-rmse:0.091820+0.001736	test-rmse:0.214074+0.017178 
[228]	train-rmse:0.091558+0.001675	test-rmse:0.214046+0.017188 
[229]	train-rmse:0.091230+0.001688	test-rmse:0.214090+0.017176 
[230]	train-rmse:0.090978+0.001643	test-rmse:0.214118+0.017153 
[231]	train-rmse:0.090673+0.001620	test-rmse:0.214161+0.017192 
[232]	train-rmse:0.090366+0.001627	test-rmse:0.214173+0.017189 
[233]	train-rmse:0.090040+0.001589	test-rmse:0.214185+0.017218 
[234]	train-rmse:0.089695+0.001539	test-rmse:0.214238+0.017187 
[235]	train-rmse:0.089411+0.001516	test-rmse:0.214228+0.017233 
[236]	train-rmse:0.089097+0.001543	test-rmse:0.214205+0.017247 
[237]	train-rmse:0.088832+0.001496	test-rmse:0.214243+0.017264 
[238]	train-rmse:0.088585+0.001468	test-rmse:0.214269+0.017262 
[239]	train-rmse:0.088317+0.001503	test-rmse:0.214289+0.017236 
[240]	train-rmse:0.088016+0.001498	test-rmse:0.214278+0.017257 
[241]	train-rmse:0.087736+0.001514	test-rmse:0.214273+0.017264 
[242]	train-rmse:0.087458+0.001536	test-rmse:0.214234+0.017286 
[243]	train-rmse:0.087172+0.001537	test-rmse:0.214199+0.017299 
[244]	train-rmse:0.086872+0.001515	test-rmse:0.214185+0.017305 
[245]	train-rmse:0.086588+0.001535	test-rmse:0.214174+0.017337 
[246]	train-rmse:0.086336+0.001517	test-rmse:0.214151+0.017309 
[247]	train-rmse:0.086018+0.001480	test-rmse:0.214168+0.017332 
[248]	train-rmse:0.085733+0.001463	test-rmse:0.214169+0.017284 
[249]	train-rmse:0.085492+0.001446	test-rmse:0.214166+0.017298 
[250]	train-rmse:0.085245+0.001455	test-rmse:0.214199+0.017303 
[251]	train-rmse:0.084976+0.001422	test-rmse:0.214163+0.017295 
[252]	train-rmse:0.084711+0.001402	test-rmse:0.214220+0.017246 
[253]	train-rmse:0.084446+0.001395	test-rmse:0.214259+0.017268 
Stopping. Best iteration:
[203]	train-rmse:0.099249+0.001670	test-rmse:0.213919+0.016851

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
