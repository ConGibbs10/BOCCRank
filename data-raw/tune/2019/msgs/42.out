> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.392437+0.000772	test-rmse:0.393516+0.004437 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.369660+0.000899	test-rmse:0.372055+0.004595 
[3]	train-rmse:0.349358+0.001134	test-rmse:0.353126+0.004794 
[4]	train-rmse:0.331448+0.001783	test-rmse:0.337076+0.004512 
[5]	train-rmse:0.317653+0.006516	test-rmse:0.324857+0.004480 
[6]	train-rmse:0.303310+0.006610	test-rmse:0.311945+0.005211 
[7]	train-rmse:0.292213+0.010518	test-rmse:0.302268+0.006737 
[8]	train-rmse:0.282694+0.008881	test-rmse:0.293523+0.004785 
[9]	train-rmse:0.274535+0.008566	test-rmse:0.286352+0.005093 
[10]	train-rmse:0.267172+0.009477	test-rmse:0.279871+0.006284 
[11]	train-rmse:0.258992+0.007284	test-rmse:0.272498+0.005770 
[12]	train-rmse:0.253087+0.008479	test-rmse:0.267591+0.005645 
[13]	train-rmse:0.245292+0.007731	test-rmse:0.261323+0.005992 
[14]	train-rmse:0.239213+0.007666	test-rmse:0.256028+0.005624 
[15]	train-rmse:0.233802+0.006776	test-rmse:0.251643+0.007336 
[16]	train-rmse:0.230067+0.007452	test-rmse:0.248689+0.008260 
[17]	train-rmse:0.226561+0.007302	test-rmse:0.246047+0.009266 
[18]	train-rmse:0.223319+0.004966	test-rmse:0.243018+0.011138 
[19]	train-rmse:0.220420+0.005836	test-rmse:0.240540+0.012192 
[20]	train-rmse:0.216585+0.006161	test-rmse:0.237910+0.011497 
[21]	train-rmse:0.213322+0.006685	test-rmse:0.235713+0.011127 
[22]	train-rmse:0.210125+0.006135	test-rmse:0.233345+0.012081 
[23]	train-rmse:0.208163+0.007509	test-rmse:0.232279+0.011449 
[24]	train-rmse:0.205513+0.007367	test-rmse:0.230380+0.012338 
[25]	train-rmse:0.202641+0.007054	test-rmse:0.228398+0.013260 
[26]	train-rmse:0.199990+0.006812	test-rmse:0.226364+0.013578 
[27]	train-rmse:0.198048+0.006842	test-rmse:0.225123+0.014331 
[28]	train-rmse:0.196094+0.007146	test-rmse:0.224340+0.014876 
[29]	train-rmse:0.193814+0.006830	test-rmse:0.223072+0.015333 
[30]	train-rmse:0.192064+0.006821	test-rmse:0.222591+0.015726 
[31]	train-rmse:0.191676+0.007290	test-rmse:0.222451+0.015562 
[32]	train-rmse:0.190333+0.006688	test-rmse:0.221774+0.016265 
[33]	train-rmse:0.188701+0.006330	test-rmse:0.221227+0.016335 
[34]	train-rmse:0.188102+0.006597	test-rmse:0.220887+0.016386 
[35]	train-rmse:0.186323+0.006365	test-rmse:0.220164+0.016459 
[36]	train-rmse:0.185407+0.006135	test-rmse:0.219614+0.016711 
[37]	train-rmse:0.184147+0.006609	test-rmse:0.219118+0.016872 
[38]	train-rmse:0.183622+0.006563	test-rmse:0.219133+0.016969 
[39]	train-rmse:0.182555+0.006471	test-rmse:0.218679+0.016843 
[40]	train-rmse:0.181829+0.006320	test-rmse:0.218391+0.016669 
[41]	train-rmse:0.180687+0.005837	test-rmse:0.217967+0.016953 
[42]	train-rmse:0.179772+0.005382	test-rmse:0.217681+0.016726 
[43]	train-rmse:0.179301+0.004967	test-rmse:0.217413+0.016850 
[44]	train-rmse:0.178619+0.005035	test-rmse:0.217220+0.016887 
[45]	train-rmse:0.178224+0.005426	test-rmse:0.217032+0.016841 
[46]	train-rmse:0.177330+0.005120	test-rmse:0.216748+0.017103 
[47]	train-rmse:0.176544+0.005519	test-rmse:0.216739+0.017159 
[48]	train-rmse:0.175686+0.005732	test-rmse:0.216694+0.017394 
[49]	train-rmse:0.174863+0.005503	test-rmse:0.216635+0.017547 
[50]	train-rmse:0.174411+0.005396	test-rmse:0.216679+0.017500 
[51]	train-rmse:0.174056+0.005405	test-rmse:0.216618+0.017549 
[52]	train-rmse:0.173318+0.005928	test-rmse:0.216390+0.017653 
[53]	train-rmse:0.172350+0.006034	test-rmse:0.216128+0.017703 
[54]	train-rmse:0.171836+0.006009	test-rmse:0.216037+0.017705 
[55]	train-rmse:0.170843+0.005760	test-rmse:0.215835+0.017890 
[56]	train-rmse:0.170164+0.005800	test-rmse:0.216002+0.017679 
[57]	train-rmse:0.169346+0.005705	test-rmse:0.216040+0.017903 
[58]	train-rmse:0.168568+0.005578	test-rmse:0.215572+0.017825 
[59]	train-rmse:0.167882+0.006008	test-rmse:0.215504+0.017883 
[60]	train-rmse:0.167457+0.005892	test-rmse:0.215736+0.018140 
[61]	train-rmse:0.167279+0.006014	test-rmse:0.215758+0.018155 
[62]	train-rmse:0.166498+0.005887	test-rmse:0.215788+0.018386 
[63]	train-rmse:0.166166+0.006150	test-rmse:0.215836+0.018422 
[64]	train-rmse:0.165413+0.006106	test-rmse:0.215943+0.018580 
[65]	train-rmse:0.164431+0.006232	test-rmse:0.215888+0.019151 
[66]	train-rmse:0.163677+0.006313	test-rmse:0.216037+0.019261 
[67]	train-rmse:0.162713+0.006323	test-rmse:0.216039+0.019101 
[68]	train-rmse:0.161925+0.006120	test-rmse:0.215905+0.019168 
[69]	train-rmse:0.161287+0.005923	test-rmse:0.215818+0.019320 
[70]	train-rmse:0.160826+0.005966	test-rmse:0.215936+0.019186 
[71]	train-rmse:0.160247+0.005921	test-rmse:0.215903+0.019207 
[72]	train-rmse:0.159867+0.005869	test-rmse:0.216052+0.019412 
[73]	train-rmse:0.159249+0.005822	test-rmse:0.216272+0.019531 
[74]	train-rmse:0.158593+0.005758	test-rmse:0.216065+0.019478 
[75]	train-rmse:0.157905+0.005899	test-rmse:0.216110+0.019485 
[76]	train-rmse:0.157191+0.005700	test-rmse:0.216109+0.019436 
[77]	train-rmse:0.156591+0.005890	test-rmse:0.216266+0.019316 
[78]	train-rmse:0.155826+0.005947	test-rmse:0.216203+0.019484 
[79]	train-rmse:0.154897+0.006048	test-rmse:0.216014+0.019503 
[80]	train-rmse:0.154066+0.005967	test-rmse:0.216223+0.019691 
[81]	train-rmse:0.153663+0.005779	test-rmse:0.216271+0.019748 
[82]	train-rmse:0.152865+0.006015	test-rmse:0.216508+0.019954 
[83]	train-rmse:0.152101+0.005641	test-rmse:0.216306+0.020233 
[84]	train-rmse:0.151222+0.005674	test-rmse:0.216094+0.020232 
[85]	train-rmse:0.150640+0.005660	test-rmse:0.215782+0.020246 
[86]	train-rmse:0.150350+0.005882	test-rmse:0.215745+0.020280 
[87]	train-rmse:0.149829+0.005958	test-rmse:0.215692+0.020253 
[88]	train-rmse:0.149522+0.005966	test-rmse:0.215649+0.020184 
[89]	train-rmse:0.149362+0.006015	test-rmse:0.215588+0.020168 
[90]	train-rmse:0.148894+0.006122	test-rmse:0.215709+0.020089 
[91]	train-rmse:0.148523+0.006378	test-rmse:0.215716+0.019998 
[92]	train-rmse:0.148007+0.006419	test-rmse:0.215853+0.019794 
[93]	train-rmse:0.147606+0.006440	test-rmse:0.215964+0.019864 
[94]	train-rmse:0.147300+0.006300	test-rmse:0.216031+0.019869 
[95]	train-rmse:0.146581+0.006306	test-rmse:0.215871+0.019837 
[96]	train-rmse:0.146363+0.006102	test-rmse:0.215863+0.019832 
[97]	train-rmse:0.145738+0.006119	test-rmse:0.215820+0.019799 
[98]	train-rmse:0.145203+0.005773	test-rmse:0.215864+0.019781 
[99]	train-rmse:0.144782+0.005940	test-rmse:0.215911+0.019842 
[100]	train-rmse:0.144497+0.006110	test-rmse:0.215951+0.019922 
[101]	train-rmse:0.143970+0.006176	test-rmse:0.216038+0.019966 
[102]	train-rmse:0.143545+0.005870	test-rmse:0.216059+0.020005 
[103]	train-rmse:0.143004+0.006039	test-rmse:0.216018+0.019950 
[104]	train-rmse:0.142650+0.005985	test-rmse:0.216016+0.020414 
[105]	train-rmse:0.142095+0.005955	test-rmse:0.216053+0.020495 
[106]	train-rmse:0.141753+0.005828	test-rmse:0.216130+0.020567 
[107]	train-rmse:0.141432+0.005952	test-rmse:0.216164+0.020391 
[108]	train-rmse:0.140754+0.006085	test-rmse:0.216080+0.020297 
[109]	train-rmse:0.140063+0.006326	test-rmse:0.216260+0.020283 
Stopping. Best iteration:
[59]	train-rmse:0.167882+0.006008	test-rmse:0.215504+0.017883

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
