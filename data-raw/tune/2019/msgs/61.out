> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.416169+0.001004	test-rmse:0.416154+0.004063 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.414577+0.001073	test-rmse:0.414565+0.003987 
[3]	train-rmse:0.413044+0.001055	test-rmse:0.413032+0.003997 
[4]	train-rmse:0.411630+0.001166	test-rmse:0.411610+0.003884 
[5]	train-rmse:0.410466+0.001398	test-rmse:0.410467+0.003705 
[6]	train-rmse:0.409062+0.001385	test-rmse:0.409061+0.003715 
[7]	train-rmse:0.407728+0.001569	test-rmse:0.407739+0.003663 
[8]	train-rmse:0.406308+0.001547	test-rmse:0.406319+0.003664 
[9]	train-rmse:0.405262+0.001482	test-rmse:0.405282+0.003943 
[10]	train-rmse:0.404010+0.001511	test-rmse:0.404025+0.003928 
[11]	train-rmse:0.402927+0.001278	test-rmse:0.402938+0.004228 
[12]	train-rmse:0.401920+0.001053	test-rmse:0.401951+0.004566 
[13]	train-rmse:0.400492+0.001130	test-rmse:0.400532+0.004573 
[14]	train-rmse:0.399651+0.001109	test-rmse:0.399687+0.004725 
[15]	train-rmse:0.398619+0.001587	test-rmse:0.398689+0.004571 
[16]	train-rmse:0.397620+0.001360	test-rmse:0.397671+0.004803 
[17]	train-rmse:0.396541+0.001589	test-rmse:0.396613+0.005017 
[18]	train-rmse:0.395466+0.001559	test-rmse:0.395546+0.005052 
[19]	train-rmse:0.394339+0.001634	test-rmse:0.394415+0.005404 
[20]	train-rmse:0.393442+0.001717	test-rmse:0.393517+0.005277 
[21]	train-rmse:0.392382+0.001461	test-rmse:0.392432+0.005203 
[22]	train-rmse:0.391333+0.001622	test-rmse:0.391383+0.004988 
[23]	train-rmse:0.390847+0.001908	test-rmse:0.390911+0.004990 
[24]	train-rmse:0.390138+0.002082	test-rmse:0.390202+0.005239 
[25]	train-rmse:0.389231+0.002248	test-rmse:0.389320+0.005516 
[26]	train-rmse:0.388024+0.002208	test-rmse:0.388104+0.005305 
[27]	train-rmse:0.387069+0.002247	test-rmse:0.387148+0.005207 
[28]	train-rmse:0.386040+0.002568	test-rmse:0.386129+0.005151 
[29]	train-rmse:0.384804+0.002535	test-rmse:0.384896+0.005107 
[30]	train-rmse:0.384189+0.002733	test-rmse:0.384273+0.005498 
[31]	train-rmse:0.383391+0.002985	test-rmse:0.383473+0.005551 
[32]	train-rmse:0.382726+0.003302	test-rmse:0.382804+0.005917 
[33]	train-rmse:0.381654+0.003723	test-rmse:0.381744+0.006018 
[34]	train-rmse:0.380809+0.004163	test-rmse:0.380907+0.006433 
[35]	train-rmse:0.379822+0.003996	test-rmse:0.379904+0.006203 
[36]	train-rmse:0.378889+0.004355	test-rmse:0.378988+0.006233 
[37]	train-rmse:0.378056+0.004320	test-rmse:0.378185+0.006292 
[38]	train-rmse:0.377481+0.004137	test-rmse:0.377615+0.006227 
[39]	train-rmse:0.376474+0.004018	test-rmse:0.376633+0.005931 
[40]	train-rmse:0.375501+0.004123	test-rmse:0.375656+0.006299 
[41]	train-rmse:0.374539+0.003932	test-rmse:0.374676+0.006298 
[42]	train-rmse:0.374173+0.004116	test-rmse:0.374329+0.006275 
[43]	train-rmse:0.373406+0.004471	test-rmse:0.373607+0.006120 
[44]	train-rmse:0.372581+0.004932	test-rmse:0.372798+0.006543 
[45]	train-rmse:0.372235+0.004617	test-rmse:0.372440+0.006509 
[46]	train-rmse:0.371257+0.004631	test-rmse:0.371492+0.006324 
[47]	train-rmse:0.370325+0.004396	test-rmse:0.370546+0.006221 
[48]	train-rmse:0.369386+0.004169	test-rmse:0.369585+0.005989 
[49]	train-rmse:0.368494+0.004265	test-rmse:0.368725+0.005863 
[50]	train-rmse:0.367777+0.004177	test-rmse:0.367989+0.006046 
[51]	train-rmse:0.366707+0.004125	test-rmse:0.366918+0.005948 
[52]	train-rmse:0.366227+0.004174	test-rmse:0.366445+0.006032 
[53]	train-rmse:0.365330+0.004040	test-rmse:0.365527+0.005866 
[54]	train-rmse:0.365056+0.004168	test-rmse:0.365273+0.005972 
[55]	train-rmse:0.364557+0.003845	test-rmse:0.364731+0.006183 
[56]	train-rmse:0.363790+0.003313	test-rmse:0.363916+0.006034 
[57]	train-rmse:0.362923+0.003488	test-rmse:0.363051+0.006395 
[58]	train-rmse:0.362422+0.003461	test-rmse:0.362576+0.006047 
[59]	train-rmse:0.361766+0.003809	test-rmse:0.361962+0.005802 
[60]	train-rmse:0.361009+0.003641	test-rmse:0.361183+0.006161 
[61]	train-rmse:0.360345+0.003980	test-rmse:0.360563+0.005919 
[62]	train-rmse:0.359734+0.004320	test-rmse:0.359992+0.005729 
[63]	train-rmse:0.359249+0.003985	test-rmse:0.359493+0.005401 
[64]	train-rmse:0.358240+0.003951	test-rmse:0.358480+0.005396 
[65]	train-rmse:0.357592+0.004138	test-rmse:0.357859+0.005607 
[66]	train-rmse:0.357118+0.004335	test-rmse:0.357405+0.005252 
[67]	train-rmse:0.356653+0.004430	test-rmse:0.356924+0.005463 
[68]	train-rmse:0.355825+0.004217	test-rmse:0.356082+0.005439 
[69]	train-rmse:0.354989+0.004336	test-rmse:0.355254+0.005785 
[70]	train-rmse:0.354361+0.004285	test-rmse:0.354600+0.006080 
[71]	train-rmse:0.353747+0.004268	test-rmse:0.353985+0.006024 
[72]	train-rmse:0.353124+0.004263	test-rmse:0.353371+0.005706 
[73]	train-rmse:0.352290+0.004404	test-rmse:0.352568+0.005517 
[74]	train-rmse:0.351457+0.004268	test-rmse:0.351712+0.005488 
[75]	train-rmse:0.350874+0.004344	test-rmse:0.351117+0.005388 
[76]	train-rmse:0.350498+0.004603	test-rmse:0.350759+0.005152 
[77]	train-rmse:0.349594+0.004545	test-rmse:0.349855+0.005108 
[78]	train-rmse:0.349027+0.004746	test-rmse:0.349317+0.005258 
[79]	train-rmse:0.348604+0.004851	test-rmse:0.348899+0.005427 
[80]	train-rmse:0.347791+0.004950	test-rmse:0.348090+0.005812 
[81]	train-rmse:0.347362+0.004902	test-rmse:0.347673+0.005982 
[82]	train-rmse:0.346818+0.004790	test-rmse:0.347136+0.005843 
[83]	train-rmse:0.346284+0.004641	test-rmse:0.346586+0.006155 
[84]	train-rmse:0.345553+0.004769	test-rmse:0.345869+0.006003 
[85]	train-rmse:0.344977+0.004802	test-rmse:0.345272+0.006316 
[86]	train-rmse:0.344217+0.004947	test-rmse:0.344514+0.006679 
[87]	train-rmse:0.343810+0.004700	test-rmse:0.344091+0.006528 
[88]	train-rmse:0.343234+0.004310	test-rmse:0.343472+0.006513 
[89]	train-rmse:0.342646+0.004288	test-rmse:0.342871+0.006335 
[90]	train-rmse:0.342105+0.004259	test-rmse:0.342300+0.006699 
[91]	train-rmse:0.341473+0.004127	test-rmse:0.341643+0.006681 
[92]	train-rmse:0.341070+0.003928	test-rmse:0.341225+0.006576 
[93]	train-rmse:0.340201+0.004013	test-rmse:0.340370+0.006570 
[94]	train-rmse:0.339498+0.004092	test-rmse:0.339681+0.006427 
[95]	train-rmse:0.338794+0.004258	test-rmse:0.339003+0.006349 
[96]	train-rmse:0.338156+0.004184	test-rmse:0.338341+0.006353 
[97]	train-rmse:0.337774+0.004157	test-rmse:0.337984+0.006020 
[98]	train-rmse:0.336913+0.004233	test-rmse:0.337127+0.006119 
[99]	train-rmse:0.336371+0.004427	test-rmse:0.336593+0.006008 
[100]	train-rmse:0.335880+0.004499	test-rmse:0.336107+0.006145 
[101]	train-rmse:0.335252+0.004462	test-rmse:0.335453+0.006278 
[102]	train-rmse:0.334472+0.004434	test-rmse:0.334673+0.006325 
[103]	train-rmse:0.333806+0.004475	test-rmse:0.334006+0.006638 
[104]	train-rmse:0.333346+0.004292	test-rmse:0.333541+0.006551 
[105]	train-rmse:0.333155+0.004577	test-rmse:0.333370+0.006553 
[106]	train-rmse:0.332363+0.004578	test-rmse:0.332575+0.006586 
[107]	train-rmse:0.331550+0.004602	test-rmse:0.331771+0.006537 
[108]	train-rmse:0.330957+0.004675	test-rmse:0.331176+0.006822 
[109]	train-rmse:0.330302+0.004420	test-rmse:0.330506+0.006870 
[110]	train-rmse:0.329976+0.004206	test-rmse:0.330153+0.006806 
[111]	train-rmse:0.329554+0.003959	test-rmse:0.329696+0.006837 
[112]	train-rmse:0.329051+0.003819	test-rmse:0.329195+0.006698 
[113]	train-rmse:0.328722+0.003706	test-rmse:0.328855+0.006576 
[114]	train-rmse:0.328069+0.003467	test-rmse:0.328185+0.006624 
[115]	train-rmse:0.327478+0.003475	test-rmse:0.327590+0.006891 
[116]	train-rmse:0.326981+0.003559	test-rmse:0.327079+0.006896 
[117]	train-rmse:0.326662+0.003452	test-rmse:0.326752+0.007074 
[118]	train-rmse:0.326317+0.003490	test-rmse:0.326432+0.006797 
[119]	train-rmse:0.325720+0.003675	test-rmse:0.325868+0.006582 
[120]	train-rmse:0.325298+0.003664	test-rmse:0.325428+0.006850 
[121]	train-rmse:0.324766+0.003677	test-rmse:0.324899+0.007070 
[122]	train-rmse:0.324206+0.003793	test-rmse:0.324347+0.007043 
[123]	train-rmse:0.323780+0.003930	test-rmse:0.323933+0.007166 
[124]	train-rmse:0.323338+0.004053	test-rmse:0.323482+0.007154 
[125]	train-rmse:0.322913+0.003997	test-rmse:0.323054+0.007149 
[126]	train-rmse:0.322753+0.004069	test-rmse:0.322916+0.007100 
[127]	train-rmse:0.322333+0.004055	test-rmse:0.322493+0.007130 
[128]	train-rmse:0.321901+0.004018	test-rmse:0.322035+0.007437 
[129]	train-rmse:0.321236+0.003975	test-rmse:0.321367+0.007448 
[130]	train-rmse:0.320819+0.003692	test-rmse:0.320905+0.007642 
[131]	train-rmse:0.320418+0.003442	test-rmse:0.320468+0.007745 
[132]	train-rmse:0.320284+0.003375	test-rmse:0.320308+0.007967 
[133]	train-rmse:0.319982+0.003619	test-rmse:0.320027+0.007746 
[134]	train-rmse:0.319582+0.003442	test-rmse:0.319611+0.008005 
[135]	train-rmse:0.319268+0.003465	test-rmse:0.319296+0.008197 
[136]	train-rmse:0.319017+0.003382	test-rmse:0.319053+0.008234 
[137]	train-rmse:0.318756+0.003638	test-rmse:0.318829+0.008138 
[138]	train-rmse:0.318112+0.003670	test-rmse:0.318189+0.008159 
[139]	train-rmse:0.317551+0.003720	test-rmse:0.317628+0.008412 
[140]	train-rmse:0.317384+0.003935	test-rmse:0.317480+0.008348 
[141]	train-rmse:0.316833+0.004020	test-rmse:0.316960+0.008174 
[142]	train-rmse:0.316327+0.004140	test-rmse:0.316462+0.008153 
[143]	train-rmse:0.315821+0.004031	test-rmse:0.315929+0.008262 
[144]	train-rmse:0.315294+0.004012	test-rmse:0.315395+0.008524 
[145]	train-rmse:0.314873+0.003837	test-rmse:0.314953+0.008618 
[146]	train-rmse:0.314388+0.003904	test-rmse:0.314470+0.008815 
[147]	train-rmse:0.314020+0.003652	test-rmse:0.314068+0.008880 
[148]	train-rmse:0.313529+0.003861	test-rmse:0.313593+0.008852 
[149]	train-rmse:0.312915+0.003873	test-rmse:0.312979+0.008860 
[150]	train-rmse:0.312445+0.003743	test-rmse:0.312498+0.008892 
[151]	train-rmse:0.312155+0.003796	test-rmse:0.312199+0.009123 
[152]	train-rmse:0.311836+0.003907	test-rmse:0.311907+0.009092 
[153]	train-rmse:0.311602+0.003741	test-rmse:0.311664+0.009003 
[154]	train-rmse:0.311279+0.003716	test-rmse:0.311353+0.008905 
[155]	train-rmse:0.310790+0.003910	test-rmse:0.310877+0.008935 
[156]	train-rmse:0.310433+0.004148	test-rmse:0.310561+0.008705 
[157]	train-rmse:0.310208+0.004078	test-rmse:0.310339+0.008762 
[158]	train-rmse:0.309961+0.004176	test-rmse:0.310113+0.008586 
[159]	train-rmse:0.309644+0.004336	test-rmse:0.309806+0.008689 
[160]	train-rmse:0.309504+0.004317	test-rmse:0.309666+0.008551 
[161]	train-rmse:0.309160+0.004390	test-rmse:0.309321+0.008604 
[162]	train-rmse:0.308771+0.004526	test-rmse:0.308940+0.008600 
[163]	train-rmse:0.308226+0.004559	test-rmse:0.308397+0.008625 
[164]	train-rmse:0.308000+0.004588	test-rmse:0.308156+0.008805 
[165]	train-rmse:0.307657+0.004367	test-rmse:0.307771+0.008959 
[166]	train-rmse:0.307200+0.004380	test-rmse:0.307316+0.009136 
[167]	train-rmse:0.306887+0.004306	test-rmse:0.306982+0.009326 
[168]	train-rmse:0.306456+0.004343	test-rmse:0.306553+0.009486 
[169]	train-rmse:0.305998+0.004392	test-rmse:0.306122+0.009345 
[170]	train-rmse:0.305551+0.004261	test-rmse:0.305655+0.009476 
[171]	train-rmse:0.305238+0.004411	test-rmse:0.305345+0.009621 
[172]	train-rmse:0.304803+0.004467	test-rmse:0.304911+0.009797 
[173]	train-rmse:0.304595+0.004550	test-rmse:0.304722+0.009662 
[174]	train-rmse:0.304147+0.004429	test-rmse:0.304251+0.009750 
[175]	train-rmse:0.303764+0.004311	test-rmse:0.303852+0.009853 
[176]	train-rmse:0.303260+0.004276	test-rmse:0.303347+0.009905 
[177]	train-rmse:0.302966+0.004330	test-rmse:0.303049+0.009943 
[178]	train-rmse:0.302549+0.004375	test-rmse:0.302656+0.009792 
[179]	train-rmse:0.302261+0.004459	test-rmse:0.302371+0.009795 
[180]	train-rmse:0.301868+0.004346	test-rmse:0.301958+0.009874 
[181]	train-rmse:0.301547+0.004378	test-rmse:0.301623+0.009929 
[182]	train-rmse:0.301174+0.004404	test-rmse:0.301250+0.010072 
[183]	train-rmse:0.300809+0.004319	test-rmse:0.300866+0.010155 
[184]	train-rmse:0.300282+0.004297	test-rmse:0.300330+0.010302 
[185]	train-rmse:0.299945+0.004325	test-rmse:0.300012+0.010210 
[186]	train-rmse:0.299690+0.004457	test-rmse:0.299765+0.010304 
[187]	train-rmse:0.299379+0.004665	test-rmse:0.299494+0.010103 
[188]	train-rmse:0.299106+0.004648	test-rmse:0.299230+0.010010 
[189]	train-rmse:0.298686+0.004526	test-rmse:0.298783+0.010115 
[190]	train-rmse:0.298393+0.004553	test-rmse:0.298510+0.010090 
[191]	train-rmse:0.298304+0.004647	test-rmse:0.298440+0.010058 
[192]	train-rmse:0.298119+0.004837	test-rmse:0.298289+0.009961 
[193]	train-rmse:0.297793+0.004808	test-rmse:0.297958+0.010109 
[194]	train-rmse:0.297518+0.005001	test-rmse:0.297720+0.009934 
[195]	train-rmse:0.297403+0.005114	test-rmse:0.297624+0.009869 
[196]	train-rmse:0.296948+0.005080	test-rmse:0.297157+0.009999 
[197]	train-rmse:0.296749+0.005058	test-rmse:0.296962+0.010014 
[198]	train-rmse:0.296392+0.005097	test-rmse:0.296624+0.009926 
[199]	train-rmse:0.296044+0.005255	test-rmse:0.296301+0.009837 
[200]	train-rmse:0.295721+0.005149	test-rmse:0.295953+0.009939 
[201]	train-rmse:0.295281+0.005151	test-rmse:0.295513+0.009995 
[202]	train-rmse:0.294975+0.005149	test-rmse:0.295204+0.010139 
[203]	train-rmse:0.294592+0.005059	test-rmse:0.294806+0.010246 
[204]	train-rmse:0.294258+0.005149	test-rmse:0.294475+0.010305 
[205]	train-rmse:0.293896+0.005038	test-rmse:0.294093+0.010460 
[206]	train-rmse:0.293541+0.005127	test-rmse:0.293742+0.010454 
[207]	train-rmse:0.293473+0.005121	test-rmse:0.293673+0.010403 
[208]	train-rmse:0.293225+0.005060	test-rmse:0.293413+0.010572 
[209]	train-rmse:0.293042+0.005039	test-rmse:0.293235+0.010580 
[210]	train-rmse:0.292787+0.005032	test-rmse:0.292985+0.010506 
[211]	train-rmse:0.292459+0.005110	test-rmse:0.292682+0.010421 
[212]	train-rmse:0.292069+0.005082	test-rmse:0.292281+0.010513 
[213]	train-rmse:0.291741+0.005165	test-rmse:0.291978+0.010425 
[214]	train-rmse:0.291309+0.005125	test-rmse:0.291532+0.010544 
[215]	train-rmse:0.290920+0.005120	test-rmse:0.291139+0.010652 
[216]	train-rmse:0.290588+0.005172	test-rmse:0.290818+0.010635 
[217]	train-rmse:0.290368+0.005160	test-rmse:0.290599+0.010596 
[218]	train-rmse:0.290166+0.005243	test-rmse:0.290406+0.010662 
[219]	train-rmse:0.289933+0.005213	test-rmse:0.290177+0.010644 
[220]	train-rmse:0.289800+0.005237	test-rmse:0.290051+0.010645 
[221]	train-rmse:0.289669+0.005281	test-rmse:0.289931+0.010556 
[222]	train-rmse:0.289529+0.005137	test-rmse:0.289757+0.010758 
[223]	train-rmse:0.289387+0.005115	test-rmse:0.289619+0.010803 
[224]	train-rmse:0.289177+0.005102	test-rmse:0.289400+0.010858 
[225]	train-rmse:0.288883+0.005196	test-rmse:0.289118+0.010840 
[226]	train-rmse:0.288638+0.005162	test-rmse:0.288871+0.010802 
[227]	train-rmse:0.288482+0.005166	test-rmse:0.288703+0.010914 
[228]	train-rmse:0.288191+0.005102	test-rmse:0.288400+0.011005 
[229]	train-rmse:0.288066+0.005051	test-rmse:0.288258+0.011051 
[230]	train-rmse:0.287669+0.005091	test-rmse:0.287864+0.011102 
[231]	train-rmse:0.287507+0.005122	test-rmse:0.287716+0.011106 
[232]	train-rmse:0.287258+0.005152	test-rmse:0.287479+0.011060 
[233]	train-rmse:0.287073+0.005274	test-rmse:0.287319+0.010938 
[234]	train-rmse:0.286800+0.005361	test-rmse:0.287068+0.010845 
[235]	train-rmse:0.286431+0.005357	test-rmse:0.286695+0.010946 
[236]	train-rmse:0.286240+0.005238	test-rmse:0.286474+0.011050 
[237]	train-rmse:0.286035+0.005089	test-rmse:0.286235+0.011201 
[238]	train-rmse:0.285807+0.005141	test-rmse:0.286019+0.011134 
[239]	train-rmse:0.285552+0.005151	test-rmse:0.285765+0.011234 
[240]	train-rmse:0.285420+0.005144	test-rmse:0.285623+0.011328 
[241]	train-rmse:0.285189+0.005092	test-rmse:0.285379+0.011418 
[242]	train-rmse:0.284924+0.005157	test-rmse:0.285120+0.011415 
[243]	train-rmse:0.284741+0.005213	test-rmse:0.284956+0.011404 
[244]	train-rmse:0.284560+0.005220	test-rmse:0.284778+0.011391 
[245]	train-rmse:0.284302+0.005200	test-rmse:0.284516+0.011511 
[246]	train-rmse:0.284226+0.005204	test-rmse:0.284439+0.011461 
[247]	train-rmse:0.283949+0.005289	test-rmse:0.284185+0.011390 
[248]	train-rmse:0.283730+0.005366	test-rmse:0.283977+0.011358 
[249]	train-rmse:0.283542+0.005242	test-rmse:0.283755+0.011500 
[250]	train-rmse:0.283474+0.005310	test-rmse:0.283705+0.011465 
[251]	train-rmse:0.283363+0.005189	test-rmse:0.283565+0.011628 
[252]	train-rmse:0.283230+0.005170	test-rmse:0.283418+0.011734 
[253]	train-rmse:0.282977+0.005274	test-rmse:0.283183+0.011679 
[254]	train-rmse:0.282762+0.005296	test-rmse:0.282975+0.011747 
[255]	train-rmse:0.282650+0.005244	test-rmse:0.282842+0.011811 
[256]	train-rmse:0.282498+0.005275	test-rmse:0.282702+0.011813 
[257]	train-rmse:0.282391+0.005258	test-rmse:0.282595+0.011844 
[258]	train-rmse:0.282110+0.005376	test-rmse:0.282335+0.011798 
[259]	train-rmse:0.281989+0.005316	test-rmse:0.282202+0.011825 
[260]	train-rmse:0.281751+0.005278	test-rmse:0.281955+0.011965 
[261]	train-rmse:0.281585+0.005334	test-rmse:0.281797+0.012012 
[262]	train-rmse:0.281389+0.005257	test-rmse:0.281582+0.012119 
[263]	train-rmse:0.281196+0.005317	test-rmse:0.281408+0.012117 
[264]	train-rmse:0.281017+0.005448	test-rmse:0.281259+0.012005 
[265]	train-rmse:0.280777+0.005522	test-rmse:0.281032+0.011998 
[266]	train-rmse:0.280623+0.005645	test-rmse:0.280907+0.011904 
[267]	train-rmse:0.280397+0.005694	test-rmse:0.280685+0.011928 
[268]	train-rmse:0.280192+0.005650	test-rmse:0.280463+0.012004 
[269]	train-rmse:0.280061+0.005598	test-rmse:0.280318+0.012103 
[270]	train-rmse:0.279956+0.005604	test-rmse:0.280222+0.012122 
[271]	train-rmse:0.279812+0.005586	test-rmse:0.280070+0.012200 
[272]	train-rmse:0.279574+0.005598	test-rmse:0.279830+0.012254 
[273]	train-rmse:0.279309+0.005611	test-rmse:0.279566+0.012299 
[274]	train-rmse:0.279150+0.005730	test-rmse:0.279436+0.012203 
[275]	train-rmse:0.278962+0.005724	test-rmse:0.279246+0.012297 
[276]	train-rmse:0.278718+0.005806	test-rmse:0.279025+0.012237 
[277]	train-rmse:0.278551+0.005733	test-rmse:0.278837+0.012332 
[278]	train-rmse:0.278313+0.005687	test-rmse:0.278585+0.012434 
[279]	train-rmse:0.278115+0.005622	test-rmse:0.278370+0.012527 
[280]	train-rmse:0.277922+0.005660	test-rmse:0.278178+0.012572 
[281]	train-rmse:0.277825+0.005688	test-rmse:0.278088+0.012576 
[282]	train-rmse:0.277628+0.005745	test-rmse:0.277908+0.012525 
[283]	train-rmse:0.277469+0.005756	test-rmse:0.277751+0.012516 
[284]	train-rmse:0.277353+0.005771	test-rmse:0.277639+0.012543 
[285]	train-rmse:0.277267+0.005794	test-rmse:0.277556+0.012573 
[286]	train-rmse:0.277208+0.005743	test-rmse:0.277487+0.012629 
[287]	train-rmse:0.277051+0.005773	test-rmse:0.277333+0.012667 
[288]	train-rmse:0.276946+0.005716	test-rmse:0.277205+0.012735 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
