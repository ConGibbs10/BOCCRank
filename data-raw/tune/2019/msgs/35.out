> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.417696+0.001019	test-rmse:0.417676+0.004059 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.417678+0.001019	test-rmse:0.417660+0.004059 
[3]	train-rmse:0.417661+0.001019	test-rmse:0.417644+0.004059 
[4]	train-rmse:0.417643+0.001019	test-rmse:0.417628+0.004059 
[5]	train-rmse:0.417626+0.001018	test-rmse:0.417612+0.004059 
[6]	train-rmse:0.417612+0.001019	test-rmse:0.417600+0.004058 
[7]	train-rmse:0.417595+0.001019	test-rmse:0.417584+0.004058 
[8]	train-rmse:0.417577+0.001019	test-rmse:0.417568+0.004057 
[9]	train-rmse:0.417562+0.001019	test-rmse:0.417554+0.004057 
[10]	train-rmse:0.417544+0.001019	test-rmse:0.417538+0.004057 
[11]	train-rmse:0.417529+0.001019	test-rmse:0.417525+0.004057 
[12]	train-rmse:0.417512+0.001019	test-rmse:0.417509+0.004057 
[13]	train-rmse:0.417497+0.001015	test-rmse:0.417495+0.004061 
[14]	train-rmse:0.417479+0.001015	test-rmse:0.417479+0.004061 
[15]	train-rmse:0.417462+0.001014	test-rmse:0.417462+0.004060 
[16]	train-rmse:0.417444+0.001014	test-rmse:0.417447+0.004060 
[17]	train-rmse:0.417427+0.001014	test-rmse:0.417430+0.004060 
[18]	train-rmse:0.417410+0.001014	test-rmse:0.417414+0.004060 
[19]	train-rmse:0.417395+0.001014	test-rmse:0.417400+0.004059 
[20]	train-rmse:0.417386+0.001010	test-rmse:0.417393+0.004062 
[21]	train-rmse:0.417369+0.001010	test-rmse:0.417377+0.004062 
[22]	train-rmse:0.417354+0.001010	test-rmse:0.417364+0.004062 
[23]	train-rmse:0.417340+0.001014	test-rmse:0.417350+0.004059 
[24]	train-rmse:0.417328+0.001014	test-rmse:0.417340+0.004059 
[25]	train-rmse:0.417311+0.001013	test-rmse:0.417324+0.004059 
[26]	train-rmse:0.417293+0.001013	test-rmse:0.417308+0.004058 
[27]	train-rmse:0.417276+0.001013	test-rmse:0.417292+0.004058 
[28]	train-rmse:0.417259+0.001013	test-rmse:0.417276+0.004058 
[29]	train-rmse:0.417244+0.001013	test-rmse:0.417263+0.004057 
[30]	train-rmse:0.417230+0.001007	test-rmse:0.417250+0.004062 
[31]	train-rmse:0.417213+0.001007	test-rmse:0.417234+0.004062 
[32]	train-rmse:0.417195+0.001007	test-rmse:0.417218+0.004062 
[33]	train-rmse:0.417178+0.001007	test-rmse:0.417202+0.004062 
[34]	train-rmse:0.417160+0.001007	test-rmse:0.417186+0.004062 
[35]	train-rmse:0.417143+0.001007	test-rmse:0.417170+0.004061 
[36]	train-rmse:0.417132+0.001010	test-rmse:0.417159+0.004058 
[37]	train-rmse:0.417118+0.001014	test-rmse:0.417146+0.004054 
[38]	train-rmse:0.417107+0.001020	test-rmse:0.417136+0.004048 
[39]	train-rmse:0.417089+0.001020	test-rmse:0.417120+0.004048 
[40]	train-rmse:0.417075+0.001014	test-rmse:0.417107+0.004053 
[41]	train-rmse:0.417061+0.001016	test-rmse:0.417094+0.004051 
[42]	train-rmse:0.417047+0.001018	test-rmse:0.417081+0.004049 
[43]	train-rmse:0.417029+0.001018	test-rmse:0.417066+0.004049 
[44]	train-rmse:0.417012+0.001018	test-rmse:0.417050+0.004049 
[45]	train-rmse:0.416995+0.001018	test-rmse:0.417034+0.004048 
[46]	train-rmse:0.416977+0.001018	test-rmse:0.417018+0.004048 
[47]	train-rmse:0.416963+0.001017	test-rmse:0.417005+0.004048 
[48]	train-rmse:0.416949+0.001011	test-rmse:0.416992+0.004054 
[49]	train-rmse:0.416935+0.001015	test-rmse:0.416979+0.004050 
[50]	train-rmse:0.416918+0.001015	test-rmse:0.416962+0.004050 
[51]	train-rmse:0.416900+0.001015	test-rmse:0.416946+0.004049 
[52]	train-rmse:0.416883+0.001015	test-rmse:0.416930+0.004049 
[53]	train-rmse:0.416865+0.001015	test-rmse:0.416914+0.004049 
[54]	train-rmse:0.416848+0.001014	test-rmse:0.416898+0.004049 
[55]	train-rmse:0.416837+0.001012	test-rmse:0.416888+0.004051 
[56]	train-rmse:0.416820+0.001012	test-rmse:0.416872+0.004050 
[57]	train-rmse:0.416806+0.001014	test-rmse:0.416859+0.004048 
[58]	train-rmse:0.416788+0.001014	test-rmse:0.416843+0.004048 
[59]	train-rmse:0.416774+0.001014	test-rmse:0.416830+0.004048 
[60]	train-rmse:0.416757+0.001014	test-rmse:0.416815+0.004048 
[61]	train-rmse:0.416740+0.001014	test-rmse:0.416799+0.004048 
[62]	train-rmse:0.416726+0.001016	test-rmse:0.416786+0.004045 
[63]	train-rmse:0.416708+0.001016	test-rmse:0.416771+0.004046 
[64]	train-rmse:0.416697+0.001020	test-rmse:0.416760+0.004042 
[65]	train-rmse:0.416683+0.001019	test-rmse:0.416748+0.004042 
[66]	train-rmse:0.416669+0.001019	test-rmse:0.416735+0.004042 
[67]	train-rmse:0.416652+0.001019	test-rmse:0.416719+0.004042 
[68]	train-rmse:0.416634+0.001019	test-rmse:0.416703+0.004042 
[69]	train-rmse:0.416624+0.001021	test-rmse:0.416693+0.004040 
[70]	train-rmse:0.416606+0.001021	test-rmse:0.416677+0.004040 
[71]	train-rmse:0.416589+0.001021	test-rmse:0.416661+0.004039 
[72]	train-rmse:0.416572+0.001021	test-rmse:0.416645+0.004039 
[73]	train-rmse:0.416558+0.001021	test-rmse:0.416633+0.004039 
[74]	train-rmse:0.416540+0.001021	test-rmse:0.416617+0.004039 
[75]	train-rmse:0.416529+0.001015	test-rmse:0.416608+0.004044 
[76]	train-rmse:0.416512+0.001015	test-rmse:0.416592+0.004044 
[77]	train-rmse:0.416498+0.001015	test-rmse:0.416579+0.004044 
[78]	train-rmse:0.416480+0.001015	test-rmse:0.416563+0.004044 
[79]	train-rmse:0.416463+0.001015	test-rmse:0.416547+0.004044 
[80]	train-rmse:0.416446+0.001015	test-rmse:0.416532+0.004043 
[81]	train-rmse:0.416428+0.001015	test-rmse:0.416516+0.004043 
[82]	train-rmse:0.416414+0.001009	test-rmse:0.416503+0.004048 
[83]	train-rmse:0.416400+0.001003	test-rmse:0.416490+0.004053 
[84]	train-rmse:0.416383+0.001003	test-rmse:0.416474+0.004053 
[85]	train-rmse:0.416369+0.001002	test-rmse:0.416462+0.004052 
[86]	train-rmse:0.416355+0.001005	test-rmse:0.416449+0.004050 
[87]	train-rmse:0.416338+0.001005	test-rmse:0.416433+0.004050 
[88]	train-rmse:0.416320+0.001005	test-rmse:0.416418+0.004050 
[89]	train-rmse:0.416310+0.001003	test-rmse:0.416408+0.004052 
[90]	train-rmse:0.416292+0.001002	test-rmse:0.416392+0.004052 
[91]	train-rmse:0.416275+0.001002	test-rmse:0.416376+0.004051 
[92]	train-rmse:0.416257+0.001002	test-rmse:0.416360+0.004051 
[93]	train-rmse:0.416240+0.001002	test-rmse:0.416344+0.004051 
[94]	train-rmse:0.416223+0.001002	test-rmse:0.416328+0.004051 
[95]	train-rmse:0.416208+0.001004	test-rmse:0.416316+0.004048 
[96]	train-rmse:0.416194+0.001004	test-rmse:0.416302+0.004048 
[97]	train-rmse:0.416177+0.001004	test-rmse:0.416287+0.004048 
[98]	train-rmse:0.416160+0.001004	test-rmse:0.416271+0.004048 
[99]	train-rmse:0.416146+0.001006	test-rmse:0.416258+0.004046 
[100]	train-rmse:0.416129+0.001006	test-rmse:0.416242+0.004045 
[101]	train-rmse:0.416111+0.001006	test-rmse:0.416227+0.004045 
[102]	train-rmse:0.416094+0.001006	test-rmse:0.416211+0.004045 
[103]	train-rmse:0.416077+0.001006	test-rmse:0.416195+0.004045 
[104]	train-rmse:0.416063+0.001010	test-rmse:0.416182+0.004041 
[105]	train-rmse:0.416049+0.001004	test-rmse:0.416170+0.004047 
[106]	train-rmse:0.416031+0.001003	test-rmse:0.416154+0.004046 
[107]	train-rmse:0.416017+0.001003	test-rmse:0.416141+0.004046 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
