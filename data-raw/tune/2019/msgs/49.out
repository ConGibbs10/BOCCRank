> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.390665+0.000781	test-rmse:0.394062+0.004064 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.366097+0.000815	test-rmse:0.372426+0.004390 
[3]	train-rmse:0.344130+0.000946	test-rmse:0.353739+0.004791 
[4]	train-rmse:0.324245+0.001681	test-rmse:0.337462+0.005199 
[5]	train-rmse:0.305785+0.001360	test-rmse:0.322996+0.006084 
[6]	train-rmse:0.290545+0.004933	test-rmse:0.311258+0.005589 
[7]	train-rmse:0.276900+0.005109	test-rmse:0.300664+0.007214 
[8]	train-rmse:0.264427+0.004435	test-rmse:0.291502+0.007601 
[9]	train-rmse:0.251544+0.004134	test-rmse:0.281981+0.007740 
[10]	train-rmse:0.240297+0.004289	test-rmse:0.274299+0.006683 
[11]	train-rmse:0.228938+0.004540	test-rmse:0.266932+0.007362 
[12]	train-rmse:0.220406+0.006594	test-rmse:0.261619+0.009173 
[13]	train-rmse:0.211807+0.007666	test-rmse:0.256243+0.008858 
[14]	train-rmse:0.203227+0.007540	test-rmse:0.251424+0.009445 
[15]	train-rmse:0.194810+0.007245	test-rmse:0.246786+0.010105 
[16]	train-rmse:0.187213+0.007042	test-rmse:0.242929+0.010493 
[17]	train-rmse:0.181340+0.008426	test-rmse:0.240173+0.010494 
[18]	train-rmse:0.177330+0.006280	test-rmse:0.238020+0.010960 
[19]	train-rmse:0.171757+0.005395	test-rmse:0.234969+0.011504 
[20]	train-rmse:0.168093+0.006159	test-rmse:0.233229+0.011322 
[21]	train-rmse:0.163263+0.005684	test-rmse:0.231217+0.012110 
[22]	train-rmse:0.159615+0.005441	test-rmse:0.229748+0.012463 
[23]	train-rmse:0.156494+0.004238	test-rmse:0.228662+0.012495 
[24]	train-rmse:0.152551+0.004258	test-rmse:0.227228+0.012243 
[25]	train-rmse:0.148834+0.005437	test-rmse:0.226301+0.012169 
[26]	train-rmse:0.146113+0.005126	test-rmse:0.225621+0.012388 
[27]	train-rmse:0.142490+0.005629	test-rmse:0.224371+0.012395 
[28]	train-rmse:0.140241+0.005728	test-rmse:0.223798+0.012779 
[29]	train-rmse:0.136748+0.005494	test-rmse:0.222622+0.012745 
[30]	train-rmse:0.133762+0.004442	test-rmse:0.221677+0.013196 
[31]	train-rmse:0.130871+0.005005	test-rmse:0.221265+0.013011 
[32]	train-rmse:0.129281+0.005285	test-rmse:0.220794+0.013174 
[33]	train-rmse:0.126508+0.004603	test-rmse:0.220056+0.013268 
[34]	train-rmse:0.124117+0.005363	test-rmse:0.219627+0.013398 
[35]	train-rmse:0.121680+0.004573	test-rmse:0.219221+0.013568 
[36]	train-rmse:0.119293+0.003932	test-rmse:0.218782+0.013707 
[37]	train-rmse:0.117629+0.004745	test-rmse:0.218720+0.013804 
[38]	train-rmse:0.116292+0.004871	test-rmse:0.218337+0.013963 
[39]	train-rmse:0.113884+0.004906	test-rmse:0.217978+0.014092 
[40]	train-rmse:0.111917+0.005025	test-rmse:0.217681+0.014207 
[41]	train-rmse:0.109685+0.005146	test-rmse:0.217406+0.014328 
[42]	train-rmse:0.108210+0.005206	test-rmse:0.217247+0.014288 
[43]	train-rmse:0.106941+0.005220	test-rmse:0.217167+0.014369 
[44]	train-rmse:0.105873+0.005037	test-rmse:0.217171+0.014466 
[45]	train-rmse:0.104955+0.004821	test-rmse:0.217068+0.014310 
[46]	train-rmse:0.103729+0.005382	test-rmse:0.216858+0.014233 
[47]	train-rmse:0.102046+0.005746	test-rmse:0.216734+0.014364 
[48]	train-rmse:0.101571+0.005921	test-rmse:0.216688+0.014358 
[49]	train-rmse:0.100257+0.005674	test-rmse:0.216528+0.014258 
[50]	train-rmse:0.098702+0.005749	test-rmse:0.216275+0.014283 
[51]	train-rmse:0.097181+0.005427	test-rmse:0.216202+0.014252 
[52]	train-rmse:0.096677+0.005821	test-rmse:0.216222+0.014242 
[53]	train-rmse:0.095881+0.005696	test-rmse:0.216282+0.014305 
[54]	train-rmse:0.094824+0.005688	test-rmse:0.216244+0.014244 
[55]	train-rmse:0.094177+0.006273	test-rmse:0.216254+0.014244 
[56]	train-rmse:0.092714+0.005761	test-rmse:0.216083+0.014350 
[57]	train-rmse:0.092255+0.006125	test-rmse:0.215995+0.014298 
[58]	train-rmse:0.091167+0.005345	test-rmse:0.216120+0.014236 
[59]	train-rmse:0.090455+0.005804	test-rmse:0.216014+0.014297 
[60]	train-rmse:0.089417+0.005315	test-rmse:0.215864+0.014374 
[61]	train-rmse:0.087901+0.005242	test-rmse:0.215678+0.014682 
[62]	train-rmse:0.087291+0.005546	test-rmse:0.215704+0.014539 
[63]	train-rmse:0.085797+0.005213	test-rmse:0.215627+0.014722 
[64]	train-rmse:0.084719+0.005210	test-rmse:0.215522+0.014656 
[65]	train-rmse:0.084113+0.005089	test-rmse:0.215388+0.014653 
[66]	train-rmse:0.083015+0.005265	test-rmse:0.215432+0.014753 
[67]	train-rmse:0.081931+0.005517	test-rmse:0.215509+0.014849 
[68]	train-rmse:0.081010+0.005420	test-rmse:0.215369+0.014801 
[69]	train-rmse:0.080139+0.005026	test-rmse:0.215281+0.014799 
[70]	train-rmse:0.079498+0.004448	test-rmse:0.215402+0.014749 
[71]	train-rmse:0.078868+0.004730	test-rmse:0.215380+0.014728 
[72]	train-rmse:0.078218+0.004727	test-rmse:0.215329+0.014803 
[73]	train-rmse:0.077353+0.004766	test-rmse:0.215209+0.014788 
[74]	train-rmse:0.076806+0.005186	test-rmse:0.215283+0.014693 
[75]	train-rmse:0.076302+0.005151	test-rmse:0.215286+0.014767 
[76]	train-rmse:0.075705+0.005505	test-rmse:0.215338+0.014819 
[77]	train-rmse:0.075016+0.005465	test-rmse:0.215342+0.014825 
[78]	train-rmse:0.074309+0.005469	test-rmse:0.215422+0.014949 
[79]	train-rmse:0.073355+0.005718	test-rmse:0.215338+0.015026 
[80]	train-rmse:0.073043+0.005622	test-rmse:0.215222+0.015028 
[81]	train-rmse:0.072520+0.005632	test-rmse:0.215213+0.015082 
[82]	train-rmse:0.072116+0.006081	test-rmse:0.215249+0.015116 
[83]	train-rmse:0.071380+0.006129	test-rmse:0.215386+0.015088 
[84]	train-rmse:0.070794+0.005938	test-rmse:0.215390+0.015111 
[85]	train-rmse:0.070246+0.005679	test-rmse:0.215524+0.015135 
[86]	train-rmse:0.069777+0.005560	test-rmse:0.215497+0.015193 
[87]	train-rmse:0.069373+0.005565	test-rmse:0.215547+0.015221 
[88]	train-rmse:0.068928+0.005647	test-rmse:0.215566+0.015187 
[89]	train-rmse:0.068810+0.005776	test-rmse:0.215563+0.015199 
[90]	train-rmse:0.068197+0.005631	test-rmse:0.215680+0.015150 
[91]	train-rmse:0.067432+0.005746	test-rmse:0.215745+0.015417 
[92]	train-rmse:0.066942+0.006010	test-rmse:0.215697+0.015378 
[93]	train-rmse:0.066311+0.006122	test-rmse:0.215685+0.015384 
[94]	train-rmse:0.065657+0.006188	test-rmse:0.215774+0.015413 
[95]	train-rmse:0.065102+0.006438	test-rmse:0.215772+0.015470 
[96]	train-rmse:0.064698+0.006506	test-rmse:0.215820+0.015457 
[97]	train-rmse:0.064154+0.006275	test-rmse:0.215940+0.015435 
[98]	train-rmse:0.063549+0.006039	test-rmse:0.216052+0.015358 
[99]	train-rmse:0.063080+0.006315	test-rmse:0.216057+0.015375 
[100]	train-rmse:0.062608+0.006237	test-rmse:0.216190+0.015223 
[101]	train-rmse:0.062064+0.006079	test-rmse:0.216293+0.015240 
[102]	train-rmse:0.061378+0.006126	test-rmse:0.216329+0.015258 
[103]	train-rmse:0.061044+0.006192	test-rmse:0.216382+0.015307 
[104]	train-rmse:0.060479+0.005873	test-rmse:0.216412+0.015255 
[105]	train-rmse:0.060066+0.005849	test-rmse:0.216465+0.015189 
[106]	train-rmse:0.059778+0.005838	test-rmse:0.216468+0.015216 
[107]	train-rmse:0.059280+0.005869	test-rmse:0.216482+0.015259 
[108]	train-rmse:0.058786+0.006121	test-rmse:0.216514+0.015290 
[109]	train-rmse:0.058087+0.005723	test-rmse:0.216526+0.015282 
[110]	train-rmse:0.057673+0.006001	test-rmse:0.216564+0.015320 
[111]	train-rmse:0.057060+0.005845	test-rmse:0.216557+0.015282 
[112]	train-rmse:0.056294+0.005791	test-rmse:0.216670+0.015355 
[113]	train-rmse:0.055681+0.005556	test-rmse:0.216612+0.015446 
[114]	train-rmse:0.055472+0.005650	test-rmse:0.216659+0.015505 
[115]	train-rmse:0.055133+0.005908	test-rmse:0.216612+0.015502 
[116]	train-rmse:0.054443+0.005747	test-rmse:0.216614+0.015580 
[117]	train-rmse:0.053910+0.005517	test-rmse:0.216526+0.015547 
[118]	train-rmse:0.053532+0.005325	test-rmse:0.216626+0.015420 
[119]	train-rmse:0.053354+0.005250	test-rmse:0.216575+0.015473 
[120]	train-rmse:0.053116+0.005034	test-rmse:0.216586+0.015477 
[121]	train-rmse:0.052770+0.004829	test-rmse:0.216746+0.015382 
[122]	train-rmse:0.052582+0.005009	test-rmse:0.216721+0.015342 
[123]	train-rmse:0.052201+0.004837	test-rmse:0.216745+0.015320 
Stopping. Best iteration:
[73]	train-rmse:0.077353+0.004766	test-rmse:0.215209+0.014788

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
