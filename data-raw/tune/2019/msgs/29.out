> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.404527+0.000797	test-rmse:0.405192+0.004056 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.392116+0.000735	test-rmse:0.393443+0.003762 
[3]	train-rmse:0.380265+0.000850	test-rmse:0.382445+0.003638 
[4]	train-rmse:0.369132+0.000755	test-rmse:0.372131+0.003625 
[5]	train-rmse:0.358367+0.000839	test-rmse:0.362142+0.003824 
[6]	train-rmse:0.348384+0.001134	test-rmse:0.352755+0.003999 
[7]	train-rmse:0.338674+0.001272	test-rmse:0.343767+0.004273 
[8]	train-rmse:0.329404+0.001522	test-rmse:0.335405+0.004537 
[9]	train-rmse:0.320732+0.001726	test-rmse:0.327902+0.004688 
[10]	train-rmse:0.313213+0.001971	test-rmse:0.321195+0.006000 
[11]	train-rmse:0.305204+0.001815	test-rmse:0.314063+0.006491 
[12]	train-rmse:0.297717+0.001894	test-rmse:0.307396+0.006600 
[13]	train-rmse:0.290604+0.001902	test-rmse:0.301085+0.006829 
[14]	train-rmse:0.283865+0.002065	test-rmse:0.295389+0.007155 
[15]	train-rmse:0.277568+0.002258	test-rmse:0.290012+0.007527 
[16]	train-rmse:0.271503+0.002201	test-rmse:0.284913+0.008050 
[17]	train-rmse:0.266440+0.002341	test-rmse:0.280716+0.008746 
[18]	train-rmse:0.260840+0.002287	test-rmse:0.275935+0.009314 
[19]	train-rmse:0.255734+0.002435	test-rmse:0.271810+0.009561 
[20]	train-rmse:0.250800+0.002373	test-rmse:0.267911+0.009868 
[21]	train-rmse:0.246753+0.002804	test-rmse:0.264563+0.010841 
[22]	train-rmse:0.242394+0.002801	test-rmse:0.261120+0.011208 
[23]	train-rmse:0.238192+0.002719	test-rmse:0.257883+0.011796 
[24]	train-rmse:0.234278+0.002710	test-rmse:0.254863+0.012047 
[25]	train-rmse:0.230992+0.002699	test-rmse:0.252354+0.012797 
[26]	train-rmse:0.227334+0.002770	test-rmse:0.249956+0.013059 
[27]	train-rmse:0.224413+0.003078	test-rmse:0.247882+0.013304 
[28]	train-rmse:0.221678+0.003591	test-rmse:0.245846+0.012923 
[29]	train-rmse:0.218625+0.003817	test-rmse:0.243854+0.013234 
[30]	train-rmse:0.215546+0.003819	test-rmse:0.241872+0.013499 
[31]	train-rmse:0.213142+0.003826	test-rmse:0.240232+0.013971 
[32]	train-rmse:0.210635+0.003982	test-rmse:0.238616+0.014321 
[33]	train-rmse:0.208350+0.004112	test-rmse:0.236984+0.014752 
[34]	train-rmse:0.206263+0.004420	test-rmse:0.235620+0.014977 
[35]	train-rmse:0.203907+0.004470	test-rmse:0.234112+0.015342 
[36]	train-rmse:0.201672+0.004458	test-rmse:0.232831+0.015566 
[37]	train-rmse:0.199923+0.004805	test-rmse:0.231842+0.015787 
[38]	train-rmse:0.198124+0.005070	test-rmse:0.230693+0.016154 
[39]	train-rmse:0.196448+0.005373	test-rmse:0.229742+0.016189 
[40]	train-rmse:0.194473+0.005198	test-rmse:0.228615+0.016458 
[41]	train-rmse:0.192626+0.005273	test-rmse:0.227550+0.016319 
[42]	train-rmse:0.190740+0.005181	test-rmse:0.226709+0.016360 
[43]	train-rmse:0.189108+0.005198	test-rmse:0.225929+0.016663 
[44]	train-rmse:0.187862+0.004926	test-rmse:0.225254+0.016628 
[45]	train-rmse:0.186294+0.004897	test-rmse:0.224603+0.016564 
[46]	train-rmse:0.184848+0.004806	test-rmse:0.224048+0.016762 
[47]	train-rmse:0.183633+0.004792	test-rmse:0.223553+0.016806 
[48]	train-rmse:0.182232+0.004723	test-rmse:0.223242+0.016909 
[49]	train-rmse:0.180877+0.004734	test-rmse:0.222760+0.016974 
[50]	train-rmse:0.179571+0.004463	test-rmse:0.222325+0.017012 
[51]	train-rmse:0.178370+0.004423	test-rmse:0.221864+0.017068 
[52]	train-rmse:0.177040+0.004392	test-rmse:0.221478+0.017195 
[53]	train-rmse:0.176101+0.004168	test-rmse:0.221130+0.017311 
[54]	train-rmse:0.174868+0.004210	test-rmse:0.220607+0.017219 
[55]	train-rmse:0.173693+0.004051	test-rmse:0.220256+0.017267 
[56]	train-rmse:0.172777+0.003962	test-rmse:0.219998+0.017196 
[57]	train-rmse:0.172013+0.004041	test-rmse:0.219636+0.017241 
[58]	train-rmse:0.171109+0.004075	test-rmse:0.219306+0.017160 
[59]	train-rmse:0.170448+0.004103	test-rmse:0.219149+0.017158 
[60]	train-rmse:0.169475+0.003847	test-rmse:0.218699+0.017302 
[61]	train-rmse:0.168665+0.003952	test-rmse:0.218421+0.017534 
[62]	train-rmse:0.167830+0.003725	test-rmse:0.218223+0.017614 
[63]	train-rmse:0.167372+0.003822	test-rmse:0.218030+0.017565 
[64]	train-rmse:0.166427+0.003843	test-rmse:0.217871+0.017585 
[65]	train-rmse:0.165370+0.004071	test-rmse:0.217639+0.017607 
[66]	train-rmse:0.164742+0.004099	test-rmse:0.217523+0.017553 
[67]	train-rmse:0.163951+0.004290	test-rmse:0.217534+0.017517 
[68]	train-rmse:0.163163+0.004278	test-rmse:0.217335+0.017696 
[69]	train-rmse:0.162386+0.004200	test-rmse:0.217206+0.017802 
[70]	train-rmse:0.161773+0.003997	test-rmse:0.217022+0.017806 
[71]	train-rmse:0.160862+0.004060	test-rmse:0.216678+0.017948 
[72]	train-rmse:0.160350+0.003918	test-rmse:0.216561+0.018055 
[73]	train-rmse:0.159912+0.003862	test-rmse:0.216557+0.018089 
[74]	train-rmse:0.159637+0.003839	test-rmse:0.216419+0.018000 
[75]	train-rmse:0.158998+0.003605	test-rmse:0.216276+0.017909 
[76]	train-rmse:0.158385+0.003865	test-rmse:0.216278+0.018053 
[77]	train-rmse:0.157655+0.003777	test-rmse:0.216327+0.018060 
[78]	train-rmse:0.156940+0.003811	test-rmse:0.216294+0.018327 
[79]	train-rmse:0.156330+0.003847	test-rmse:0.216140+0.018544 
[80]	train-rmse:0.155630+0.003923	test-rmse:0.215979+0.018638 
[81]	train-rmse:0.154938+0.003995	test-rmse:0.215752+0.018597 
[82]	train-rmse:0.154562+0.003933	test-rmse:0.215756+0.018582 
[83]	train-rmse:0.154096+0.003709	test-rmse:0.215644+0.018582 
[84]	train-rmse:0.153570+0.003795	test-rmse:0.215509+0.018579 
[85]	train-rmse:0.152867+0.003762	test-rmse:0.215450+0.018545 
[86]	train-rmse:0.152128+0.003952	test-rmse:0.215440+0.018582 
[87]	train-rmse:0.151539+0.004028	test-rmse:0.215350+0.018581 
[88]	train-rmse:0.150803+0.003918	test-rmse:0.215267+0.018738 
[89]	train-rmse:0.150146+0.003958	test-rmse:0.215125+0.018690 
[90]	train-rmse:0.149808+0.003998	test-rmse:0.215372+0.018559 
[91]	train-rmse:0.149249+0.003941	test-rmse:0.215207+0.018645 
[92]	train-rmse:0.148590+0.003920	test-rmse:0.215260+0.018509 
[93]	train-rmse:0.147786+0.003885	test-rmse:0.214970+0.018641 
[94]	train-rmse:0.147161+0.003860	test-rmse:0.214957+0.018503 
[95]	train-rmse:0.146806+0.003641	test-rmse:0.215012+0.018444 
[96]	train-rmse:0.146266+0.003804	test-rmse:0.214903+0.018483 
[97]	train-rmse:0.145808+0.003823	test-rmse:0.214842+0.018615 
[98]	train-rmse:0.145413+0.003919	test-rmse:0.214738+0.018568 
[99]	train-rmse:0.144898+0.003856	test-rmse:0.214627+0.018512 
[100]	train-rmse:0.144363+0.003687	test-rmse:0.214614+0.018412 
[101]	train-rmse:0.143905+0.003688	test-rmse:0.214700+0.018437 
[102]	train-rmse:0.143407+0.003665	test-rmse:0.214709+0.018480 
[103]	train-rmse:0.142792+0.003563	test-rmse:0.214567+0.018514 
[104]	train-rmse:0.142288+0.003510	test-rmse:0.214641+0.018240 
[105]	train-rmse:0.141752+0.003632	test-rmse:0.214691+0.018187 
[106]	train-rmse:0.141157+0.003613	test-rmse:0.214687+0.018195 
[107]	train-rmse:0.140667+0.003701	test-rmse:0.214651+0.018286 
[108]	train-rmse:0.140368+0.003771	test-rmse:0.214829+0.018225 
[109]	train-rmse:0.139890+0.003885	test-rmse:0.214901+0.018285 
[110]	train-rmse:0.139303+0.003943	test-rmse:0.214898+0.018129 
[111]	train-rmse:0.138730+0.003988	test-rmse:0.215021+0.018088 
[112]	train-rmse:0.138101+0.003843	test-rmse:0.214861+0.018171 
[113]	train-rmse:0.137366+0.003793	test-rmse:0.214961+0.018288 
[114]	train-rmse:0.136857+0.003748	test-rmse:0.214782+0.018393 
[115]	train-rmse:0.136356+0.003751	test-rmse:0.214905+0.018470 
[116]	train-rmse:0.135718+0.003825	test-rmse:0.214801+0.018408 
[117]	train-rmse:0.135379+0.003629	test-rmse:0.214719+0.018345 
[118]	train-rmse:0.134883+0.003648	test-rmse:0.214796+0.018383 
[119]	train-rmse:0.134449+0.003652	test-rmse:0.214582+0.018499 
[120]	train-rmse:0.133972+0.003714	test-rmse:0.214431+0.018287 
[121]	train-rmse:0.133467+0.003795	test-rmse:0.214392+0.018304 
[122]	train-rmse:0.132999+0.003782	test-rmse:0.214403+0.018330 
[123]	train-rmse:0.132617+0.003933	test-rmse:0.214331+0.018432 
[124]	train-rmse:0.132077+0.003941	test-rmse:0.214367+0.018485 
[125]	train-rmse:0.131620+0.003985	test-rmse:0.214372+0.018501 
[126]	train-rmse:0.131216+0.003845	test-rmse:0.214443+0.018482 
[127]	train-rmse:0.130860+0.003874	test-rmse:0.214301+0.018481 
[128]	train-rmse:0.130400+0.004064	test-rmse:0.214206+0.018470 
[129]	train-rmse:0.129983+0.004009	test-rmse:0.214090+0.018582 
[130]	train-rmse:0.129661+0.004039	test-rmse:0.214083+0.018546 
[131]	train-rmse:0.129348+0.004077	test-rmse:0.214087+0.018516 
[132]	train-rmse:0.128761+0.004083	test-rmse:0.213950+0.018549 
[133]	train-rmse:0.128218+0.004006	test-rmse:0.213906+0.018466 
[134]	train-rmse:0.127828+0.004122	test-rmse:0.213977+0.018480 
[135]	train-rmse:0.127555+0.004190	test-rmse:0.213897+0.018454 
[136]	train-rmse:0.127300+0.004179	test-rmse:0.213825+0.018370 
[137]	train-rmse:0.126971+0.004118	test-rmse:0.213677+0.018285 
[138]	train-rmse:0.126663+0.004180	test-rmse:0.213557+0.018117 
[139]	train-rmse:0.126056+0.004149	test-rmse:0.213558+0.018227 
[140]	train-rmse:0.125638+0.004111	test-rmse:0.213456+0.018215 
[141]	train-rmse:0.125189+0.004062	test-rmse:0.213281+0.018137 
[142]	train-rmse:0.124758+0.004100	test-rmse:0.213290+0.018323 
[143]	train-rmse:0.124431+0.004320	test-rmse:0.213344+0.018445 
[144]	train-rmse:0.124030+0.004360	test-rmse:0.213461+0.018378 
[145]	train-rmse:0.123589+0.004289	test-rmse:0.213530+0.018297 
[146]	train-rmse:0.123112+0.004211	test-rmse:0.213550+0.018357 
[147]	train-rmse:0.122784+0.004318	test-rmse:0.213595+0.018380 
[148]	train-rmse:0.122414+0.004377	test-rmse:0.213608+0.018328 
[149]	train-rmse:0.122008+0.004479	test-rmse:0.213582+0.018354 
[150]	train-rmse:0.121691+0.004424	test-rmse:0.213541+0.018375 
[151]	train-rmse:0.121343+0.004503	test-rmse:0.213642+0.018397 
[152]	train-rmse:0.120905+0.004452	test-rmse:0.213556+0.018362 
[153]	train-rmse:0.120502+0.004562	test-rmse:0.213620+0.018393 
[154]	train-rmse:0.120081+0.004686	test-rmse:0.213527+0.018366 
[155]	train-rmse:0.119698+0.004703	test-rmse:0.213619+0.018354 
[156]	train-rmse:0.119358+0.004806	test-rmse:0.213607+0.018328 
[157]	train-rmse:0.118917+0.004816	test-rmse:0.213613+0.018326 
[158]	train-rmse:0.118454+0.004782	test-rmse:0.213741+0.018418 
[159]	train-rmse:0.118116+0.004764	test-rmse:0.213779+0.018469 
[160]	train-rmse:0.117818+0.004655	test-rmse:0.213753+0.018494 
[161]	train-rmse:0.117462+0.004846	test-rmse:0.213787+0.018414 
[162]	train-rmse:0.116958+0.004827	test-rmse:0.213754+0.018359 
[163]	train-rmse:0.116521+0.004631	test-rmse:0.213778+0.018340 
[164]	train-rmse:0.116327+0.004773	test-rmse:0.213746+0.018296 
[165]	train-rmse:0.115907+0.004851	test-rmse:0.213857+0.018352 
[166]	train-rmse:0.115597+0.004950	test-rmse:0.213976+0.018381 
[167]	train-rmse:0.115311+0.004996	test-rmse:0.213999+0.018457 
[168]	train-rmse:0.114895+0.005082	test-rmse:0.214056+0.018503 
[169]	train-rmse:0.114472+0.005118	test-rmse:0.214119+0.018551 
[170]	train-rmse:0.114023+0.005160	test-rmse:0.214091+0.018494 
[171]	train-rmse:0.113666+0.004971	test-rmse:0.214016+0.018542 
[172]	train-rmse:0.113237+0.004938	test-rmse:0.214070+0.018390 
[173]	train-rmse:0.112983+0.004876	test-rmse:0.213990+0.018153 
[174]	train-rmse:0.112706+0.004906	test-rmse:0.214092+0.018014 
[175]	train-rmse:0.112329+0.004920	test-rmse:0.214123+0.018145 
[176]	train-rmse:0.111980+0.004771	test-rmse:0.214075+0.018162 
[177]	train-rmse:0.111795+0.004764	test-rmse:0.214081+0.018221 
[178]	train-rmse:0.111394+0.004745	test-rmse:0.214177+0.018397 
[179]	train-rmse:0.111025+0.004729	test-rmse:0.214192+0.018349 
[180]	train-rmse:0.110626+0.004579	test-rmse:0.214108+0.018400 
[181]	train-rmse:0.110194+0.004566	test-rmse:0.214132+0.018500 
[182]	train-rmse:0.109921+0.004523	test-rmse:0.214260+0.018416 
[183]	train-rmse:0.109637+0.004523	test-rmse:0.214175+0.018598 
[184]	train-rmse:0.109379+0.004334	test-rmse:0.214071+0.018685 
[185]	train-rmse:0.109033+0.004332	test-rmse:0.214032+0.018724 
[186]	train-rmse:0.108716+0.004215	test-rmse:0.214095+0.018829 
[187]	train-rmse:0.108339+0.004255	test-rmse:0.214088+0.018853 
[188]	train-rmse:0.108048+0.004330	test-rmse:0.214126+0.018885 
[189]	train-rmse:0.107712+0.004261	test-rmse:0.214038+0.018813 
[190]	train-rmse:0.107476+0.004223	test-rmse:0.214043+0.018777 
[191]	train-rmse:0.107040+0.004192	test-rmse:0.214070+0.018773 
Stopping. Best iteration:
[141]	train-rmse:0.125189+0.004062	test-rmse:0.213281+0.018137

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
