> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.382435+0.001053	test-rmse:0.382434+0.004183 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.382190+0.001051	test-rmse:0.382214+0.004179 
[3]	train-rmse:0.381945+0.001050	test-rmse:0.381990+0.004175 
[4]	train-rmse:0.381702+0.001048	test-rmse:0.381773+0.004172 
[5]	train-rmse:0.381457+0.001047	test-rmse:0.381548+0.004169 
[6]	train-rmse:0.381213+0.001046	test-rmse:0.381328+0.004165 
[7]	train-rmse:0.380968+0.001044	test-rmse:0.381107+0.004162 
[8]	train-rmse:0.380726+0.001043	test-rmse:0.380889+0.004160 
[9]	train-rmse:0.380482+0.001041	test-rmse:0.380667+0.004158 
[10]	train-rmse:0.380240+0.001038	test-rmse:0.380448+0.004154 
[11]	train-rmse:0.379998+0.001037	test-rmse:0.380227+0.004152 
[12]	train-rmse:0.379754+0.001035	test-rmse:0.380008+0.004148 
[13]	train-rmse:0.379511+0.001035	test-rmse:0.379785+0.004145 
[14]	train-rmse:0.379271+0.001033	test-rmse:0.379567+0.004144 
[15]	train-rmse:0.379030+0.001033	test-rmse:0.379346+0.004144 
[16]	train-rmse:0.378788+0.001031	test-rmse:0.379125+0.004141 
[17]	train-rmse:0.378547+0.001029	test-rmse:0.378906+0.004139 
[18]	train-rmse:0.378304+0.001028	test-rmse:0.378686+0.004136 
[19]	train-rmse:0.378063+0.001025	test-rmse:0.378469+0.004134 
[20]	train-rmse:0.377822+0.001025	test-rmse:0.378251+0.004133 
[21]	train-rmse:0.377582+0.001024	test-rmse:0.378033+0.004130 
[22]	train-rmse:0.377340+0.001023	test-rmse:0.377815+0.004124 
[23]	train-rmse:0.377099+0.001021	test-rmse:0.377598+0.004123 
[24]	train-rmse:0.376860+0.001020	test-rmse:0.377380+0.004120 
[25]	train-rmse:0.376620+0.001018	test-rmse:0.377160+0.004118 
[26]	train-rmse:0.376379+0.001017	test-rmse:0.376942+0.004115 
[27]	train-rmse:0.376140+0.001016	test-rmse:0.376724+0.004111 
[28]	train-rmse:0.375900+0.001014	test-rmse:0.376506+0.004106 
[29]	train-rmse:0.375661+0.001013	test-rmse:0.376292+0.004101 
[30]	train-rmse:0.375422+0.001012	test-rmse:0.376074+0.004099 
[31]	train-rmse:0.375181+0.001011	test-rmse:0.375856+0.004096 
[32]	train-rmse:0.374944+0.001009	test-rmse:0.375639+0.004095 
[33]	train-rmse:0.374706+0.001006	test-rmse:0.375425+0.004094 
[34]	train-rmse:0.374469+0.001003	test-rmse:0.375210+0.004092 
[35]	train-rmse:0.374231+0.001002	test-rmse:0.374992+0.004090 
[36]	train-rmse:0.373992+0.001001	test-rmse:0.374777+0.004088 
[37]	train-rmse:0.373755+0.001000	test-rmse:0.374561+0.004081 
[38]	train-rmse:0.373517+0.001000	test-rmse:0.374344+0.004080 
[39]	train-rmse:0.373280+0.000998	test-rmse:0.374129+0.004076 
[40]	train-rmse:0.373043+0.000996	test-rmse:0.373913+0.004074 
[41]	train-rmse:0.372807+0.000995	test-rmse:0.373702+0.004074 
[42]	train-rmse:0.372593+0.001003	test-rmse:0.373509+0.004064 
[43]	train-rmse:0.372357+0.001001	test-rmse:0.373296+0.004061 
[44]	train-rmse:0.372122+0.001000	test-rmse:0.373084+0.004057 
[45]	train-rmse:0.371885+0.000999	test-rmse:0.372870+0.004050 
[46]	train-rmse:0.371651+0.000996	test-rmse:0.372658+0.004047 
[47]	train-rmse:0.371415+0.000993	test-rmse:0.372445+0.004048 
[48]	train-rmse:0.371179+0.000992	test-rmse:0.372233+0.004047 
[49]	train-rmse:0.370945+0.000990	test-rmse:0.372024+0.004042 
[50]	train-rmse:0.370710+0.000990	test-rmse:0.371811+0.004037 
[51]	train-rmse:0.370475+0.000987	test-rmse:0.371601+0.004035 
[52]	train-rmse:0.370242+0.000986	test-rmse:0.371389+0.004030 
[53]	train-rmse:0.370007+0.000986	test-rmse:0.371177+0.004026 
[54]	train-rmse:0.369773+0.000985	test-rmse:0.370966+0.004022 
[55]	train-rmse:0.369540+0.000984	test-rmse:0.370756+0.004019 
[56]	train-rmse:0.369306+0.000983	test-rmse:0.370545+0.004016 
[57]	train-rmse:0.369073+0.000982	test-rmse:0.370331+0.004013 
[58]	train-rmse:0.368840+0.000980	test-rmse:0.370121+0.004011 
[59]	train-rmse:0.368606+0.000978	test-rmse:0.369910+0.004007 
[60]	train-rmse:0.368374+0.000975	test-rmse:0.369700+0.004006 
[61]	train-rmse:0.368142+0.000975	test-rmse:0.369491+0.004002 
[62]	train-rmse:0.367909+0.000973	test-rmse:0.369280+0.003999 
[63]	train-rmse:0.367680+0.000973	test-rmse:0.369073+0.003996 
[64]	train-rmse:0.367447+0.000971	test-rmse:0.368863+0.003996 
[65]	train-rmse:0.367214+0.000970	test-rmse:0.368655+0.003992 
[66]	train-rmse:0.366983+0.000969	test-rmse:0.368445+0.003987 
[67]	train-rmse:0.366752+0.000968	test-rmse:0.368235+0.003984 
[68]	train-rmse:0.366522+0.000968	test-rmse:0.368028+0.003981 
[69]	train-rmse:0.366291+0.000966	test-rmse:0.367821+0.003979 
[70]	train-rmse:0.366060+0.000964	test-rmse:0.367615+0.003976 
[71]	train-rmse:0.365831+0.000963	test-rmse:0.367405+0.003977 
[72]	train-rmse:0.365599+0.000960	test-rmse:0.367198+0.003974 
[73]	train-rmse:0.365368+0.000960	test-rmse:0.366989+0.003970 
[74]	train-rmse:0.365139+0.000960	test-rmse:0.366784+0.003967 
[75]	train-rmse:0.364932+0.000971	test-rmse:0.366595+0.003956 
[76]	train-rmse:0.364703+0.000968	test-rmse:0.366389+0.003953 
[77]	train-rmse:0.364474+0.000968	test-rmse:0.366187+0.003950 
[78]	train-rmse:0.364247+0.000966	test-rmse:0.365981+0.003946 
[79]	train-rmse:0.364018+0.000964	test-rmse:0.365774+0.003944 
[80]	train-rmse:0.363792+0.000962	test-rmse:0.365570+0.003943 
[81]	train-rmse:0.363563+0.000961	test-rmse:0.365365+0.003941 
[82]	train-rmse:0.363334+0.000959	test-rmse:0.365162+0.003937 
[83]	train-rmse:0.363105+0.000957	test-rmse:0.364957+0.003934 
[84]	train-rmse:0.362877+0.000956	test-rmse:0.364750+0.003930 
[85]	train-rmse:0.362649+0.000956	test-rmse:0.364545+0.003927 
[86]	train-rmse:0.362421+0.000956	test-rmse:0.364342+0.003923 
[87]	train-rmse:0.362193+0.000954	test-rmse:0.364135+0.003919 
[88]	train-rmse:0.361966+0.000954	test-rmse:0.363930+0.003916 
[89]	train-rmse:0.361740+0.000952	test-rmse:0.363726+0.003916 
[90]	train-rmse:0.361514+0.000950	test-rmse:0.363523+0.003914 
[91]	train-rmse:0.361286+0.000948	test-rmse:0.363319+0.003913 
[92]	train-rmse:0.361059+0.000948	test-rmse:0.363116+0.003908 
[93]	train-rmse:0.360831+0.000947	test-rmse:0.362911+0.003904 
[94]	train-rmse:0.360604+0.000947	test-rmse:0.362708+0.003901 
[95]	train-rmse:0.360378+0.000945	test-rmse:0.362507+0.003899 
[96]	train-rmse:0.360175+0.000958	test-rmse:0.362324+0.003889 
[97]	train-rmse:0.359950+0.000955	test-rmse:0.362121+0.003888 
[98]	train-rmse:0.359724+0.000952	test-rmse:0.361920+0.003886 
[99]	train-rmse:0.359498+0.000950	test-rmse:0.361715+0.003884 
[100]	train-rmse:0.359296+0.000908	test-rmse:0.361536+0.003919 
[101]	train-rmse:0.359071+0.000907	test-rmse:0.361336+0.003917 
[102]	train-rmse:0.358846+0.000907	test-rmse:0.361133+0.003916 
[103]	train-rmse:0.358620+0.000905	test-rmse:0.360934+0.003914 
[104]	train-rmse:0.358397+0.000903	test-rmse:0.360735+0.003912 
[105]	train-rmse:0.358201+0.000847	test-rmse:0.360561+0.003960 
[106]	train-rmse:0.357977+0.000846	test-rmse:0.360360+0.003958 
[107]	train-rmse:0.357753+0.000844	test-rmse:0.360158+0.003954 
[108]	train-rmse:0.357529+0.000843	test-rmse:0.359959+0.003955 
[109]	train-rmse:0.357306+0.000843	test-rmse:0.359762+0.003950 
[110]	train-rmse:0.357106+0.000838	test-rmse:0.359582+0.003955 
[111]	train-rmse:0.356884+0.000836	test-rmse:0.359385+0.003951 
[112]	train-rmse:0.356661+0.000834	test-rmse:0.359185+0.003950 
[113]	train-rmse:0.356438+0.000832	test-rmse:0.358986+0.003947 
[114]	train-rmse:0.356214+0.000830	test-rmse:0.358786+0.003944 
[115]	train-rmse:0.355991+0.000829	test-rmse:0.358584+0.003942 
[116]	train-rmse:0.355769+0.000829	test-rmse:0.358386+0.003941 
[117]	train-rmse:0.355546+0.000829	test-rmse:0.358187+0.003938 
[118]	train-rmse:0.355324+0.000827	test-rmse:0.357987+0.003935 
[119]	train-rmse:0.355102+0.000827	test-rmse:0.357789+0.003931 
[120]	train-rmse:0.354881+0.000826	test-rmse:0.357592+0.003930 
[121]	train-rmse:0.354660+0.000826	test-rmse:0.357393+0.003927 
[122]	train-rmse:0.354439+0.000824	test-rmse:0.357199+0.003925 
[123]	train-rmse:0.354240+0.000784	test-rmse:0.357024+0.003957 
[124]	train-rmse:0.354041+0.000799	test-rmse:0.356847+0.003944 
[125]	train-rmse:0.353822+0.000798	test-rmse:0.356650+0.003943 
[126]	train-rmse:0.353601+0.000797	test-rmse:0.356451+0.003941 
[127]	train-rmse:0.353381+0.000796	test-rmse:0.356253+0.003938 
[128]	train-rmse:0.353184+0.000798	test-rmse:0.356077+0.003926 
[129]	train-rmse:0.352964+0.000797	test-rmse:0.355882+0.003922 
[130]	train-rmse:0.352765+0.000814	test-rmse:0.355705+0.003914 
[131]	train-rmse:0.352546+0.000812	test-rmse:0.355509+0.003912 
[132]	train-rmse:0.352325+0.000809	test-rmse:0.355315+0.003910 
[133]	train-rmse:0.352106+0.000809	test-rmse:0.355120+0.003908 
[134]	train-rmse:0.351888+0.000809	test-rmse:0.354927+0.003903 
[135]	train-rmse:0.351669+0.000808	test-rmse:0.354734+0.003903 
[136]	train-rmse:0.351450+0.000806	test-rmse:0.354540+0.003902 
[137]	train-rmse:0.351232+0.000803	test-rmse:0.354346+0.003904 
[138]	train-rmse:0.351015+0.000802	test-rmse:0.354154+0.003904 
[139]	train-rmse:0.350797+0.000801	test-rmse:0.353957+0.003902 
[140]	train-rmse:0.350580+0.000800	test-rmse:0.353762+0.003900 
[141]	train-rmse:0.350362+0.000798	test-rmse:0.353570+0.003899 
[142]	train-rmse:0.350146+0.000796	test-rmse:0.353378+0.003897 
[143]	train-rmse:0.349959+0.000802	test-rmse:0.353211+0.003885 
[144]	train-rmse:0.349743+0.000802	test-rmse:0.353018+0.003884 
[145]	train-rmse:0.349527+0.000801	test-rmse:0.352823+0.003883 
[146]	train-rmse:0.349311+0.000801	test-rmse:0.352629+0.003880 
[147]	train-rmse:0.349095+0.000801	test-rmse:0.352436+0.003876 
[148]	train-rmse:0.348879+0.000799	test-rmse:0.352243+0.003875 
[149]	train-rmse:0.348738+0.000853	test-rmse:0.352118+0.003831 
[150]	train-rmse:0.348521+0.000852	test-rmse:0.351924+0.003827 
[151]	train-rmse:0.348328+0.000845	test-rmse:0.351751+0.003828 
[152]	train-rmse:0.348112+0.000842	test-rmse:0.351561+0.003829 
[153]	train-rmse:0.347926+0.000852	test-rmse:0.351396+0.003820 
[154]	train-rmse:0.347709+0.000849	test-rmse:0.351203+0.003819 
[155]	train-rmse:0.347516+0.000858	test-rmse:0.351031+0.003811 
[156]	train-rmse:0.347300+0.000856	test-rmse:0.350839+0.003811 
[157]	train-rmse:0.347086+0.000852	test-rmse:0.350650+0.003810 
[158]	train-rmse:0.346871+0.000851	test-rmse:0.350460+0.003808 
[159]	train-rmse:0.346656+0.000850	test-rmse:0.350269+0.003807 
[160]	train-rmse:0.346442+0.000848	test-rmse:0.350074+0.003804 
[161]	train-rmse:0.346229+0.000846	test-rmse:0.349885+0.003803 
[162]	train-rmse:0.346067+0.000879	test-rmse:0.349742+0.003787 
[163]	train-rmse:0.345874+0.000889	test-rmse:0.349568+0.003780 
[164]	train-rmse:0.345661+0.000888	test-rmse:0.349379+0.003781 
[165]	train-rmse:0.345449+0.000888	test-rmse:0.349192+0.003779 
[166]	train-rmse:0.345258+0.000880	test-rmse:0.349019+0.003783 
[167]	train-rmse:0.345045+0.000880	test-rmse:0.348828+0.003780 
[168]	train-rmse:0.344832+0.000879	test-rmse:0.348638+0.003777 
[169]	train-rmse:0.344619+0.000878	test-rmse:0.348448+0.003773 
[170]	train-rmse:0.344428+0.000874	test-rmse:0.348281+0.003775 
[171]	train-rmse:0.344247+0.000899	test-rmse:0.348122+0.003764 
[172]	train-rmse:0.344078+0.000875	test-rmse:0.347973+0.003770 
[173]	train-rmse:0.343898+0.000904	test-rmse:0.347814+0.003761 
[174]	train-rmse:0.343686+0.000905	test-rmse:0.347627+0.003759 
[175]	train-rmse:0.343497+0.000919	test-rmse:0.347459+0.003733 
[176]	train-rmse:0.343284+0.000918	test-rmse:0.347274+0.003734 
[177]	train-rmse:0.343074+0.000918	test-rmse:0.347088+0.003733 
[178]	train-rmse:0.342864+0.000917	test-rmse:0.346902+0.003730 
[179]	train-rmse:0.342654+0.000916	test-rmse:0.346717+0.003729 
[180]	train-rmse:0.342443+0.000915	test-rmse:0.346530+0.003728 
[181]	train-rmse:0.342233+0.000912	test-rmse:0.346341+0.003726 
[182]	train-rmse:0.342022+0.000912	test-rmse:0.346151+0.003724 
[183]	train-rmse:0.341846+0.000924	test-rmse:0.345995+0.003716 
[184]	train-rmse:0.341637+0.000922	test-rmse:0.345810+0.003716 
[185]	train-rmse:0.341427+0.000920	test-rmse:0.345624+0.003712 
[186]	train-rmse:0.341218+0.000919	test-rmse:0.345440+0.003710 
[187]	train-rmse:0.341037+0.000868	test-rmse:0.345279+0.003753 
[188]	train-rmse:0.340865+0.000886	test-rmse:0.345125+0.003744 
[189]	train-rmse:0.340655+0.000885	test-rmse:0.344940+0.003742 
[190]	train-rmse:0.340446+0.000883	test-rmse:0.344753+0.003741 
[191]	train-rmse:0.340237+0.000881	test-rmse:0.344568+0.003741 
[192]	train-rmse:0.340057+0.000902	test-rmse:0.344410+0.003707 
[193]	train-rmse:0.339880+0.000930	test-rmse:0.344254+0.003696 
[194]	train-rmse:0.339671+0.000928	test-rmse:0.344071+0.003695 
[195]	train-rmse:0.339463+0.000926	test-rmse:0.343886+0.003693 
[196]	train-rmse:0.339254+0.000926	test-rmse:0.343701+0.003693 
[197]	train-rmse:0.339046+0.000924	test-rmse:0.343517+0.003693 
[198]	train-rmse:0.338839+0.000922	test-rmse:0.343332+0.003693 
[199]	train-rmse:0.338652+0.000913	test-rmse:0.343169+0.003697 
[200]	train-rmse:0.338445+0.000913	test-rmse:0.342985+0.003697 
[201]	train-rmse:0.338293+0.000931	test-rmse:0.342851+0.003696 
[202]	train-rmse:0.338084+0.000930	test-rmse:0.342666+0.003693 
[203]	train-rmse:0.337879+0.000930	test-rmse:0.342484+0.003688 
[204]	train-rmse:0.337692+0.000952	test-rmse:0.342319+0.003683 
[205]	train-rmse:0.337486+0.000950	test-rmse:0.342138+0.003684 
[206]	train-rmse:0.337328+0.000975	test-rmse:0.342000+0.003685 
[207]	train-rmse:0.337157+0.000971	test-rmse:0.341848+0.003695 
[208]	train-rmse:0.336950+0.000970	test-rmse:0.341666+0.003693 
[209]	train-rmse:0.336766+0.000970	test-rmse:0.341504+0.003697 
[210]	train-rmse:0.336589+0.000971	test-rmse:0.341349+0.003705 
[211]	train-rmse:0.336383+0.000971	test-rmse:0.341166+0.003705 
[212]	train-rmse:0.336206+0.000985	test-rmse:0.341011+0.003669 
[213]	train-rmse:0.336001+0.000984	test-rmse:0.340831+0.003672 
[214]	train-rmse:0.335854+0.000993	test-rmse:0.340697+0.003678 
[215]	train-rmse:0.335649+0.000991	test-rmse:0.340518+0.003676 
[216]	train-rmse:0.335465+0.001010	test-rmse:0.340358+0.003670 
[217]	train-rmse:0.335261+0.001009	test-rmse:0.340178+0.003666 
[218]	train-rmse:0.335057+0.001008	test-rmse:0.339996+0.003664 
[219]	train-rmse:0.334852+0.001006	test-rmse:0.339817+0.003663 
[220]	train-rmse:0.334647+0.001005	test-rmse:0.339640+0.003664 
[221]	train-rmse:0.334479+0.001011	test-rmse:0.339490+0.003657 
[222]	train-rmse:0.334301+0.001024	test-rmse:0.339336+0.003620 
[223]	train-rmse:0.334132+0.000959	test-rmse:0.339190+0.003666 
[224]	train-rmse:0.333929+0.000959	test-rmse:0.339014+0.003665 
[225]	train-rmse:0.333746+0.000978	test-rmse:0.338853+0.003661 
[226]	train-rmse:0.333578+0.000916	test-rmse:0.338708+0.003708 
[227]	train-rmse:0.333375+0.000914	test-rmse:0.338529+0.003711 
[228]	train-rmse:0.333172+0.000914	test-rmse:0.338350+0.003710 
[229]	train-rmse:0.332971+0.000912	test-rmse:0.338172+0.003708 
[230]	train-rmse:0.332788+0.000935	test-rmse:0.338012+0.003701 
[231]	train-rmse:0.332614+0.000938	test-rmse:0.337858+0.003696 
[232]	train-rmse:0.332412+0.000938	test-rmse:0.337682+0.003693 
[233]	train-rmse:0.332238+0.000939	test-rmse:0.337529+0.003701 
[234]	train-rmse:0.332065+0.000943	test-rmse:0.337376+0.003699 
[235]	train-rmse:0.331882+0.000951	test-rmse:0.337216+0.003672 
[236]	train-rmse:0.331755+0.000933	test-rmse:0.337103+0.003671 
[237]	train-rmse:0.331554+0.000932	test-rmse:0.336926+0.003670 
[238]	train-rmse:0.331354+0.000933	test-rmse:0.336749+0.003668 
[239]	train-rmse:0.331152+0.000932	test-rmse:0.336573+0.003666 
[240]	train-rmse:0.330952+0.000930	test-rmse:0.336404+0.003664 
[241]	train-rmse:0.330782+0.000948	test-rmse:0.336257+0.003623 
[242]	train-rmse:0.330601+0.000947	test-rmse:0.336098+0.003628 
[243]	train-rmse:0.330428+0.000966	test-rmse:0.335945+0.003590 
[244]	train-rmse:0.330227+0.000965	test-rmse:0.335767+0.003590 
[245]	train-rmse:0.330028+0.000965	test-rmse:0.335588+0.003591 
[246]	train-rmse:0.329829+0.000965	test-rmse:0.335414+0.003592 
[247]	train-rmse:0.329630+0.000963	test-rmse:0.335240+0.003591 
[248]	train-rmse:0.329430+0.000963	test-rmse:0.335066+0.003590 
[249]	train-rmse:0.329231+0.000963	test-rmse:0.334892+0.003588 
[250]	train-rmse:0.329032+0.000962	test-rmse:0.334720+0.003587 
[251]	train-rmse:0.328866+0.000992	test-rmse:0.334576+0.003576 
[252]	train-rmse:0.328666+0.000991	test-rmse:0.334401+0.003575 
[253]	train-rmse:0.328467+0.000988	test-rmse:0.334230+0.003578 
[254]	train-rmse:0.328269+0.000987	test-rmse:0.334056+0.003577 
[255]	train-rmse:0.328071+0.000986	test-rmse:0.333882+0.003576 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
