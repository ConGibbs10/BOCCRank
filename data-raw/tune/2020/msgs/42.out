> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.360905+0.000833	test-rmse:0.361582+0.004118 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.342154+0.001409	test-rmse:0.343568+0.003514 
[3]	train-rmse:0.325146+0.001123	test-rmse:0.327061+0.004435 
[4]	train-rmse:0.309236+0.000814	test-rmse:0.311697+0.004239 
[5]	train-rmse:0.296638+0.002777	test-rmse:0.299751+0.006701 
[6]	train-rmse:0.286614+0.005098	test-rmse:0.290195+0.008849 
[7]	train-rmse:0.276771+0.004359	test-rmse:0.280596+0.008980 
[8]	train-rmse:0.269224+0.004815	test-rmse:0.273411+0.008769 
[9]	train-rmse:0.261123+0.006851	test-rmse:0.265724+0.010477 
[10]	train-rmse:0.252231+0.005792	test-rmse:0.257358+0.009924 
[11]	train-rmse:0.245431+0.006048	test-rmse:0.251445+0.010710 
[12]	train-rmse:0.240470+0.006937	test-rmse:0.247230+0.011044 
[13]	train-rmse:0.234832+0.006621	test-rmse:0.242263+0.010110 
[14]	train-rmse:0.228679+0.005979	test-rmse:0.236847+0.009555 
[15]	train-rmse:0.224921+0.005641	test-rmse:0.233693+0.007820 
[16]	train-rmse:0.220654+0.004587	test-rmse:0.230142+0.006796 
[17]	train-rmse:0.217099+0.005512	test-rmse:0.226898+0.006546 
[18]	train-rmse:0.213100+0.005049	test-rmse:0.223493+0.006456 
[19]	train-rmse:0.210509+0.004109	test-rmse:0.221416+0.005613 
[20]	train-rmse:0.207782+0.004154	test-rmse:0.219183+0.006215 
[21]	train-rmse:0.204683+0.004072	test-rmse:0.216660+0.006276 
[22]	train-rmse:0.202452+0.004384	test-rmse:0.215028+0.006744 
[23]	train-rmse:0.200361+0.003786	test-rmse:0.213058+0.007465 
[24]	train-rmse:0.198626+0.003671	test-rmse:0.211638+0.007191 
[25]	train-rmse:0.196764+0.002989	test-rmse:0.210314+0.007062 
[26]	train-rmse:0.195159+0.003253	test-rmse:0.209256+0.007078 
[27]	train-rmse:0.193902+0.003200	test-rmse:0.208326+0.006698 
[28]	train-rmse:0.192381+0.002776	test-rmse:0.207154+0.006952 
[29]	train-rmse:0.191079+0.003023	test-rmse:0.206414+0.007035 
[30]	train-rmse:0.190005+0.003283	test-rmse:0.205734+0.007218 
[31]	train-rmse:0.189126+0.003374	test-rmse:0.205356+0.007203 
[32]	train-rmse:0.188566+0.003768	test-rmse:0.204965+0.007065 
[33]	train-rmse:0.187592+0.004120	test-rmse:0.204390+0.007422 
[34]	train-rmse:0.186480+0.003885	test-rmse:0.203621+0.007246 
[35]	train-rmse:0.185472+0.003518	test-rmse:0.203041+0.007304 
[36]	train-rmse:0.184654+0.003405	test-rmse:0.202603+0.007399 
[37]	train-rmse:0.184246+0.003767	test-rmse:0.202364+0.007277 
[38]	train-rmse:0.183729+0.003572	test-rmse:0.201994+0.007548 
[39]	train-rmse:0.182884+0.003744	test-rmse:0.201615+0.007438 
[40]	train-rmse:0.182600+0.004228	test-rmse:0.201633+0.007497 
[41]	train-rmse:0.182020+0.003914	test-rmse:0.201293+0.007624 
[42]	train-rmse:0.181355+0.003681	test-rmse:0.200861+0.007949 
[43]	train-rmse:0.180833+0.003797	test-rmse:0.200673+0.008145 
[44]	train-rmse:0.180092+0.004023	test-rmse:0.200201+0.008305 
[45]	train-rmse:0.179661+0.004201	test-rmse:0.200203+0.008218 
[46]	train-rmse:0.178659+0.004436	test-rmse:0.199860+0.008084 
[47]	train-rmse:0.177768+0.004277	test-rmse:0.199558+0.008142 
[48]	train-rmse:0.176940+0.004126	test-rmse:0.199199+0.007968 
[49]	train-rmse:0.176403+0.004272	test-rmse:0.198911+0.008146 
[50]	train-rmse:0.175520+0.004007	test-rmse:0.198697+0.008073 
[51]	train-rmse:0.174882+0.003646	test-rmse:0.198472+0.008147 
[52]	train-rmse:0.174771+0.003588	test-rmse:0.198429+0.008192 
[53]	train-rmse:0.173962+0.003619	test-rmse:0.198053+0.008086 
[54]	train-rmse:0.173097+0.003588	test-rmse:0.197871+0.008181 
[55]	train-rmse:0.172721+0.003400	test-rmse:0.197638+0.008308 
[56]	train-rmse:0.172311+0.003442	test-rmse:0.197567+0.008257 
[57]	train-rmse:0.171706+0.003474	test-rmse:0.197410+0.008372 
[58]	train-rmse:0.171281+0.003362	test-rmse:0.197358+0.008378 
[59]	train-rmse:0.170808+0.003301	test-rmse:0.197166+0.008397 
[60]	train-rmse:0.170468+0.003648	test-rmse:0.197063+0.008285 
[61]	train-rmse:0.169905+0.003591	test-rmse:0.196835+0.008287 
[62]	train-rmse:0.169410+0.003562	test-rmse:0.196724+0.008321 
[63]	train-rmse:0.168797+0.003755	test-rmse:0.196628+0.008255 
[64]	train-rmse:0.168277+0.003808	test-rmse:0.196513+0.008289 
[65]	train-rmse:0.168027+0.003709	test-rmse:0.196538+0.008295 
[66]	train-rmse:0.167577+0.003759	test-rmse:0.196454+0.008129 
[67]	train-rmse:0.167101+0.003930	test-rmse:0.196298+0.007995 
[68]	train-rmse:0.166767+0.004171	test-rmse:0.196145+0.007893 
[69]	train-rmse:0.166240+0.004309	test-rmse:0.196035+0.007777 
[70]	train-rmse:0.165852+0.004424	test-rmse:0.195864+0.007638 
[71]	train-rmse:0.165572+0.004285	test-rmse:0.195876+0.007643 
[72]	train-rmse:0.165039+0.004254	test-rmse:0.195685+0.007868 
[73]	train-rmse:0.164861+0.004300	test-rmse:0.195734+0.007846 
[74]	train-rmse:0.164180+0.004321	test-rmse:0.195435+0.007822 
[75]	train-rmse:0.163558+0.004268	test-rmse:0.195306+0.007879 
[76]	train-rmse:0.163206+0.004012	test-rmse:0.195220+0.007912 
[77]	train-rmse:0.162815+0.004038	test-rmse:0.195140+0.007959 
[78]	train-rmse:0.162569+0.004279	test-rmse:0.195181+0.008083 
[79]	train-rmse:0.162256+0.004435	test-rmse:0.195069+0.008038 
[80]	train-rmse:0.161713+0.004444	test-rmse:0.194914+0.008034 
[81]	train-rmse:0.161574+0.004470	test-rmse:0.194883+0.008050 
[82]	train-rmse:0.161135+0.004582	test-rmse:0.194800+0.008344 
[83]	train-rmse:0.160873+0.004569	test-rmse:0.194724+0.008392 
[84]	train-rmse:0.160341+0.004557	test-rmse:0.194586+0.008530 
[85]	train-rmse:0.160165+0.004660	test-rmse:0.194641+0.008616 
[86]	train-rmse:0.159804+0.004772	test-rmse:0.194367+0.008486 
[87]	train-rmse:0.159561+0.004650	test-rmse:0.194273+0.008542 
[88]	train-rmse:0.159182+0.004433	test-rmse:0.194110+0.008637 
[89]	train-rmse:0.158884+0.004517	test-rmse:0.194138+0.008834 
[90]	train-rmse:0.158454+0.004466	test-rmse:0.194147+0.008780 
[91]	train-rmse:0.158277+0.004378	test-rmse:0.194191+0.008738 
[92]	train-rmse:0.157982+0.004408	test-rmse:0.194192+0.008901 
[93]	train-rmse:0.157847+0.004374	test-rmse:0.194199+0.008909 
[94]	train-rmse:0.157500+0.004498	test-rmse:0.194176+0.008971 
[95]	train-rmse:0.157185+0.004533	test-rmse:0.194097+0.008936 
[96]	train-rmse:0.156845+0.004466	test-rmse:0.194148+0.008943 
[97]	train-rmse:0.156635+0.004584	test-rmse:0.194176+0.008838 
[98]	train-rmse:0.156439+0.004561	test-rmse:0.194108+0.008870 
[99]	train-rmse:0.155981+0.004472	test-rmse:0.194039+0.008986 
[100]	train-rmse:0.155660+0.004746	test-rmse:0.194105+0.009011 
[101]	train-rmse:0.155405+0.004715	test-rmse:0.194160+0.009147 
[102]	train-rmse:0.155023+0.004503	test-rmse:0.194079+0.009212 
[103]	train-rmse:0.154781+0.004765	test-rmse:0.194112+0.009105 
[104]	train-rmse:0.154569+0.004567	test-rmse:0.194087+0.009122 
[105]	train-rmse:0.154211+0.004554	test-rmse:0.194067+0.008987 
[106]	train-rmse:0.153962+0.004572	test-rmse:0.194145+0.009038 
[107]	train-rmse:0.153652+0.004438	test-rmse:0.194044+0.009112 
[108]	train-rmse:0.153254+0.004539	test-rmse:0.193957+0.009239 
[109]	train-rmse:0.153071+0.004516	test-rmse:0.193987+0.009291 
[110]	train-rmse:0.152635+0.004533	test-rmse:0.194112+0.009150 
[111]	train-rmse:0.152269+0.004599	test-rmse:0.194160+0.009123 
[112]	train-rmse:0.151786+0.004585	test-rmse:0.194079+0.009049 
[113]	train-rmse:0.151501+0.004464	test-rmse:0.194014+0.009065 
[114]	train-rmse:0.150939+0.004388	test-rmse:0.194044+0.009054 
[115]	train-rmse:0.150518+0.004407	test-rmse:0.193982+0.009106 
[116]	train-rmse:0.150328+0.004385	test-rmse:0.193910+0.009031 
[117]	train-rmse:0.149922+0.004315	test-rmse:0.193914+0.009021 
[118]	train-rmse:0.149706+0.004256	test-rmse:0.193965+0.008991 
[119]	train-rmse:0.149530+0.004155	test-rmse:0.193918+0.009023 
[120]	train-rmse:0.149317+0.004040	test-rmse:0.193793+0.009093 
[121]	train-rmse:0.149062+0.004024	test-rmse:0.193729+0.009143 
[122]	train-rmse:0.148850+0.004174	test-rmse:0.193780+0.009244 
[123]	train-rmse:0.148457+0.004076	test-rmse:0.193711+0.009134 
[124]	train-rmse:0.148207+0.004211	test-rmse:0.193842+0.009071 
[125]	train-rmse:0.147908+0.004329	test-rmse:0.193802+0.008994 
[126]	train-rmse:0.147548+0.004256	test-rmse:0.193799+0.008982 
[127]	train-rmse:0.147291+0.004315	test-rmse:0.193754+0.008961 
[128]	train-rmse:0.147084+0.004449	test-rmse:0.193723+0.009048 
[129]	train-rmse:0.146810+0.004666	test-rmse:0.193715+0.008952 
[130]	train-rmse:0.146505+0.004454	test-rmse:0.193687+0.008973 
[131]	train-rmse:0.146251+0.004255	test-rmse:0.193593+0.009012 
[132]	train-rmse:0.145865+0.004066	test-rmse:0.193624+0.008960 
[133]	train-rmse:0.145742+0.004159	test-rmse:0.193638+0.008984 
[134]	train-rmse:0.145427+0.004172	test-rmse:0.193684+0.009023 
[135]	train-rmse:0.145362+0.004246	test-rmse:0.193649+0.008978 
[136]	train-rmse:0.145066+0.004307	test-rmse:0.193599+0.008954 
[137]	train-rmse:0.144846+0.004293	test-rmse:0.193559+0.009058 
[138]	train-rmse:0.144654+0.004411	test-rmse:0.193552+0.009014 
[139]	train-rmse:0.144265+0.004379	test-rmse:0.193646+0.008941 
[140]	train-rmse:0.144094+0.004482	test-rmse:0.193727+0.009091 
[141]	train-rmse:0.143905+0.004442	test-rmse:0.193697+0.009102 
[142]	train-rmse:0.143526+0.004344	test-rmse:0.193657+0.009063 
[143]	train-rmse:0.143400+0.004280	test-rmse:0.193612+0.009084 
[144]	train-rmse:0.143253+0.004234	test-rmse:0.193556+0.009138 
[145]	train-rmse:0.143104+0.004134	test-rmse:0.193563+0.009146 
[146]	train-rmse:0.143025+0.004126	test-rmse:0.193572+0.009141 
[147]	train-rmse:0.142903+0.004003	test-rmse:0.193572+0.009139 
[148]	train-rmse:0.142501+0.004094	test-rmse:0.193445+0.008997 
[149]	train-rmse:0.142333+0.004039	test-rmse:0.193461+0.008979 
[150]	train-rmse:0.142020+0.003870	test-rmse:0.193192+0.009136 
[151]	train-rmse:0.141801+0.003791	test-rmse:0.193168+0.009132 
[152]	train-rmse:0.141509+0.003767	test-rmse:0.193219+0.009053 
[153]	train-rmse:0.141217+0.003601	test-rmse:0.193256+0.009030 
[154]	train-rmse:0.140982+0.003635	test-rmse:0.193216+0.009004 
[155]	train-rmse:0.140886+0.003578	test-rmse:0.193181+0.009008 
[156]	train-rmse:0.140716+0.003567	test-rmse:0.193037+0.008977 
[157]	train-rmse:0.140529+0.003611	test-rmse:0.193010+0.009058 
[158]	train-rmse:0.140211+0.003420	test-rmse:0.192914+0.009098 
[159]	train-rmse:0.139982+0.003458	test-rmse:0.192860+0.009140 
[160]	train-rmse:0.139875+0.003453	test-rmse:0.192854+0.009138 
[161]	train-rmse:0.139712+0.003498	test-rmse:0.192930+0.009185 
[162]	train-rmse:0.139441+0.003407	test-rmse:0.193050+0.009121 
[163]	train-rmse:0.139280+0.003555	test-rmse:0.193084+0.008957 
[164]	train-rmse:0.139010+0.003625	test-rmse:0.193055+0.008901 
[165]	train-rmse:0.138594+0.003618	test-rmse:0.193022+0.008886 
[166]	train-rmse:0.138182+0.003615	test-rmse:0.193204+0.008912 
[167]	train-rmse:0.137840+0.003677	test-rmse:0.193146+0.008930 
[168]	train-rmse:0.137570+0.003669	test-rmse:0.193089+0.008971 
[169]	train-rmse:0.137450+0.003668	test-rmse:0.193116+0.008963 
[170]	train-rmse:0.137173+0.003539	test-rmse:0.193041+0.008997 
[171]	train-rmse:0.136981+0.003554	test-rmse:0.193074+0.009058 
[172]	train-rmse:0.136842+0.003694	test-rmse:0.193058+0.009062 
[173]	train-rmse:0.136649+0.003829	test-rmse:0.193026+0.008985 
[174]	train-rmse:0.136396+0.003643	test-rmse:0.193038+0.008978 
[175]	train-rmse:0.136196+0.003547	test-rmse:0.193069+0.008970 
[176]	train-rmse:0.135843+0.003442	test-rmse:0.193129+0.009001 
[177]	train-rmse:0.135616+0.003387	test-rmse:0.193060+0.009095 
[178]	train-rmse:0.135510+0.003375	test-rmse:0.193102+0.009077 
[179]	train-rmse:0.135322+0.003451	test-rmse:0.193178+0.008991 
[180]	train-rmse:0.135040+0.003364	test-rmse:0.193100+0.008907 
[181]	train-rmse:0.134840+0.003312	test-rmse:0.193111+0.008861 
[182]	train-rmse:0.134645+0.003223	test-rmse:0.193021+0.008949 
[183]	train-rmse:0.134246+0.003219	test-rmse:0.193017+0.008994 
[184]	train-rmse:0.134062+0.003316	test-rmse:0.193010+0.009050 
[185]	train-rmse:0.133886+0.003323	test-rmse:0.193097+0.008929 
[186]	train-rmse:0.133639+0.003292	test-rmse:0.193125+0.008930 
[187]	train-rmse:0.133387+0.003161	test-rmse:0.193049+0.008955 
[188]	train-rmse:0.133257+0.003295	test-rmse:0.193001+0.008824 
[189]	train-rmse:0.133092+0.003306	test-rmse:0.192892+0.008836 
[190]	train-rmse:0.132831+0.003269	test-rmse:0.192954+0.008837 
[191]	train-rmse:0.132588+0.003257	test-rmse:0.192997+0.008872 
[192]	train-rmse:0.132322+0.003274	test-rmse:0.192973+0.008912 
[193]	train-rmse:0.132023+0.003282	test-rmse:0.193066+0.008829 
[194]	train-rmse:0.131790+0.003273	test-rmse:0.193021+0.008897 
[195]	train-rmse:0.131562+0.003255	test-rmse:0.192953+0.009014 
[196]	train-rmse:0.131266+0.003265	test-rmse:0.192932+0.009055 
[197]	train-rmse:0.130994+0.003360	test-rmse:0.192827+0.009101 
[198]	train-rmse:0.130893+0.003357	test-rmse:0.192805+0.009105 
[199]	train-rmse:0.130729+0.003281	test-rmse:0.192868+0.009056 
[200]	train-rmse:0.130558+0.003283	test-rmse:0.192823+0.009123 
[201]	train-rmse:0.130279+0.003273	test-rmse:0.192807+0.009035 
[202]	train-rmse:0.130090+0.003270	test-rmse:0.192772+0.009034 
[203]	train-rmse:0.129885+0.003196	test-rmse:0.192851+0.008995 
[204]	train-rmse:0.129662+0.003203	test-rmse:0.192932+0.008982 
[205]	train-rmse:0.129518+0.003285	test-rmse:0.192934+0.009011 
[206]	train-rmse:0.129405+0.003183	test-rmse:0.192896+0.009027 
[207]	train-rmse:0.129183+0.003012	test-rmse:0.192805+0.009090 
[208]	train-rmse:0.128961+0.003019	test-rmse:0.192902+0.009144 
[209]	train-rmse:0.128883+0.002955	test-rmse:0.192900+0.009144 
[210]	train-rmse:0.128823+0.002912	test-rmse:0.192954+0.009100 
[211]	train-rmse:0.128529+0.002999	test-rmse:0.192914+0.009069 
[212]	train-rmse:0.128300+0.003127	test-rmse:0.192990+0.009183 
[213]	train-rmse:0.128161+0.003163	test-rmse:0.193073+0.009260 
[214]	train-rmse:0.127914+0.003185	test-rmse:0.192989+0.009231 
[215]	train-rmse:0.127612+0.003107	test-rmse:0.193085+0.009145 
[216]	train-rmse:0.127440+0.003239	test-rmse:0.193103+0.009130 
[217]	train-rmse:0.127214+0.003290	test-rmse:0.193130+0.009101 
[218]	train-rmse:0.127003+0.003191	test-rmse:0.193255+0.008996 
[219]	train-rmse:0.126779+0.003154	test-rmse:0.193330+0.008930 
[220]	train-rmse:0.126584+0.003035	test-rmse:0.193366+0.008908 
[221]	train-rmse:0.126319+0.003006	test-rmse:0.193482+0.008938 
[222]	train-rmse:0.126101+0.002883	test-rmse:0.193489+0.008943 
[223]	train-rmse:0.125839+0.002746	test-rmse:0.193486+0.008923 
[224]	train-rmse:0.125660+0.002636	test-rmse:0.193413+0.008972 
[225]	train-rmse:0.125472+0.002549	test-rmse:0.193387+0.008990 
[226]	train-rmse:0.125197+0.002582	test-rmse:0.193372+0.008985 
[227]	train-rmse:0.125012+0.002599	test-rmse:0.193313+0.009003 
[228]	train-rmse:0.124906+0.002702	test-rmse:0.193377+0.009023 
[229]	train-rmse:0.124713+0.002712	test-rmse:0.193447+0.008987 
[230]	train-rmse:0.124459+0.002813	test-rmse:0.193381+0.008984 
[231]	train-rmse:0.124229+0.002700	test-rmse:0.193488+0.008960 
[232]	train-rmse:0.124096+0.002814	test-rmse:0.193541+0.008911 
[233]	train-rmse:0.123847+0.002820	test-rmse:0.193566+0.008946 
[234]	train-rmse:0.123700+0.002856	test-rmse:0.193549+0.008890 
[235]	train-rmse:0.123479+0.002922	test-rmse:0.193490+0.008927 
[236]	train-rmse:0.123246+0.002962	test-rmse:0.193504+0.008903 
[237]	train-rmse:0.122927+0.002871	test-rmse:0.193589+0.008921 
[238]	train-rmse:0.122607+0.002830	test-rmse:0.193421+0.008902 
[239]	train-rmse:0.122434+0.002760	test-rmse:0.193428+0.008914 
[240]	train-rmse:0.122130+0.002666	test-rmse:0.193308+0.008921 
[241]	train-rmse:0.121953+0.002720	test-rmse:0.193221+0.009013 
[242]	train-rmse:0.121750+0.002734	test-rmse:0.193129+0.008927 
[243]	train-rmse:0.121556+0.002644	test-rmse:0.193074+0.008924 
[244]	train-rmse:0.121333+0.002663	test-rmse:0.193021+0.009060 
[245]	train-rmse:0.121107+0.002660	test-rmse:0.193065+0.009165 
[246]	train-rmse:0.120954+0.002721	test-rmse:0.193063+0.009124 
[247]	train-rmse:0.120647+0.002745	test-rmse:0.193059+0.009078 
[248]	train-rmse:0.120390+0.002716	test-rmse:0.193126+0.008947 
[249]	train-rmse:0.120157+0.002779	test-rmse:0.193197+0.008963 
[250]	train-rmse:0.120033+0.002784	test-rmse:0.193228+0.008906 
[251]	train-rmse:0.119875+0.002828	test-rmse:0.193204+0.008824 
[252]	train-rmse:0.119756+0.002797	test-rmse:0.193149+0.008838 
Stopping. Best iteration:
[202]	train-rmse:0.130090+0.003270	test-rmse:0.192772+0.009034

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
