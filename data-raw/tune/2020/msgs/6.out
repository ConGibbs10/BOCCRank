> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.376375+0.000996	test-rmse:0.376915+0.004120 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.370208+0.000946	test-rmse:0.371238+0.004039 
[3]	train-rmse:0.364226+0.000941	test-rmse:0.365813+0.003920 
[4]	train-rmse:0.358368+0.000919	test-rmse:0.360469+0.003937 
[5]	train-rmse:0.352676+0.000856	test-rmse:0.355390+0.003963 
[6]	train-rmse:0.347099+0.000807	test-rmse:0.350329+0.003897 
[7]	train-rmse:0.341621+0.000801	test-rmse:0.345456+0.003823 
[8]	train-rmse:0.336288+0.000771	test-rmse:0.340665+0.003789 
[9]	train-rmse:0.331058+0.000741	test-rmse:0.335994+0.003747 
[10]	train-rmse:0.325984+0.000746	test-rmse:0.331515+0.003625 
[11]	train-rmse:0.320991+0.000734	test-rmse:0.327151+0.003633 
[12]	train-rmse:0.316117+0.000727	test-rmse:0.322878+0.003633 
[13]	train-rmse:0.311356+0.000709	test-rmse:0.318757+0.003672 
[14]	train-rmse:0.306721+0.000699	test-rmse:0.314746+0.003760 
[15]	train-rmse:0.302129+0.000701	test-rmse:0.310752+0.003766 
[16]	train-rmse:0.297675+0.000694	test-rmse:0.306991+0.003809 
[17]	train-rmse:0.294122+0.001808	test-rmse:0.303947+0.004218 
[18]	train-rmse:0.290687+0.002335	test-rmse:0.301125+0.003272 
[19]	train-rmse:0.286518+0.002245	test-rmse:0.297672+0.003242 
[20]	train-rmse:0.283220+0.002419	test-rmse:0.294907+0.002767 
[21]	train-rmse:0.279226+0.002373	test-rmse:0.291627+0.002804 
[22]	train-rmse:0.275281+0.002366	test-rmse:0.288376+0.002916 
[23]	train-rmse:0.271486+0.002360	test-rmse:0.285317+0.003004 
[24]	train-rmse:0.267693+0.002320	test-rmse:0.282325+0.003064 
[25]	train-rmse:0.264679+0.003247	test-rmse:0.279876+0.003129 
[26]	train-rmse:0.261072+0.003219	test-rmse:0.277031+0.003130 
[27]	train-rmse:0.257531+0.003163	test-rmse:0.274144+0.003246 
[28]	train-rmse:0.254724+0.002731	test-rmse:0.271898+0.003752 
[29]	train-rmse:0.251344+0.002750	test-rmse:0.269323+0.003786 
[30]	train-rmse:0.248063+0.002727	test-rmse:0.266794+0.003789 
[31]	train-rmse:0.245444+0.003789	test-rmse:0.264837+0.003973 
[32]	train-rmse:0.242274+0.003732	test-rmse:0.262404+0.004025 
[33]	train-rmse:0.239166+0.003689	test-rmse:0.260096+0.004024 
[34]	train-rmse:0.236171+0.003630	test-rmse:0.257875+0.004053 
[35]	train-rmse:0.233239+0.003550	test-rmse:0.255715+0.004141 
[36]	train-rmse:0.230336+0.003523	test-rmse:0.253489+0.004179 
[37]	train-rmse:0.227498+0.003455	test-rmse:0.251387+0.004217 
[38]	train-rmse:0.224696+0.003400	test-rmse:0.249397+0.004306 
[39]	train-rmse:0.222492+0.002738	test-rmse:0.247734+0.004808 
[40]	train-rmse:0.219773+0.002666	test-rmse:0.245778+0.004856 
[41]	train-rmse:0.217145+0.002597	test-rmse:0.243956+0.004972 
[42]	train-rmse:0.214588+0.002555	test-rmse:0.242202+0.005031 
[43]	train-rmse:0.212067+0.002490	test-rmse:0.240508+0.005142 
[44]	train-rmse:0.210088+0.003419	test-rmse:0.239225+0.005228 
[45]	train-rmse:0.207688+0.003345	test-rmse:0.237560+0.005313 
[46]	train-rmse:0.205297+0.003301	test-rmse:0.235868+0.005448 
[47]	train-rmse:0.203004+0.003251	test-rmse:0.234314+0.005516 
[48]	train-rmse:0.200764+0.003189	test-rmse:0.232821+0.005555 
[49]	train-rmse:0.198550+0.003135	test-rmse:0.231344+0.005685 
[50]	train-rmse:0.196383+0.003081	test-rmse:0.230006+0.005830 
[51]	train-rmse:0.194279+0.003022	test-rmse:0.228670+0.005988 
[52]	train-rmse:0.192188+0.003018	test-rmse:0.227344+0.006127 
[53]	train-rmse:0.190168+0.002976	test-rmse:0.226120+0.006234 
[54]	train-rmse:0.188209+0.002955	test-rmse:0.224972+0.006404 
[55]	train-rmse:0.186298+0.002907	test-rmse:0.223683+0.006557 
[56]	train-rmse:0.184824+0.003611	test-rmse:0.222810+0.006622 
[57]	train-rmse:0.182970+0.003549	test-rmse:0.221707+0.006675 
[58]	train-rmse:0.181151+0.003489	test-rmse:0.220693+0.006768 
[59]	train-rmse:0.179353+0.003425	test-rmse:0.219623+0.006823 
[60]	train-rmse:0.177971+0.004086	test-rmse:0.218810+0.006836 
[61]	train-rmse:0.176235+0.004007	test-rmse:0.217803+0.006921 
[62]	train-rmse:0.174581+0.003941	test-rmse:0.216874+0.007044 
[63]	train-rmse:0.172907+0.003872	test-rmse:0.215996+0.007127 
[64]	train-rmse:0.171277+0.003787	test-rmse:0.215161+0.007186 
[65]	train-rmse:0.169726+0.003731	test-rmse:0.214364+0.007292 
[66]	train-rmse:0.168216+0.003702	test-rmse:0.213636+0.007387 
[67]	train-rmse:0.166672+0.003619	test-rmse:0.212813+0.007468 
[68]	train-rmse:0.165185+0.003524	test-rmse:0.212029+0.007494 
[69]	train-rmse:0.163710+0.003452	test-rmse:0.211302+0.007512 
[70]	train-rmse:0.162284+0.003409	test-rmse:0.210601+0.007614 
[71]	train-rmse:0.160873+0.003355	test-rmse:0.209945+0.007715 
[72]	train-rmse:0.159511+0.003257	test-rmse:0.209240+0.007812 
[73]	train-rmse:0.158153+0.003182	test-rmse:0.208608+0.007890 
[74]	train-rmse:0.156824+0.003146	test-rmse:0.207942+0.007877 
[75]	train-rmse:0.155498+0.003110	test-rmse:0.207367+0.007911 
[76]	train-rmse:0.154203+0.003064	test-rmse:0.206766+0.007984 
[77]	train-rmse:0.152955+0.003007	test-rmse:0.206305+0.008139 
[78]	train-rmse:0.151699+0.002955	test-rmse:0.205765+0.008188 
[79]	train-rmse:0.150489+0.002911	test-rmse:0.205263+0.008260 
[80]	train-rmse:0.149322+0.002898	test-rmse:0.204815+0.008352 
[81]	train-rmse:0.148179+0.002878	test-rmse:0.204335+0.008441 
[82]	train-rmse:0.147019+0.002822	test-rmse:0.203903+0.008524 
[83]	train-rmse:0.145877+0.002793	test-rmse:0.203402+0.008525 
[84]	train-rmse:0.144798+0.002761	test-rmse:0.202947+0.008495 
[85]	train-rmse:0.143734+0.002720	test-rmse:0.202582+0.008562 
[86]	train-rmse:0.142674+0.002642	test-rmse:0.202137+0.008621 
[87]	train-rmse:0.141633+0.002611	test-rmse:0.201737+0.008730 
[88]	train-rmse:0.140593+0.002597	test-rmse:0.201420+0.008767 
[89]	train-rmse:0.139588+0.002566	test-rmse:0.201017+0.008811 
[90]	train-rmse:0.138594+0.002492	test-rmse:0.200653+0.008849 
[91]	train-rmse:0.137647+0.002474	test-rmse:0.200350+0.008915 
[92]	train-rmse:0.136678+0.002465	test-rmse:0.200023+0.008950 
[93]	train-rmse:0.135738+0.002437	test-rmse:0.199708+0.008970 
[94]	train-rmse:0.134823+0.002432	test-rmse:0.199383+0.008952 
[95]	train-rmse:0.133915+0.002381	test-rmse:0.199069+0.008973 
[96]	train-rmse:0.133024+0.002339	test-rmse:0.198810+0.008975 
[97]	train-rmse:0.132128+0.002330	test-rmse:0.198520+0.009009 
[98]	train-rmse:0.131280+0.002292	test-rmse:0.198286+0.009023 
[99]	train-rmse:0.130442+0.002270	test-rmse:0.197978+0.009024 
[100]	train-rmse:0.129575+0.002218	test-rmse:0.197711+0.009010 
[101]	train-rmse:0.128753+0.002183	test-rmse:0.197483+0.009022 
[102]	train-rmse:0.127950+0.002172	test-rmse:0.197302+0.009116 
[103]	train-rmse:0.127130+0.002130	test-rmse:0.197055+0.009095 
[104]	train-rmse:0.126350+0.002088	test-rmse:0.196838+0.009102 
[105]	train-rmse:0.125748+0.002345	test-rmse:0.196711+0.009112 
[106]	train-rmse:0.124993+0.002321	test-rmse:0.196534+0.009145 
[107]	train-rmse:0.124226+0.002326	test-rmse:0.196321+0.009101 
[108]	train-rmse:0.123490+0.002323	test-rmse:0.196135+0.009084 
[109]	train-rmse:0.122763+0.002273	test-rmse:0.195918+0.009097 
[110]	train-rmse:0.122082+0.002235	test-rmse:0.195793+0.009125 
[111]	train-rmse:0.121523+0.002472	test-rmse:0.195604+0.009169 
[112]	train-rmse:0.120834+0.002444	test-rmse:0.195482+0.009185 
[113]	train-rmse:0.120133+0.002406	test-rmse:0.195345+0.009241 
[114]	train-rmse:0.119432+0.002374	test-rmse:0.195139+0.009265 
[115]	train-rmse:0.118741+0.002376	test-rmse:0.195043+0.009312 
[116]	train-rmse:0.118071+0.002346	test-rmse:0.194901+0.009383 
[117]	train-rmse:0.117390+0.002304	test-rmse:0.194760+0.009433 
[118]	train-rmse:0.116782+0.002297	test-rmse:0.194653+0.009477 
[119]	train-rmse:0.116151+0.002238	test-rmse:0.194506+0.009494 
[120]	train-rmse:0.115539+0.002218	test-rmse:0.194396+0.009529 
[121]	train-rmse:0.114941+0.002174	test-rmse:0.194285+0.009561 
[122]	train-rmse:0.114331+0.002130	test-rmse:0.194095+0.009594 
[123]	train-rmse:0.113698+0.002098	test-rmse:0.193953+0.009615 
[124]	train-rmse:0.113079+0.002052	test-rmse:0.193889+0.009601 
[125]	train-rmse:0.112489+0.002009	test-rmse:0.193841+0.009633 
[126]	train-rmse:0.111892+0.001988	test-rmse:0.193804+0.009590 
[127]	train-rmse:0.111313+0.001943	test-rmse:0.193702+0.009631 
[128]	train-rmse:0.110752+0.001909	test-rmse:0.193602+0.009707 
[129]	train-rmse:0.110183+0.001892	test-rmse:0.193464+0.009681 
[130]	train-rmse:0.109645+0.001881	test-rmse:0.193358+0.009722 
[131]	train-rmse:0.109101+0.001846	test-rmse:0.193313+0.009768 
[132]	train-rmse:0.108566+0.001805	test-rmse:0.193215+0.009781 
[133]	train-rmse:0.108011+0.001802	test-rmse:0.193186+0.009792 
[134]	train-rmse:0.107503+0.001802	test-rmse:0.193126+0.009856 
[135]	train-rmse:0.106988+0.001773	test-rmse:0.193046+0.009883 
[136]	train-rmse:0.106498+0.001746	test-rmse:0.193016+0.009979 
[137]	train-rmse:0.106014+0.001773	test-rmse:0.192957+0.009951 
[138]	train-rmse:0.105522+0.001773	test-rmse:0.192905+0.009990 
[139]	train-rmse:0.105042+0.001752	test-rmse:0.192846+0.010034 
[140]	train-rmse:0.104541+0.001742	test-rmse:0.192763+0.010088 
[141]	train-rmse:0.104063+0.001738	test-rmse:0.192740+0.010114 
[142]	train-rmse:0.103558+0.001715	test-rmse:0.192629+0.010150 
[143]	train-rmse:0.103071+0.001702	test-rmse:0.192560+0.010210 
[144]	train-rmse:0.102591+0.001694	test-rmse:0.192506+0.010260 
[145]	train-rmse:0.102196+0.001716	test-rmse:0.192465+0.010253 
[146]	train-rmse:0.101724+0.001699	test-rmse:0.192471+0.010331 
[147]	train-rmse:0.101245+0.001706	test-rmse:0.192455+0.010395 
[148]	train-rmse:0.100882+0.001668	test-rmse:0.192445+0.010428 
[149]	train-rmse:0.100421+0.001672	test-rmse:0.192417+0.010438 
[150]	train-rmse:0.099951+0.001633	test-rmse:0.192421+0.010536 
[151]	train-rmse:0.099499+0.001614	test-rmse:0.192388+0.010552 
[152]	train-rmse:0.099068+0.001613	test-rmse:0.192335+0.010558 
[153]	train-rmse:0.098630+0.001580	test-rmse:0.192305+0.010586 
[154]	train-rmse:0.098217+0.001578	test-rmse:0.192281+0.010580 
[155]	train-rmse:0.097814+0.001576	test-rmse:0.192265+0.010618 
[156]	train-rmse:0.097388+0.001559	test-rmse:0.192234+0.010642 
[157]	train-rmse:0.096948+0.001578	test-rmse:0.192149+0.010670 
[158]	train-rmse:0.096541+0.001525	test-rmse:0.192106+0.010714 
[159]	train-rmse:0.096159+0.001518	test-rmse:0.192086+0.010656 
[160]	train-rmse:0.095830+0.001363	test-rmse:0.192056+0.010652 
[161]	train-rmse:0.095494+0.001354	test-rmse:0.192068+0.010704 
[162]	train-rmse:0.095092+0.001316	test-rmse:0.192004+0.010729 
[163]	train-rmse:0.094697+0.001289	test-rmse:0.191992+0.010784 
[164]	train-rmse:0.094318+0.001255	test-rmse:0.192001+0.010756 
[165]	train-rmse:0.093931+0.001231	test-rmse:0.191973+0.010776 
[166]	train-rmse:0.093535+0.001212	test-rmse:0.191913+0.010767 
[167]	train-rmse:0.093154+0.001190	test-rmse:0.191883+0.010751 
[168]	train-rmse:0.092772+0.001195	test-rmse:0.191884+0.010753 
[169]	train-rmse:0.092376+0.001180	test-rmse:0.191835+0.010735 
[170]	train-rmse:0.091989+0.001154	test-rmse:0.191835+0.010697 
[171]	train-rmse:0.091601+0.001139	test-rmse:0.191855+0.010698 
[172]	train-rmse:0.091216+0.001130	test-rmse:0.191859+0.010720 
[173]	train-rmse:0.090844+0.001119	test-rmse:0.191833+0.010697 
[174]	train-rmse:0.090480+0.001118	test-rmse:0.191820+0.010684 
[175]	train-rmse:0.090109+0.001131	test-rmse:0.191796+0.010693 
[176]	train-rmse:0.089765+0.001120	test-rmse:0.191805+0.010692 
[177]	train-rmse:0.089398+0.001102	test-rmse:0.191801+0.010755 
[178]	train-rmse:0.089071+0.001126	test-rmse:0.191773+0.010697 
[179]	train-rmse:0.088732+0.001101	test-rmse:0.191769+0.010675 
[180]	train-rmse:0.088387+0.001105	test-rmse:0.191766+0.010678 
[181]	train-rmse:0.088026+0.001103	test-rmse:0.191779+0.010678 
[182]	train-rmse:0.087682+0.001108	test-rmse:0.191780+0.010713 
[183]	train-rmse:0.087336+0.001094	test-rmse:0.191770+0.010722 
[184]	train-rmse:0.086961+0.001074	test-rmse:0.191746+0.010742 
[185]	train-rmse:0.086641+0.001037	test-rmse:0.191736+0.010763 
[186]	train-rmse:0.086335+0.001031	test-rmse:0.191736+0.010808 
[187]	train-rmse:0.086003+0.001030	test-rmse:0.191707+0.010818 
[188]	train-rmse:0.085679+0.001034	test-rmse:0.191711+0.010818 
[189]	train-rmse:0.085360+0.001000	test-rmse:0.191706+0.010803 
[190]	train-rmse:0.085042+0.000978	test-rmse:0.191691+0.010804 
[191]	train-rmse:0.084765+0.000965	test-rmse:0.191716+0.010794 
[192]	train-rmse:0.084428+0.000969	test-rmse:0.191702+0.010828 
[193]	train-rmse:0.084115+0.000981	test-rmse:0.191731+0.010840 
[194]	train-rmse:0.083774+0.000985	test-rmse:0.191727+0.010876 
[195]	train-rmse:0.083466+0.000984	test-rmse:0.191711+0.010871 
[196]	train-rmse:0.083207+0.001054	test-rmse:0.191696+0.010892 
[197]	train-rmse:0.082905+0.001028	test-rmse:0.191674+0.010930 
[198]	train-rmse:0.082676+0.001053	test-rmse:0.191652+0.010941 
[199]	train-rmse:0.082362+0.001056	test-rmse:0.191680+0.010957 
[200]	train-rmse:0.082054+0.001051	test-rmse:0.191685+0.010972 
[201]	train-rmse:0.081761+0.001069	test-rmse:0.191694+0.010978 
[202]	train-rmse:0.081459+0.001067	test-rmse:0.191694+0.010967 
[203]	train-rmse:0.081153+0.001061	test-rmse:0.191712+0.010994 
[204]	train-rmse:0.080849+0.001029	test-rmse:0.191674+0.010980 
[205]	train-rmse:0.080555+0.001010	test-rmse:0.191674+0.010933 
[206]	train-rmse:0.080246+0.000991	test-rmse:0.191625+0.010975 
[207]	train-rmse:0.079971+0.001036	test-rmse:0.191664+0.010981 
[208]	train-rmse:0.079686+0.001050	test-rmse:0.191656+0.011034 
[209]	train-rmse:0.079405+0.001023	test-rmse:0.191711+0.011009 
[210]	train-rmse:0.079128+0.001020	test-rmse:0.191745+0.011032 
[211]	train-rmse:0.078866+0.001025	test-rmse:0.191735+0.011019 
[212]	train-rmse:0.078603+0.001017	test-rmse:0.191752+0.010997 
[213]	train-rmse:0.078333+0.001017	test-rmse:0.191784+0.011003 
[214]	train-rmse:0.078048+0.001002	test-rmse:0.191794+0.010987 
[215]	train-rmse:0.077762+0.000989	test-rmse:0.191816+0.010999 
[216]	train-rmse:0.077485+0.000986	test-rmse:0.191829+0.011023 
[217]	train-rmse:0.077215+0.000989	test-rmse:0.191843+0.010994 
[218]	train-rmse:0.077016+0.000910	test-rmse:0.191834+0.010999 
[219]	train-rmse:0.076747+0.000924	test-rmse:0.191862+0.011019 
[220]	train-rmse:0.076494+0.000922	test-rmse:0.191850+0.011026 
[221]	train-rmse:0.076230+0.000910	test-rmse:0.191839+0.011050 
[222]	train-rmse:0.075998+0.000934	test-rmse:0.191828+0.011020 
[223]	train-rmse:0.075758+0.000939	test-rmse:0.191847+0.011035 
[224]	train-rmse:0.075520+0.000907	test-rmse:0.191829+0.011045 
[225]	train-rmse:0.075290+0.000918	test-rmse:0.191836+0.011038 
[226]	train-rmse:0.075016+0.000902	test-rmse:0.191854+0.011071 
[227]	train-rmse:0.074770+0.000917	test-rmse:0.191858+0.011055 
[228]	train-rmse:0.074535+0.000927	test-rmse:0.191826+0.011046 
[229]	train-rmse:0.074275+0.000920	test-rmse:0.191847+0.011056 
[230]	train-rmse:0.074023+0.000910	test-rmse:0.191845+0.011075 
[231]	train-rmse:0.073778+0.000892	test-rmse:0.191838+0.011088 
[232]	train-rmse:0.073533+0.000886	test-rmse:0.191837+0.011124 
[233]	train-rmse:0.073311+0.000876	test-rmse:0.191842+0.011166 
[234]	train-rmse:0.073087+0.000874	test-rmse:0.191840+0.011146 
[235]	train-rmse:0.072898+0.000849	test-rmse:0.191844+0.011189 
[236]	train-rmse:0.072643+0.000852	test-rmse:0.191843+0.011190 
[237]	train-rmse:0.072427+0.000839	test-rmse:0.191842+0.011188 
[238]	train-rmse:0.072204+0.000844	test-rmse:0.191833+0.011178 
[239]	train-rmse:0.071974+0.000842	test-rmse:0.191839+0.011173 
[240]	train-rmse:0.071756+0.000871	test-rmse:0.191825+0.011177 
[241]	train-rmse:0.071549+0.000868	test-rmse:0.191835+0.011198 
[242]	train-rmse:0.071313+0.000882	test-rmse:0.191814+0.011199 
[243]	train-rmse:0.071065+0.000888	test-rmse:0.191816+0.011205 
[244]	train-rmse:0.070874+0.000886	test-rmse:0.191817+0.011193 
[245]	train-rmse:0.070674+0.000888	test-rmse:0.191828+0.011243 
[246]	train-rmse:0.070464+0.000900	test-rmse:0.191832+0.011255 
[247]	train-rmse:0.070238+0.000890	test-rmse:0.191850+0.011262 
[248]	train-rmse:0.070035+0.000902	test-rmse:0.191890+0.011304 
[249]	train-rmse:0.069771+0.000885	test-rmse:0.191924+0.011333 
[250]	train-rmse:0.069568+0.000851	test-rmse:0.191935+0.011340 
[251]	train-rmse:0.069342+0.000869	test-rmse:0.191945+0.011362 
[252]	train-rmse:0.069125+0.000858	test-rmse:0.191909+0.011375 
[253]	train-rmse:0.068931+0.000844	test-rmse:0.191931+0.011380 
[254]	train-rmse:0.068718+0.000858	test-rmse:0.191926+0.011408 
[255]	train-rmse:0.068548+0.000864	test-rmse:0.191930+0.011428 
[256]	train-rmse:0.068339+0.000873	test-rmse:0.191898+0.011441 
Stopping. Best iteration:
[206]	train-rmse:0.080246+0.000991	test-rmse:0.191625+0.010975

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
