> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.360165+0.000870	test-rmse:0.360567+0.003786 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.341201+0.001320	test-rmse:0.342225+0.003219 
[3]	train-rmse:0.323790+0.001764	test-rmse:0.325330+0.002428 
[4]	train-rmse:0.311794+0.004606	test-rmse:0.313891+0.002756 
[5]	train-rmse:0.297444+0.004175	test-rmse:0.300226+0.002300 
[6]	train-rmse:0.285323+0.003504	test-rmse:0.288680+0.001701 
[7]	train-rmse:0.273879+0.003578	test-rmse:0.278077+0.001320 
[8]	train-rmse:0.266491+0.000786	test-rmse:0.270946+0.003890 
[9]	train-rmse:0.261301+0.002828	test-rmse:0.266255+0.004269 
[10]	train-rmse:0.253838+0.002317	test-rmse:0.259601+0.003444 
[11]	train-rmse:0.249073+0.002151	test-rmse:0.255250+0.004466 
[12]	train-rmse:0.246693+0.002635	test-rmse:0.253061+0.004181 
[13]	train-rmse:0.241778+0.004498	test-rmse:0.248711+0.005142 
[14]	train-rmse:0.240170+0.004641	test-rmse:0.247180+0.005315 
[15]	train-rmse:0.238205+0.004015	test-rmse:0.245354+0.005673 
[16]	train-rmse:0.234497+0.004486	test-rmse:0.241853+0.005156 
[17]	train-rmse:0.232678+0.004535	test-rmse:0.240150+0.004697 
[18]	train-rmse:0.230588+0.005068	test-rmse:0.238224+0.004278 
[19]	train-rmse:0.229510+0.004826	test-rmse:0.237218+0.004060 
[20]	train-rmse:0.227831+0.004667	test-rmse:0.235714+0.003665 
[21]	train-rmse:0.225410+0.004043	test-rmse:0.233438+0.003812 
[22]	train-rmse:0.223148+0.005204	test-rmse:0.231530+0.003059 
[23]	train-rmse:0.222250+0.004665	test-rmse:0.230714+0.003148 
[24]	train-rmse:0.220805+0.005600	test-rmse:0.229491+0.002032 
[25]	train-rmse:0.219896+0.005249	test-rmse:0.228660+0.002469 
[26]	train-rmse:0.219269+0.005321	test-rmse:0.228150+0.002382 
[27]	train-rmse:0.217475+0.006037	test-rmse:0.226669+0.002336 
[28]	train-rmse:0.216924+0.005851	test-rmse:0.226204+0.002478 
[29]	train-rmse:0.215792+0.006419	test-rmse:0.225318+0.001957 
[30]	train-rmse:0.215286+0.006332	test-rmse:0.224852+0.002036 
[31]	train-rmse:0.214341+0.006298	test-rmse:0.224006+0.002782 
[32]	train-rmse:0.213900+0.006212	test-rmse:0.223624+0.002846 
[33]	train-rmse:0.212822+0.005718	test-rmse:0.222591+0.003435 
[34]	train-rmse:0.212417+0.005608	test-rmse:0.222241+0.003497 
[35]	train-rmse:0.212016+0.005477	test-rmse:0.221874+0.003569 
[36]	train-rmse:0.211100+0.005135	test-rmse:0.221047+0.004315 
[37]	train-rmse:0.210776+0.005049	test-rmse:0.220808+0.004382 
[38]	train-rmse:0.209914+0.004236	test-rmse:0.219983+0.004611 
[39]	train-rmse:0.209679+0.004253	test-rmse:0.219785+0.004599 
[40]	train-rmse:0.209397+0.004259	test-rmse:0.219558+0.004642 
[41]	train-rmse:0.209117+0.004218	test-rmse:0.219326+0.004641 
[42]	train-rmse:0.208884+0.004185	test-rmse:0.219099+0.004644 
[43]	train-rmse:0.208347+0.004646	test-rmse:0.218708+0.004410 
[44]	train-rmse:0.208143+0.004626	test-rmse:0.218539+0.004418 
[45]	train-rmse:0.207954+0.004594	test-rmse:0.218373+0.004455 
[46]	train-rmse:0.207776+0.004579	test-rmse:0.218214+0.004464 
[47]	train-rmse:0.206945+0.003862	test-rmse:0.217431+0.004952 
[48]	train-rmse:0.206645+0.003764	test-rmse:0.217156+0.005067 
[49]	train-rmse:0.206344+0.003674	test-rmse:0.216916+0.005108 
[50]	train-rmse:0.206236+0.003651	test-rmse:0.216806+0.005162 
[51]	train-rmse:0.206040+0.003608	test-rmse:0.216605+0.005227 
[52]	train-rmse:0.205885+0.003546	test-rmse:0.216478+0.005250 
[53]	train-rmse:0.205427+0.004158	test-rmse:0.216163+0.004998 
[54]	train-rmse:0.205204+0.004153	test-rmse:0.215977+0.005055 
[55]	train-rmse:0.205042+0.004126	test-rmse:0.215841+0.005117 
[56]	train-rmse:0.204853+0.004074	test-rmse:0.215662+0.005161 
[57]	train-rmse:0.204707+0.004049	test-rmse:0.215530+0.005196 
[58]	train-rmse:0.204592+0.004029	test-rmse:0.215427+0.005238 
[59]	train-rmse:0.204426+0.004016	test-rmse:0.215274+0.005224 
[60]	train-rmse:0.204285+0.004001	test-rmse:0.215158+0.005243 
[61]	train-rmse:0.204167+0.004010	test-rmse:0.215052+0.005251 
[62]	train-rmse:0.203706+0.004688	test-rmse:0.214709+0.004959 
[63]	train-rmse:0.203217+0.004383	test-rmse:0.214258+0.005323 
[64]	train-rmse:0.203073+0.004337	test-rmse:0.214129+0.005290 
[65]	train-rmse:0.202976+0.004333	test-rmse:0.214051+0.005313 
[66]	train-rmse:0.202885+0.004319	test-rmse:0.213976+0.005306 
[67]	train-rmse:0.202815+0.004305	test-rmse:0.213918+0.005329 
[68]	train-rmse:0.202158+0.004127	test-rmse:0.213380+0.005000 
[69]	train-rmse:0.202055+0.004091	test-rmse:0.213272+0.005035 
[70]	train-rmse:0.201524+0.003825	test-rmse:0.212825+0.005459 
[71]	train-rmse:0.201422+0.003797	test-rmse:0.212749+0.005453 
[72]	train-rmse:0.201303+0.003769	test-rmse:0.212658+0.005450 
[73]	train-rmse:0.201218+0.003778	test-rmse:0.212592+0.005466 
[74]	train-rmse:0.201130+0.003772	test-rmse:0.212499+0.005487 
[75]	train-rmse:0.200415+0.003415	test-rmse:0.211882+0.005854 
[76]	train-rmse:0.200363+0.003408	test-rmse:0.211830+0.005868 
[77]	train-rmse:0.200281+0.003377	test-rmse:0.211759+0.005875 
[78]	train-rmse:0.199549+0.003124	test-rmse:0.211014+0.006111 
[79]	train-rmse:0.199490+0.003110	test-rmse:0.210959+0.006123 
[80]	train-rmse:0.199435+0.003100	test-rmse:0.210919+0.006139 
[81]	train-rmse:0.199365+0.003078	test-rmse:0.210857+0.006148 
[82]	train-rmse:0.199051+0.003054	test-rmse:0.210578+0.006405 
[83]	train-rmse:0.198994+0.003052	test-rmse:0.210512+0.006395 
[84]	train-rmse:0.198934+0.003051	test-rmse:0.210466+0.006409 
[85]	train-rmse:0.198890+0.003043	test-rmse:0.210420+0.006414 
[86]	train-rmse:0.198840+0.003044	test-rmse:0.210371+0.006396 
[87]	train-rmse:0.198505+0.003012	test-rmse:0.209980+0.006275 
[88]	train-rmse:0.198204+0.003455	test-rmse:0.209815+0.006075 
[89]	train-rmse:0.198139+0.003438	test-rmse:0.209749+0.006084 
[90]	train-rmse:0.198092+0.003435	test-rmse:0.209706+0.006092 
[91]	train-rmse:0.197784+0.003442	test-rmse:0.209457+0.006362 
[92]	train-rmse:0.197732+0.003448	test-rmse:0.209413+0.006355 
[93]	train-rmse:0.197500+0.003239	test-rmse:0.209167+0.006552 
[94]	train-rmse:0.197467+0.003247	test-rmse:0.209139+0.006553 
[95]	train-rmse:0.197427+0.003229	test-rmse:0.209096+0.006567 
[96]	train-rmse:0.197406+0.003226	test-rmse:0.209077+0.006578 
[97]	train-rmse:0.197360+0.003233	test-rmse:0.209042+0.006578 
[98]	train-rmse:0.197310+0.003213	test-rmse:0.209004+0.006578 
[99]	train-rmse:0.197275+0.003215	test-rmse:0.208988+0.006579 
[100]	train-rmse:0.197239+0.003233	test-rmse:0.208961+0.006562 
[101]	train-rmse:0.197222+0.003239	test-rmse:0.208951+0.006553 
[102]	train-rmse:0.197171+0.003225	test-rmse:0.208908+0.006574 
[103]	train-rmse:0.197135+0.003215	test-rmse:0.208876+0.006584 
[104]	train-rmse:0.197101+0.003200	test-rmse:0.208849+0.006593 
[105]	train-rmse:0.197071+0.003191	test-rmse:0.208824+0.006600 
[106]	train-rmse:0.197053+0.003185	test-rmse:0.208809+0.006603 
[107]	train-rmse:0.197001+0.003176	test-rmse:0.208766+0.006611 
[108]	train-rmse:0.196958+0.003163	test-rmse:0.208734+0.006615 
[109]	train-rmse:0.196696+0.003226	test-rmse:0.208513+0.006870 
[110]	train-rmse:0.196373+0.003216	test-rmse:0.208230+0.006801 
[111]	train-rmse:0.196332+0.003204	test-rmse:0.208194+0.006807 
[112]	train-rmse:0.196025+0.003351	test-rmse:0.207955+0.007101 
[113]	train-rmse:0.195795+0.003372	test-rmse:0.207742+0.007079 
[114]	train-rmse:0.195766+0.003375	test-rmse:0.207718+0.007081 
[115]	train-rmse:0.195568+0.003614	test-rmse:0.207585+0.006889 
[116]	train-rmse:0.195544+0.003615	test-rmse:0.207568+0.006887 
[117]	train-rmse:0.195254+0.003341	test-rmse:0.207286+0.007090 
[118]	train-rmse:0.195071+0.003424	test-rmse:0.207131+0.007247 
[119]	train-rmse:0.195056+0.003421	test-rmse:0.207120+0.007250 
[120]	train-rmse:0.195027+0.003414	test-rmse:0.207097+0.007246 
[121]	train-rmse:0.194930+0.003524	test-rmse:0.207029+0.007152 
[122]	train-rmse:0.194887+0.003517	test-rmse:0.206999+0.007148 
[123]	train-rmse:0.194700+0.003539	test-rmse:0.206842+0.007125 
[124]	train-rmse:0.194668+0.003540	test-rmse:0.206821+0.007132 
[125]	train-rmse:0.194650+0.003536	test-rmse:0.206805+0.007133 
[126]	train-rmse:0.194623+0.003539	test-rmse:0.206785+0.007134 
[127]	train-rmse:0.194600+0.003531	test-rmse:0.206770+0.007137 
[128]	train-rmse:0.194569+0.003520	test-rmse:0.206746+0.007139 
[129]	train-rmse:0.194433+0.003590	test-rmse:0.206612+0.007301 
[130]	train-rmse:0.194313+0.003489	test-rmse:0.206501+0.007375 
[131]	train-rmse:0.194283+0.003475	test-rmse:0.206477+0.007368 
[132]	train-rmse:0.194264+0.003464	test-rmse:0.206466+0.007369 
[133]	train-rmse:0.194241+0.003455	test-rmse:0.206450+0.007373 
[134]	train-rmse:0.194221+0.003452	test-rmse:0.206431+0.007376 
[135]	train-rmse:0.193984+0.003503	test-rmse:0.206272+0.007354 
[136]	train-rmse:0.193972+0.003499	test-rmse:0.206259+0.007360 
[137]	train-rmse:0.193957+0.003498	test-rmse:0.206245+0.007352 
[138]	train-rmse:0.193935+0.003497	test-rmse:0.206226+0.007350 
[139]	train-rmse:0.193911+0.003478	test-rmse:0.206212+0.007357 
[140]	train-rmse:0.193886+0.003470	test-rmse:0.206199+0.007359 
[141]	train-rmse:0.193564+0.003071	test-rmse:0.205897+0.007298 
[142]	train-rmse:0.193542+0.003068	test-rmse:0.205881+0.007303 
[143]	train-rmse:0.193349+0.002879	test-rmse:0.205685+0.007447 
[144]	train-rmse:0.193321+0.002884	test-rmse:0.205663+0.007440 
[145]	train-rmse:0.193297+0.002885	test-rmse:0.205646+0.007436 
[146]	train-rmse:0.193277+0.002881	test-rmse:0.205630+0.007429 
[147]	train-rmse:0.193049+0.002618	test-rmse:0.205429+0.007422 
[148]	train-rmse:0.193031+0.002616	test-rmse:0.205418+0.007425 
[149]	train-rmse:0.193015+0.002612	test-rmse:0.205412+0.007425 
[150]	train-rmse:0.192673+0.002302	test-rmse:0.205219+0.007581 
[151]	train-rmse:0.192656+0.002296	test-rmse:0.205209+0.007585 
[152]	train-rmse:0.192640+0.002284	test-rmse:0.205194+0.007587 
[153]	train-rmse:0.192630+0.002286	test-rmse:0.205184+0.007587 
[154]	train-rmse:0.192621+0.002278	test-rmse:0.205177+0.007590 
[155]	train-rmse:0.192616+0.002276	test-rmse:0.205171+0.007589 
[156]	train-rmse:0.192606+0.002274	test-rmse:0.205164+0.007591 
[157]	train-rmse:0.192586+0.002269	test-rmse:0.205154+0.007588 
[158]	train-rmse:0.192570+0.002264	test-rmse:0.205141+0.007594 
[159]	train-rmse:0.192558+0.002255	test-rmse:0.205135+0.007602 
[160]	train-rmse:0.192547+0.002257	test-rmse:0.205123+0.007603 
[161]	train-rmse:0.192535+0.002262	test-rmse:0.205112+0.007607 
[162]	train-rmse:0.192524+0.002257	test-rmse:0.205100+0.007611 
[163]	train-rmse:0.192310+0.002366	test-rmse:0.204971+0.007743 
[164]	train-rmse:0.192299+0.002363	test-rmse:0.204961+0.007746 
[165]	train-rmse:0.192035+0.002592	test-rmse:0.204799+0.007930 
[166]	train-rmse:0.192018+0.002586	test-rmse:0.204788+0.007934 
[167]	train-rmse:0.191830+0.002779	test-rmse:0.204655+0.007967 
[168]	train-rmse:0.191817+0.002775	test-rmse:0.204645+0.007965 
[169]	train-rmse:0.191801+0.002770	test-rmse:0.204634+0.007964 
[170]	train-rmse:0.191791+0.002762	test-rmse:0.204627+0.007963 
[171]	train-rmse:0.191773+0.002751	test-rmse:0.204615+0.007966 
[172]	train-rmse:0.191756+0.002743	test-rmse:0.204598+0.007963 
[173]	train-rmse:0.191744+0.002736	test-rmse:0.204590+0.007967 
[174]	train-rmse:0.191487+0.002425	test-rmse:0.204362+0.007966 
[175]	train-rmse:0.191490+0.002422	test-rmse:0.204340+0.007989 
[176]	train-rmse:0.191482+0.002415	test-rmse:0.204329+0.007993 
[177]	train-rmse:0.191473+0.002411	test-rmse:0.204320+0.007995 
[178]	train-rmse:0.191282+0.002650	test-rmse:0.204194+0.007791 
[179]	train-rmse:0.191273+0.002651	test-rmse:0.204188+0.007794 
[180]	train-rmse:0.191025+0.002628	test-rmse:0.204009+0.007751 
[181]	train-rmse:0.191017+0.002622	test-rmse:0.204003+0.007752 
[182]	train-rmse:0.190927+0.002737	test-rmse:0.203926+0.007625 
[183]	train-rmse:0.190917+0.002731	test-rmse:0.203916+0.007626 
[184]	train-rmse:0.190907+0.002727	test-rmse:0.203909+0.007630 
[185]	train-rmse:0.190907+0.002725	test-rmse:0.203905+0.007629 
[186]	train-rmse:0.190667+0.002773	test-rmse:0.203725+0.007600 
[187]	train-rmse:0.190472+0.002565	test-rmse:0.203571+0.007719 
[188]	train-rmse:0.190469+0.002562	test-rmse:0.203571+0.007721 
[189]	train-rmse:0.190465+0.002555	test-rmse:0.203566+0.007721 
[190]	train-rmse:0.190450+0.002543	test-rmse:0.203552+0.007721 
[191]	train-rmse:0.190441+0.002536	test-rmse:0.203547+0.007726 
[192]	train-rmse:0.190336+0.002400	test-rmse:0.203480+0.007725 
[193]	train-rmse:0.190276+0.002435	test-rmse:0.203388+0.007822 
[194]	train-rmse:0.190269+0.002431	test-rmse:0.203383+0.007822 
[195]	train-rmse:0.190264+0.002425	test-rmse:0.203378+0.007825 
[196]	train-rmse:0.190259+0.002426	test-rmse:0.203373+0.007822 
[197]	train-rmse:0.190252+0.002423	test-rmse:0.203365+0.007819 
[198]	train-rmse:0.190242+0.002419	test-rmse:0.203357+0.007820 
[199]	train-rmse:0.190240+0.002418	test-rmse:0.203351+0.007819 
[200]	train-rmse:0.190235+0.002411	test-rmse:0.203346+0.007824 
[201]	train-rmse:0.190225+0.002403	test-rmse:0.203343+0.007825 
[202]	train-rmse:0.190082+0.002510	test-rmse:0.203259+0.007915 
[203]	train-rmse:0.189972+0.002638	test-rmse:0.203187+0.007800 
[204]	train-rmse:0.189973+0.002637	test-rmse:0.203184+0.007803 
[205]	train-rmse:0.189972+0.002637	test-rmse:0.203187+0.007802 
[206]	train-rmse:0.189969+0.002636	test-rmse:0.203183+0.007803 
[207]	train-rmse:0.189962+0.002636	test-rmse:0.203179+0.007804 
[208]	train-rmse:0.189961+0.002633	test-rmse:0.203175+0.007808 
[209]	train-rmse:0.189839+0.002486	test-rmse:0.203080+0.007810 
[210]	train-rmse:0.189834+0.002480	test-rmse:0.203075+0.007814 
[211]	train-rmse:0.189801+0.002524	test-rmse:0.203047+0.007776 
[212]	train-rmse:0.189794+0.002529	test-rmse:0.203043+0.007772 
[213]	train-rmse:0.189788+0.002532	test-rmse:0.203039+0.007772 
[214]	train-rmse:0.189559+0.002735	test-rmse:0.203081+0.007720 
[215]	train-rmse:0.189443+0.002689	test-rmse:0.202970+0.007758 
[216]	train-rmse:0.189433+0.002687	test-rmse:0.202963+0.007762 
[217]	train-rmse:0.189431+0.002687	test-rmse:0.202958+0.007762 
[218]	train-rmse:0.189427+0.002686	test-rmse:0.202954+0.007764 
[219]	train-rmse:0.189419+0.002686	test-rmse:0.202947+0.007761 
[220]	train-rmse:0.189412+0.002682	test-rmse:0.202941+0.007766 
[221]	train-rmse:0.189407+0.002688	test-rmse:0.202938+0.007763 
[222]	train-rmse:0.189357+0.002733	test-rmse:0.202859+0.007855 
[223]	train-rmse:0.189229+0.002594	test-rmse:0.202726+0.007954 
[224]	train-rmse:0.189224+0.002593	test-rmse:0.202722+0.007954 
[225]	train-rmse:0.189216+0.002592	test-rmse:0.202714+0.007955 
[226]	train-rmse:0.189097+0.002461	test-rmse:0.202648+0.007959 
[227]	train-rmse:0.189092+0.002460	test-rmse:0.202641+0.007960 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
