> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.357947+0.000923	test-rmse:0.360726+0.004062 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.336060+0.000977	test-rmse:0.341331+0.003859 
[3]	train-rmse:0.315508+0.000624	test-rmse:0.323567+0.004091 
[4]	train-rmse:0.296789+0.000501	test-rmse:0.307750+0.004086 
[5]	train-rmse:0.279808+0.000362	test-rmse:0.293862+0.004352 
[6]	train-rmse:0.264579+0.000908	test-rmse:0.281949+0.004332 
[7]	train-rmse:0.252167+0.004883	test-rmse:0.272516+0.005281 
[8]	train-rmse:0.240351+0.007551	test-rmse:0.263722+0.006876 
[9]	train-rmse:0.227957+0.007010	test-rmse:0.254817+0.006808 
[10]	train-rmse:0.217500+0.006975	test-rmse:0.247476+0.006928 
[11]	train-rmse:0.208731+0.008815	test-rmse:0.241568+0.008029 
[12]	train-rmse:0.200228+0.007812	test-rmse:0.236203+0.006736 
[13]	train-rmse:0.192966+0.010334	test-rmse:0.231736+0.008212 
[14]	train-rmse:0.187964+0.010368	test-rmse:0.228658+0.009072 
[15]	train-rmse:0.182302+0.011733	test-rmse:0.225259+0.009758 
[16]	train-rmse:0.176007+0.013562	test-rmse:0.221945+0.010666 
[17]	train-rmse:0.168799+0.012519	test-rmse:0.218294+0.010452 
[18]	train-rmse:0.163176+0.013471	test-rmse:0.215544+0.010789 
[19]	train-rmse:0.157202+0.012450	test-rmse:0.212545+0.010435 
[20]	train-rmse:0.153477+0.010660	test-rmse:0.210888+0.010023 
[21]	train-rmse:0.148302+0.009859	test-rmse:0.208720+0.009943 
[22]	train-rmse:0.143503+0.009068	test-rmse:0.206636+0.009668 
[23]	train-rmse:0.140238+0.007902	test-rmse:0.205361+0.009529 
[24]	train-rmse:0.136538+0.006906	test-rmse:0.203792+0.009333 
[25]	train-rmse:0.133021+0.008082	test-rmse:0.202702+0.009519 
[26]	train-rmse:0.129631+0.007386	test-rmse:0.201686+0.009709 
[27]	train-rmse:0.127832+0.007952	test-rmse:0.201305+0.009636 
[28]	train-rmse:0.125771+0.008459	test-rmse:0.200733+0.009850 
[29]	train-rmse:0.123094+0.007686	test-rmse:0.200041+0.010061 
[30]	train-rmse:0.120015+0.007047	test-rmse:0.199100+0.010120 
[31]	train-rmse:0.118755+0.005790	test-rmse:0.198792+0.010068 
[32]	train-rmse:0.117284+0.004934	test-rmse:0.198324+0.010018 
[33]	train-rmse:0.115317+0.005358	test-rmse:0.197765+0.010195 
[34]	train-rmse:0.114571+0.005635	test-rmse:0.197583+0.010219 
[35]	train-rmse:0.113129+0.006218	test-rmse:0.197276+0.010236 
[36]	train-rmse:0.111453+0.006434	test-rmse:0.196936+0.010409 
[37]	train-rmse:0.109339+0.005891	test-rmse:0.196561+0.010471 
[38]	train-rmse:0.107646+0.005561	test-rmse:0.196179+0.010458 
[39]	train-rmse:0.106319+0.005241	test-rmse:0.195819+0.010417 
[40]	train-rmse:0.105087+0.004218	test-rmse:0.195555+0.010408 
[41]	train-rmse:0.103852+0.004505	test-rmse:0.195316+0.010439 
[42]	train-rmse:0.102367+0.004836	test-rmse:0.195045+0.010513 
[43]	train-rmse:0.101531+0.005265	test-rmse:0.194943+0.010555 
[44]	train-rmse:0.100478+0.005410	test-rmse:0.194713+0.010656 
[45]	train-rmse:0.099661+0.004541	test-rmse:0.194544+0.010747 
[46]	train-rmse:0.098866+0.004870	test-rmse:0.194438+0.010796 
[47]	train-rmse:0.097725+0.005307	test-rmse:0.194261+0.010739 
[48]	train-rmse:0.096397+0.005020	test-rmse:0.194035+0.010716 
[49]	train-rmse:0.095465+0.005370	test-rmse:0.193783+0.010527 
[50]	train-rmse:0.093995+0.005072	test-rmse:0.193603+0.010457 
[51]	train-rmse:0.092786+0.004931	test-rmse:0.193503+0.010435 
[52]	train-rmse:0.092208+0.005095	test-rmse:0.193481+0.010358 
[53]	train-rmse:0.091513+0.005439	test-rmse:0.193347+0.010323 
[54]	train-rmse:0.090581+0.005863	test-rmse:0.193355+0.010354 
[55]	train-rmse:0.089292+0.005653	test-rmse:0.193258+0.010420 
[56]	train-rmse:0.088665+0.005675	test-rmse:0.193224+0.010464 
[57]	train-rmse:0.087244+0.005456	test-rmse:0.193187+0.010619 
[58]	train-rmse:0.086175+0.005168	test-rmse:0.193118+0.010660 
[59]	train-rmse:0.085503+0.004649	test-rmse:0.193069+0.010675 
[60]	train-rmse:0.084805+0.004945	test-rmse:0.193150+0.010641 
[61]	train-rmse:0.084075+0.004598	test-rmse:0.193004+0.010523 
[62]	train-rmse:0.083356+0.004521	test-rmse:0.192991+0.010579 
[63]	train-rmse:0.082654+0.004710	test-rmse:0.192943+0.010543 
[64]	train-rmse:0.082129+0.004700	test-rmse:0.192919+0.010534 
[65]	train-rmse:0.081643+0.004851	test-rmse:0.192894+0.010526 
[66]	train-rmse:0.080892+0.004709	test-rmse:0.192804+0.010518 
[67]	train-rmse:0.079949+0.004268	test-rmse:0.192770+0.010488 
[68]	train-rmse:0.079248+0.004288	test-rmse:0.192801+0.010530 
[69]	train-rmse:0.078707+0.004226	test-rmse:0.192772+0.010567 
[70]	train-rmse:0.077964+0.004572	test-rmse:0.192795+0.010460 
[71]	train-rmse:0.077107+0.004983	test-rmse:0.192890+0.010476 
[72]	train-rmse:0.076422+0.004815	test-rmse:0.192933+0.010481 
[73]	train-rmse:0.076401+0.004821	test-rmse:0.192928+0.010478 
[74]	train-rmse:0.075841+0.004496	test-rmse:0.192951+0.010460 
[75]	train-rmse:0.075506+0.004738	test-rmse:0.193029+0.010526 
[76]	train-rmse:0.075084+0.004710	test-rmse:0.192997+0.010549 
[77]	train-rmse:0.074441+0.004357	test-rmse:0.192965+0.010461 
[78]	train-rmse:0.073878+0.004349	test-rmse:0.192896+0.010359 
[79]	train-rmse:0.073512+0.004163	test-rmse:0.192953+0.010348 
[80]	train-rmse:0.072905+0.004384	test-rmse:0.192875+0.010325 
[81]	train-rmse:0.072485+0.004331	test-rmse:0.192960+0.010385 
[82]	train-rmse:0.072033+0.004085	test-rmse:0.192969+0.010338 
[83]	train-rmse:0.071575+0.004364	test-rmse:0.192955+0.010344 
[84]	train-rmse:0.071128+0.004624	test-rmse:0.192935+0.010347 
[85]	train-rmse:0.070506+0.004372	test-rmse:0.193036+0.010400 
[86]	train-rmse:0.070165+0.004298	test-rmse:0.193061+0.010393 
[87]	train-rmse:0.069753+0.004523	test-rmse:0.193074+0.010376 
[88]	train-rmse:0.069262+0.004498	test-rmse:0.193125+0.010448 
[89]	train-rmse:0.068644+0.004457	test-rmse:0.193155+0.010425 
[90]	train-rmse:0.068308+0.004443	test-rmse:0.193162+0.010435 
[91]	train-rmse:0.067951+0.004676	test-rmse:0.193288+0.010495 
[92]	train-rmse:0.067475+0.004677	test-rmse:0.193325+0.010497 
[93]	train-rmse:0.067054+0.004990	test-rmse:0.193312+0.010456 
[94]	train-rmse:0.066616+0.004920	test-rmse:0.193353+0.010473 
[95]	train-rmse:0.066369+0.004893	test-rmse:0.193363+0.010460 
[96]	train-rmse:0.066156+0.004812	test-rmse:0.193346+0.010498 
[97]	train-rmse:0.065902+0.004857	test-rmse:0.193416+0.010567 
[98]	train-rmse:0.065383+0.004742	test-rmse:0.193468+0.010589 
[99]	train-rmse:0.064921+0.004875	test-rmse:0.193532+0.010632 
[100]	train-rmse:0.064550+0.004911	test-rmse:0.193553+0.010629 
[101]	train-rmse:0.064176+0.005111	test-rmse:0.193621+0.010592 
[102]	train-rmse:0.063725+0.004748	test-rmse:0.193621+0.010580 
[103]	train-rmse:0.063599+0.004567	test-rmse:0.193612+0.010579 
[104]	train-rmse:0.063235+0.004627	test-rmse:0.193670+0.010532 
[105]	train-rmse:0.063030+0.004729	test-rmse:0.193694+0.010515 
[106]	train-rmse:0.062727+0.004488	test-rmse:0.193794+0.010506 
[107]	train-rmse:0.062127+0.004335	test-rmse:0.193856+0.010546 
[108]	train-rmse:0.061744+0.004239	test-rmse:0.193866+0.010583 
[109]	train-rmse:0.061271+0.004106	test-rmse:0.193854+0.010573 
[110]	train-rmse:0.060887+0.003933	test-rmse:0.193899+0.010558 
[111]	train-rmse:0.060578+0.004138	test-rmse:0.193885+0.010550 
[112]	train-rmse:0.060032+0.003966	test-rmse:0.193808+0.010511 
[113]	train-rmse:0.059692+0.003796	test-rmse:0.193877+0.010518 
[114]	train-rmse:0.059274+0.003893	test-rmse:0.193889+0.010473 
[115]	train-rmse:0.058867+0.003704	test-rmse:0.193948+0.010459 
[116]	train-rmse:0.058386+0.003818	test-rmse:0.193959+0.010473 
[117]	train-rmse:0.058072+0.003810	test-rmse:0.193976+0.010430 
Stopping. Best iteration:
[67]	train-rmse:0.079949+0.004268	test-rmse:0.192770+0.010488

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
