> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.370626+0.000961	test-rmse:0.371353+0.004081 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.359307+0.000904	test-rmse:0.360577+0.003945 
[3]	train-rmse:0.348502+0.000842	test-rmse:0.350477+0.003978 
[4]	train-rmse:0.338168+0.000651	test-rmse:0.341043+0.004143 
[5]	train-rmse:0.328300+0.000641	test-rmse:0.331818+0.004005 
[6]	train-rmse:0.318831+0.000707	test-rmse:0.323179+0.003680 
[7]	train-rmse:0.309969+0.000752	test-rmse:0.314941+0.003506 
[8]	train-rmse:0.301482+0.000765	test-rmse:0.307400+0.003535 
[9]	train-rmse:0.293437+0.000621	test-rmse:0.300221+0.003629 
[10]	train-rmse:0.285859+0.000600	test-rmse:0.293462+0.003740 
[11]	train-rmse:0.278571+0.000569	test-rmse:0.287116+0.003896 
[12]	train-rmse:0.271648+0.000702	test-rmse:0.281146+0.003960 
[13]	train-rmse:0.265038+0.000707	test-rmse:0.275518+0.004073 
[14]	train-rmse:0.258739+0.000643	test-rmse:0.270214+0.004167 
[15]	train-rmse:0.252800+0.000645	test-rmse:0.265244+0.004232 
[16]	train-rmse:0.247163+0.000713	test-rmse:0.260431+0.004424 
[17]	train-rmse:0.241712+0.000808	test-rmse:0.256123+0.004491 
[18]	train-rmse:0.236728+0.000895	test-rmse:0.252118+0.004480 
[19]	train-rmse:0.232330+0.000517	test-rmse:0.248541+0.005143 
[20]	train-rmse:0.228235+0.001307	test-rmse:0.245288+0.005859 
[21]	train-rmse:0.224391+0.002483	test-rmse:0.242268+0.006671 
[22]	train-rmse:0.220231+0.002396	test-rmse:0.239099+0.006900 
[23]	train-rmse:0.216128+0.002154	test-rmse:0.235949+0.006885 
[24]	train-rmse:0.212193+0.001999	test-rmse:0.233082+0.007085 
[25]	train-rmse:0.208563+0.001808	test-rmse:0.230508+0.007216 
[26]	train-rmse:0.205116+0.001657	test-rmse:0.228256+0.007211 
[27]	train-rmse:0.201689+0.001489	test-rmse:0.225828+0.007396 
[28]	train-rmse:0.198989+0.002305	test-rmse:0.223846+0.007936 
[29]	train-rmse:0.196010+0.002197	test-rmse:0.221656+0.008039 
[30]	train-rmse:0.193080+0.001981	test-rmse:0.219683+0.008120 
[31]	train-rmse:0.190191+0.001810	test-rmse:0.217840+0.008204 
[32]	train-rmse:0.187803+0.001618	test-rmse:0.216292+0.008275 
[33]	train-rmse:0.185510+0.001657	test-rmse:0.214981+0.008244 
[34]	train-rmse:0.183011+0.001544	test-rmse:0.213247+0.008311 
[35]	train-rmse:0.181062+0.001576	test-rmse:0.212005+0.008167 
[36]	train-rmse:0.178943+0.001509	test-rmse:0.210779+0.008221 
[37]	train-rmse:0.176795+0.001429	test-rmse:0.209436+0.008277 
[38]	train-rmse:0.174748+0.001340	test-rmse:0.208398+0.008344 
[39]	train-rmse:0.172695+0.001275	test-rmse:0.207439+0.008573 
[40]	train-rmse:0.171223+0.001735	test-rmse:0.206582+0.008901 
[41]	train-rmse:0.169325+0.001558	test-rmse:0.205549+0.009045 
[42]	train-rmse:0.167764+0.001796	test-rmse:0.204819+0.009204 
[43]	train-rmse:0.166037+0.001785	test-rmse:0.204039+0.009373 
[44]	train-rmse:0.164680+0.001385	test-rmse:0.203421+0.009431 
[45]	train-rmse:0.163072+0.001359	test-rmse:0.202552+0.009618 
[46]	train-rmse:0.161513+0.001213	test-rmse:0.201864+0.009572 
[47]	train-rmse:0.160302+0.001660	test-rmse:0.201247+0.009829 
[48]	train-rmse:0.159094+0.001471	test-rmse:0.200656+0.009795 
[49]	train-rmse:0.157942+0.001424	test-rmse:0.200181+0.009911 
[50]	train-rmse:0.156607+0.001279	test-rmse:0.199647+0.009820 
[51]	train-rmse:0.155389+0.001155	test-rmse:0.199106+0.009782 
[52]	train-rmse:0.154146+0.001058	test-rmse:0.198732+0.009855 
[53]	train-rmse:0.153137+0.000914	test-rmse:0.198401+0.009718 
[54]	train-rmse:0.152191+0.000828	test-rmse:0.198121+0.009788 
[55]	train-rmse:0.151208+0.000864	test-rmse:0.197794+0.009852 
[56]	train-rmse:0.150260+0.001050	test-rmse:0.197526+0.009925 
[57]	train-rmse:0.149422+0.001147	test-rmse:0.197204+0.009961 
[58]	train-rmse:0.148547+0.001187	test-rmse:0.196839+0.010087 
[59]	train-rmse:0.147627+0.000787	test-rmse:0.196414+0.009948 
[60]	train-rmse:0.146567+0.000750	test-rmse:0.196186+0.010079 
[61]	train-rmse:0.145534+0.000756	test-rmse:0.195871+0.010036 
[62]	train-rmse:0.144576+0.000735	test-rmse:0.195671+0.010203 
[63]	train-rmse:0.143852+0.000609	test-rmse:0.195508+0.010035 
[64]	train-rmse:0.143083+0.000693	test-rmse:0.195400+0.010118 
[65]	train-rmse:0.142210+0.000661	test-rmse:0.195301+0.010131 
[66]	train-rmse:0.141403+0.000518	test-rmse:0.195104+0.009990 
[67]	train-rmse:0.140604+0.000434	test-rmse:0.194888+0.010012 
[68]	train-rmse:0.139954+0.000467	test-rmse:0.194753+0.009992 
[69]	train-rmse:0.139306+0.000538	test-rmse:0.194578+0.009946 
[70]	train-rmse:0.138661+0.000656	test-rmse:0.194405+0.009906 
[71]	train-rmse:0.137929+0.000515	test-rmse:0.194349+0.009987 
[72]	train-rmse:0.137172+0.000579	test-rmse:0.194219+0.010069 
[73]	train-rmse:0.136301+0.000561	test-rmse:0.194036+0.010159 
[74]	train-rmse:0.135589+0.000693	test-rmse:0.193880+0.010182 
[75]	train-rmse:0.134821+0.000724	test-rmse:0.193870+0.010089 
[76]	train-rmse:0.134102+0.000730	test-rmse:0.193722+0.010099 
[77]	train-rmse:0.133350+0.000760	test-rmse:0.193710+0.010217 
[78]	train-rmse:0.132768+0.000848	test-rmse:0.193554+0.010076 
[79]	train-rmse:0.132022+0.000897	test-rmse:0.193537+0.010100 
[80]	train-rmse:0.131552+0.000871	test-rmse:0.193454+0.010111 
[81]	train-rmse:0.130974+0.001027	test-rmse:0.193476+0.010224 
[82]	train-rmse:0.130443+0.001130	test-rmse:0.193387+0.010238 
[83]	train-rmse:0.129815+0.001023	test-rmse:0.193248+0.010130 
[84]	train-rmse:0.129119+0.001098	test-rmse:0.193059+0.010264 
[85]	train-rmse:0.128635+0.001179	test-rmse:0.192990+0.010314 
[86]	train-rmse:0.128205+0.001291	test-rmse:0.192993+0.010365 
[87]	train-rmse:0.127698+0.001435	test-rmse:0.192897+0.010395 
[88]	train-rmse:0.127152+0.001482	test-rmse:0.192783+0.010449 
[89]	train-rmse:0.126796+0.001587	test-rmse:0.192710+0.010537 
[90]	train-rmse:0.126301+0.001675	test-rmse:0.192624+0.010473 
[91]	train-rmse:0.125830+0.001678	test-rmse:0.192564+0.010500 
[92]	train-rmse:0.125364+0.001773	test-rmse:0.192466+0.010550 
[93]	train-rmse:0.125006+0.001771	test-rmse:0.192416+0.010520 
[94]	train-rmse:0.124463+0.001904	test-rmse:0.192401+0.010370 
[95]	train-rmse:0.124002+0.001889	test-rmse:0.192319+0.010395 
[96]	train-rmse:0.123410+0.001857	test-rmse:0.192190+0.010393 
[97]	train-rmse:0.122981+0.002050	test-rmse:0.192143+0.010335 
[98]	train-rmse:0.122531+0.001991	test-rmse:0.192104+0.010276 
[99]	train-rmse:0.122174+0.001852	test-rmse:0.192005+0.010229 
[100]	train-rmse:0.121597+0.001935	test-rmse:0.192005+0.010295 
[101]	train-rmse:0.121227+0.001964	test-rmse:0.191835+0.010348 
[102]	train-rmse:0.120672+0.001879	test-rmse:0.191698+0.010383 
[103]	train-rmse:0.120283+0.001913	test-rmse:0.191672+0.010489 
[104]	train-rmse:0.119736+0.001930	test-rmse:0.191636+0.010632 
[105]	train-rmse:0.119304+0.001997	test-rmse:0.191663+0.010535 
[106]	train-rmse:0.118837+0.002099	test-rmse:0.191552+0.010467 
[107]	train-rmse:0.118389+0.002230	test-rmse:0.191458+0.010601 
[108]	train-rmse:0.117877+0.002186	test-rmse:0.191457+0.010625 
[109]	train-rmse:0.117374+0.002222	test-rmse:0.191340+0.010740 
[110]	train-rmse:0.116864+0.002189	test-rmse:0.191467+0.010643 
[111]	train-rmse:0.116654+0.002225	test-rmse:0.191480+0.010541 
[112]	train-rmse:0.116085+0.002178	test-rmse:0.191455+0.010662 
[113]	train-rmse:0.115685+0.002199	test-rmse:0.191374+0.010705 
[114]	train-rmse:0.115195+0.002165	test-rmse:0.191398+0.010640 
[115]	train-rmse:0.114767+0.002238	test-rmse:0.191321+0.010629 
[116]	train-rmse:0.114297+0.002229	test-rmse:0.191270+0.010670 
[117]	train-rmse:0.113836+0.002242	test-rmse:0.191240+0.010589 
[118]	train-rmse:0.113448+0.002047	test-rmse:0.191298+0.010564 
[119]	train-rmse:0.113113+0.002055	test-rmse:0.191379+0.010542 
[120]	train-rmse:0.112786+0.001902	test-rmse:0.191419+0.010540 
[121]	train-rmse:0.112408+0.001844	test-rmse:0.191449+0.010497 
[122]	train-rmse:0.111981+0.001852	test-rmse:0.191503+0.010491 
[123]	train-rmse:0.111468+0.001899	test-rmse:0.191558+0.010419 
[124]	train-rmse:0.111110+0.001902	test-rmse:0.191534+0.010343 
[125]	train-rmse:0.110645+0.001941	test-rmse:0.191536+0.010373 
[126]	train-rmse:0.110356+0.002070	test-rmse:0.191585+0.010406 
[127]	train-rmse:0.110035+0.002090	test-rmse:0.191685+0.010491 
[128]	train-rmse:0.109663+0.002185	test-rmse:0.191662+0.010485 
[129]	train-rmse:0.109183+0.002199	test-rmse:0.191736+0.010467 
[130]	train-rmse:0.108753+0.002261	test-rmse:0.191836+0.010526 
[131]	train-rmse:0.108293+0.002174	test-rmse:0.191811+0.010503 
[132]	train-rmse:0.107921+0.002214	test-rmse:0.191911+0.010494 
[133]	train-rmse:0.107582+0.002089	test-rmse:0.191851+0.010504 
[134]	train-rmse:0.107348+0.002198	test-rmse:0.191888+0.010535 
[135]	train-rmse:0.107038+0.002239	test-rmse:0.191942+0.010585 
[136]	train-rmse:0.106719+0.002317	test-rmse:0.192023+0.010698 
[137]	train-rmse:0.106222+0.002313	test-rmse:0.192028+0.010685 
[138]	train-rmse:0.105932+0.002293	test-rmse:0.192047+0.010668 
[139]	train-rmse:0.105613+0.002383	test-rmse:0.192117+0.010575 
[140]	train-rmse:0.105146+0.002294	test-rmse:0.192080+0.010572 
[141]	train-rmse:0.104801+0.002454	test-rmse:0.192090+0.010522 
[142]	train-rmse:0.104386+0.002441	test-rmse:0.192147+0.010537 
[143]	train-rmse:0.104029+0.002433	test-rmse:0.192157+0.010514 
[144]	train-rmse:0.103818+0.002372	test-rmse:0.192134+0.010525 
[145]	train-rmse:0.103460+0.002346	test-rmse:0.192059+0.010571 
[146]	train-rmse:0.103150+0.002446	test-rmse:0.192068+0.010533 
[147]	train-rmse:0.102737+0.002507	test-rmse:0.192076+0.010522 
[148]	train-rmse:0.102521+0.002571	test-rmse:0.192133+0.010561 
[149]	train-rmse:0.102102+0.002559	test-rmse:0.192209+0.010540 
[150]	train-rmse:0.101806+0.002507	test-rmse:0.192226+0.010453 
[151]	train-rmse:0.101494+0.002530	test-rmse:0.192275+0.010468 
[152]	train-rmse:0.101159+0.002633	test-rmse:0.192342+0.010420 
[153]	train-rmse:0.100957+0.002485	test-rmse:0.192444+0.010373 
[154]	train-rmse:0.100663+0.002441	test-rmse:0.192538+0.010430 
[155]	train-rmse:0.100218+0.002439	test-rmse:0.192511+0.010330 
[156]	train-rmse:0.099869+0.002414	test-rmse:0.192533+0.010321 
[157]	train-rmse:0.099448+0.002410	test-rmse:0.192567+0.010368 
[158]	train-rmse:0.099099+0.002341	test-rmse:0.192568+0.010381 
[159]	train-rmse:0.098763+0.002315	test-rmse:0.192551+0.010324 
[160]	train-rmse:0.098394+0.002277	test-rmse:0.192588+0.010297 
[161]	train-rmse:0.098060+0.002196	test-rmse:0.192540+0.010347 
[162]	train-rmse:0.097765+0.002064	test-rmse:0.192645+0.010302 
[163]	train-rmse:0.097461+0.002191	test-rmse:0.192655+0.010365 
[164]	train-rmse:0.097217+0.002180	test-rmse:0.192660+0.010344 
[165]	train-rmse:0.096826+0.002187	test-rmse:0.192641+0.010466 
[166]	train-rmse:0.096456+0.002189	test-rmse:0.192628+0.010491 
[167]	train-rmse:0.096066+0.002173	test-rmse:0.192563+0.010537 
Stopping. Best iteration:
[117]	train-rmse:0.113836+0.002242	test-rmse:0.191240+0.010589

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
