> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.382575+0.001054	test-rmse:0.382554+0.004187 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.382468+0.001053	test-rmse:0.382450+0.004186 
[3]	train-rmse:0.382404+0.001042	test-rmse:0.382388+0.004195 
[4]	train-rmse:0.382341+0.001064	test-rmse:0.382325+0.004173 
[5]	train-rmse:0.382298+0.001061	test-rmse:0.382283+0.004176 
[6]	train-rmse:0.382210+0.001077	test-rmse:0.382197+0.004159 
[7]	train-rmse:0.382173+0.001109	test-rmse:0.382161+0.004127 
[8]	train-rmse:0.382140+0.001110	test-rmse:0.382129+0.004125 
[9]	train-rmse:0.382108+0.001107	test-rmse:0.382098+0.004130 
[10]	train-rmse:0.382077+0.001092	test-rmse:0.382069+0.004143 
[11]	train-rmse:0.382029+0.001092	test-rmse:0.382021+0.004146 
[12]	train-rmse:0.381961+0.001056	test-rmse:0.381956+0.004182 
[13]	train-rmse:0.381952+0.001055	test-rmse:0.381947+0.004182 
[14]	train-rmse:0.381924+0.001058	test-rmse:0.381920+0.004180 
[15]	train-rmse:0.381897+0.001057	test-rmse:0.381893+0.004186 
[16]	train-rmse:0.381890+0.001057	test-rmse:0.381886+0.004186 
[17]	train-rmse:0.381883+0.001057	test-rmse:0.381879+0.004186 
[18]	train-rmse:0.381857+0.001057	test-rmse:0.381853+0.004193 
[19]	train-rmse:0.381831+0.001060	test-rmse:0.381828+0.004200 
[20]	train-rmse:0.381825+0.001060	test-rmse:0.381822+0.004200 
[21]	train-rmse:0.381759+0.001022	test-rmse:0.381758+0.004239 
[22]	train-rmse:0.381754+0.001022	test-rmse:0.381753+0.004239 
[23]	train-rmse:0.381729+0.001057	test-rmse:0.381728+0.004201 
[24]	train-rmse:0.381704+0.001091	test-rmse:0.381704+0.004164 
[25]	train-rmse:0.381700+0.001091	test-rmse:0.381699+0.004164 
[26]	train-rmse:0.381674+0.001075	test-rmse:0.381675+0.004178 
[27]	train-rmse:0.381630+0.001096	test-rmse:0.381632+0.004155 
[28]	train-rmse:0.381585+0.001075	test-rmse:0.381588+0.004180 
[29]	train-rmse:0.381581+0.001075	test-rmse:0.381585+0.004180 
[30]	train-rmse:0.381557+0.001074	test-rmse:0.381561+0.004177 
[31]	train-rmse:0.381533+0.001110	test-rmse:0.381537+0.004138 
[32]	train-rmse:0.381509+0.001087	test-rmse:0.381513+0.004156 
[33]	train-rmse:0.381485+0.001072	test-rmse:0.381491+0.004170 
[34]	train-rmse:0.381440+0.001060	test-rmse:0.381449+0.004192 
[35]	train-rmse:0.381437+0.001060	test-rmse:0.381446+0.004192 
[36]	train-rmse:0.381434+0.001060	test-rmse:0.381443+0.004192 
[37]	train-rmse:0.381411+0.001096	test-rmse:0.381419+0.004154 
[38]	train-rmse:0.381387+0.001133	test-rmse:0.381396+0.004116 
[39]	train-rmse:0.381384+0.001133	test-rmse:0.381393+0.004116 
[40]	train-rmse:0.381361+0.001110	test-rmse:0.381370+0.004134 
[41]	train-rmse:0.381358+0.001110	test-rmse:0.381367+0.004134 
[42]	train-rmse:0.381356+0.001110	test-rmse:0.381365+0.004134 
[43]	train-rmse:0.381333+0.001112	test-rmse:0.381342+0.004141 
[44]	train-rmse:0.381309+0.001090	test-rmse:0.381319+0.004160 
[45]	train-rmse:0.381307+0.001090	test-rmse:0.381317+0.004160 
[46]	train-rmse:0.381304+0.001090	test-rmse:0.381314+0.004160 
[47]	train-rmse:0.381302+0.001090	test-rmse:0.381312+0.004160 
[48]	train-rmse:0.381259+0.001124	test-rmse:0.381269+0.004118 
[49]	train-rmse:0.381215+0.001146	test-rmse:0.381227+0.004095 
[50]	train-rmse:0.381151+0.001130	test-rmse:0.381166+0.004113 
[51]	train-rmse:0.381149+0.001130	test-rmse:0.381164+0.004113 
[52]	train-rmse:0.381126+0.001132	test-rmse:0.381141+0.004121 
[53]	train-rmse:0.381083+0.001146	test-rmse:0.381098+0.004100 
[54]	train-rmse:0.381081+0.001146	test-rmse:0.381096+0.004100 
[55]	train-rmse:0.380975+0.001145	test-rmse:0.380992+0.004099 
[56]	train-rmse:0.380973+0.001145	test-rmse:0.380990+0.004099 
[57]	train-rmse:0.380950+0.001183	test-rmse:0.380968+0.004060 
[58]	train-rmse:0.380928+0.001221	test-rmse:0.380945+0.004023 
[59]	train-rmse:0.380885+0.001261	test-rmse:0.380902+0.003992 
[60]	train-rmse:0.380862+0.001264	test-rmse:0.380879+0.004001 
[61]	train-rmse:0.380839+0.001250	test-rmse:0.380858+0.004015 
[62]	train-rmse:0.380816+0.001227	test-rmse:0.380835+0.004034 
[63]	train-rmse:0.380793+0.001231	test-rmse:0.380813+0.004043 
[64]	train-rmse:0.380771+0.001226	test-rmse:0.380791+0.004038 
[65]	train-rmse:0.380728+0.001258	test-rmse:0.380749+0.003995 
[66]	train-rmse:0.380706+0.001263	test-rmse:0.380726+0.004003 
[67]	train-rmse:0.380663+0.001304	test-rmse:0.380684+0.003974 
[68]	train-rmse:0.380641+0.001341	test-rmse:0.380661+0.003936 
[69]	train-rmse:0.380640+0.001341	test-rmse:0.380660+0.003936 
[70]	train-rmse:0.380638+0.001341	test-rmse:0.380658+0.003936 
[71]	train-rmse:0.380616+0.001326	test-rmse:0.380638+0.003950 
[72]	train-rmse:0.380614+0.001326	test-rmse:0.380636+0.003950 
[73]	train-rmse:0.380592+0.001363	test-rmse:0.380614+0.003911 
[74]	train-rmse:0.380591+0.001363	test-rmse:0.380613+0.003911 
[75]	train-rmse:0.380568+0.001349	test-rmse:0.380592+0.003926 
[76]	train-rmse:0.380525+0.001348	test-rmse:0.380549+0.003930 
[77]	train-rmse:0.380461+0.001367	test-rmse:0.380487+0.003901 
[78]	train-rmse:0.380419+0.001380	test-rmse:0.380445+0.003882 
[79]	train-rmse:0.380418+0.001380	test-rmse:0.380444+0.003882 
[80]	train-rmse:0.380374+0.001363	test-rmse:0.380402+0.003892 
[81]	train-rmse:0.380309+0.001322	test-rmse:0.380341+0.003921 
[82]	train-rmse:0.380266+0.001347	test-rmse:0.380299+0.003899 
[83]	train-rmse:0.380222+0.001312	test-rmse:0.380258+0.003933 
[84]	train-rmse:0.380200+0.001302	test-rmse:0.380237+0.003949 
[85]	train-rmse:0.380157+0.001341	test-rmse:0.380194+0.003918 
[86]	train-rmse:0.380135+0.001343	test-rmse:0.380172+0.003926 
[87]	train-rmse:0.380134+0.001343	test-rmse:0.380171+0.003926 
[88]	train-rmse:0.380069+0.001346	test-rmse:0.380108+0.003921 
[89]	train-rmse:0.380047+0.001340	test-rmse:0.380086+0.003916 
[90]	train-rmse:0.380025+0.001377	test-rmse:0.380064+0.003878 
[91]	train-rmse:0.380003+0.001372	test-rmse:0.380043+0.003873 
[92]	train-rmse:0.380002+0.001372	test-rmse:0.380042+0.003873 
[93]	train-rmse:0.380001+0.001372	test-rmse:0.380041+0.003873 
[94]	train-rmse:0.380000+0.001372	test-rmse:0.380040+0.003873 
[95]	train-rmse:0.379999+0.001372	test-rmse:0.380039+0.003873 
[96]	train-rmse:0.379977+0.001373	test-rmse:0.380017+0.003881 
[97]	train-rmse:0.379955+0.001376	test-rmse:0.379995+0.003890 
[98]	train-rmse:0.379913+0.001354	test-rmse:0.379954+0.003916 
[99]	train-rmse:0.379869+0.001338	test-rmse:0.379912+0.003927 
[100]	train-rmse:0.379868+0.001338	test-rmse:0.379911+0.003927 
[101]	train-rmse:0.379847+0.001375	test-rmse:0.379889+0.003889 
[102]	train-rmse:0.379845+0.001375	test-rmse:0.379888+0.003889 
[103]	train-rmse:0.379823+0.001350	test-rmse:0.379867+0.003906 
[104]	train-rmse:0.379822+0.001350	test-rmse:0.379866+0.003906 
[105]	train-rmse:0.379780+0.001348	test-rmse:0.379825+0.003910 
[106]	train-rmse:0.379758+0.001385	test-rmse:0.379803+0.003872 
[107]	train-rmse:0.379716+0.001398	test-rmse:0.379761+0.003852 
[108]	train-rmse:0.379694+0.001401	test-rmse:0.379740+0.003860 
[109]	train-rmse:0.379693+0.001401	test-rmse:0.379739+0.003860 
[110]	train-rmse:0.379692+0.001401	test-rmse:0.379738+0.003860 
[111]	train-rmse:0.379671+0.001396	test-rmse:0.379716+0.003855 
[112]	train-rmse:0.379649+0.001400	test-rmse:0.379694+0.003865 
[113]	train-rmse:0.379627+0.001388	test-rmse:0.379674+0.003880 
[114]	train-rmse:0.379626+0.001388	test-rmse:0.379673+0.003879 
[115]	train-rmse:0.379604+0.001383	test-rmse:0.379651+0.003875 
[116]	train-rmse:0.379582+0.001388	test-rmse:0.379630+0.003884 
[117]	train-rmse:0.379581+0.001388	test-rmse:0.379629+0.003884 
[118]	train-rmse:0.379560+0.001362	test-rmse:0.379608+0.003901 
[119]	train-rmse:0.379538+0.001338	test-rmse:0.379586+0.003919 
[120]	train-rmse:0.379516+0.001344	test-rmse:0.379564+0.003929 
[121]	train-rmse:0.379473+0.001315	test-rmse:0.379523+0.003942 
[122]	train-rmse:0.379451+0.001292	test-rmse:0.379502+0.003960 
[123]	train-rmse:0.379430+0.001298	test-rmse:0.379480+0.003969 
[124]	train-rmse:0.379388+0.001311	test-rmse:0.379439+0.003950 
[125]	train-rmse:0.379325+0.001280	test-rmse:0.379378+0.003991 
[126]	train-rmse:0.379262+0.001308	test-rmse:0.379317+0.003976 
[127]	train-rmse:0.379220+0.001279	test-rmse:0.379276+0.003989 
[128]	train-rmse:0.379178+0.001257	test-rmse:0.379236+0.003999 
[129]	train-rmse:0.379156+0.001235	test-rmse:0.379215+0.004017 
[130]	train-rmse:0.379155+0.001235	test-rmse:0.379214+0.004017 
[131]	train-rmse:0.379133+0.001214	test-rmse:0.379193+0.004036 
[132]	train-rmse:0.379112+0.001220	test-rmse:0.379172+0.004045 
[133]	train-rmse:0.379111+0.001220	test-rmse:0.379171+0.004045 
[134]	train-rmse:0.379110+0.001220	test-rmse:0.379170+0.004045 
[135]	train-rmse:0.379046+0.001234	test-rmse:0.379109+0.004016 
[136]	train-rmse:0.379045+0.001234	test-rmse:0.379108+0.004016 
[137]	train-rmse:0.379003+0.001256	test-rmse:0.379067+0.003993 
[138]	train-rmse:0.379002+0.001256	test-rmse:0.379066+0.003993 
[139]	train-rmse:0.379001+0.001256	test-rmse:0.379066+0.003993 
[140]	train-rmse:0.378917+0.001251	test-rmse:0.378984+0.003983 
[141]	train-rmse:0.378895+0.001237	test-rmse:0.378964+0.003997 
[142]	train-rmse:0.378873+0.001225	test-rmse:0.378944+0.004012 
[143]	train-rmse:0.378831+0.001267	test-rmse:0.378902+0.003981 
[144]	train-rmse:0.378768+0.001270	test-rmse:0.378841+0.003977 
[145]	train-rmse:0.378747+0.001249	test-rmse:0.378820+0.003996 
[146]	train-rmse:0.378725+0.001287	test-rmse:0.378799+0.003959 
[147]	train-rmse:0.378725+0.001287	test-rmse:0.378798+0.003959 
[148]	train-rmse:0.378703+0.001276	test-rmse:0.378778+0.003974 
[149]	train-rmse:0.378682+0.001314	test-rmse:0.378757+0.003937 
[150]	train-rmse:0.378661+0.001305	test-rmse:0.378736+0.003931 
[151]	train-rmse:0.378640+0.001342	test-rmse:0.378715+0.003895 
[152]	train-rmse:0.378639+0.001342	test-rmse:0.378715+0.003895 
[153]	train-rmse:0.378618+0.001334	test-rmse:0.378694+0.003889 
[154]	train-rmse:0.378596+0.001327	test-rmse:0.378673+0.003883 
[155]	train-rmse:0.378596+0.001327	test-rmse:0.378673+0.003883 
[156]	train-rmse:0.378575+0.001329	test-rmse:0.378651+0.003891 
[157]	train-rmse:0.378574+0.001328	test-rmse:0.378651+0.003891 
[158]	train-rmse:0.378573+0.001328	test-rmse:0.378650+0.003891 
[159]	train-rmse:0.378573+0.001328	test-rmse:0.378649+0.003891 
[160]	train-rmse:0.378530+0.001311	test-rmse:0.378609+0.003901 
[161]	train-rmse:0.378508+0.001288	test-rmse:0.378587+0.003921 
[162]	train-rmse:0.378486+0.001266	test-rmse:0.378566+0.003939 
[163]	train-rmse:0.378444+0.001298	test-rmse:0.378525+0.003896 
[164]	train-rmse:0.378423+0.001277	test-rmse:0.378504+0.003916 
[165]	train-rmse:0.378422+0.001277	test-rmse:0.378503+0.003916 
[166]	train-rmse:0.378380+0.001252	test-rmse:0.378462+0.003930 
[167]	train-rmse:0.378380+0.001252	test-rmse:0.378461+0.003930 
[168]	train-rmse:0.378359+0.001290	test-rmse:0.378440+0.003893 
[169]	train-rmse:0.378317+0.001266	test-rmse:0.378399+0.003908 
[170]	train-rmse:0.378294+0.001252	test-rmse:0.378379+0.003923 
[171]	train-rmse:0.378253+0.001272	test-rmse:0.378338+0.003905 
[172]	train-rmse:0.378252+0.001272	test-rmse:0.378337+0.003905 
[173]	train-rmse:0.378231+0.001270	test-rmse:0.378316+0.003912 
[174]	train-rmse:0.378210+0.001269	test-rmse:0.378295+0.003919 
[175]	train-rmse:0.378209+0.001269	test-rmse:0.378294+0.003919 
[176]	train-rmse:0.378208+0.001269	test-rmse:0.378294+0.003919 
[177]	train-rmse:0.378187+0.001255	test-rmse:0.378274+0.003934 
[178]	train-rmse:0.378165+0.001249	test-rmse:0.378253+0.003929 
[179]	train-rmse:0.378165+0.001249	test-rmse:0.378253+0.003929 
[180]	train-rmse:0.378123+0.001235	test-rmse:0.378211+0.003951 
[181]	train-rmse:0.378122+0.001235	test-rmse:0.378211+0.003951 
[182]	train-rmse:0.378101+0.001216	test-rmse:0.378190+0.003970 
[183]	train-rmse:0.378100+0.001216	test-rmse:0.378190+0.003970 
[184]	train-rmse:0.378079+0.001254	test-rmse:0.378169+0.003933 
[185]	train-rmse:0.378016+0.001235	test-rmse:0.378108+0.003950 
[186]	train-rmse:0.378016+0.001235	test-rmse:0.378108+0.003950 
[187]	train-rmse:0.377994+0.001235	test-rmse:0.378087+0.003958 
[188]	train-rmse:0.377994+0.001235	test-rmse:0.378086+0.003958 
[189]	train-rmse:0.377993+0.001235	test-rmse:0.378086+0.003958 
[190]	train-rmse:0.377973+0.001273	test-rmse:0.378065+0.003921 
[191]	train-rmse:0.377931+0.001306	test-rmse:0.378023+0.003877 
[192]	train-rmse:0.377888+0.001280	test-rmse:0.377981+0.003891 
[193]	train-rmse:0.377847+0.001299	test-rmse:0.377940+0.003874 
[194]	train-rmse:0.377846+0.001299	test-rmse:0.377940+0.003874 
[195]	train-rmse:0.377825+0.001285	test-rmse:0.377921+0.003887 
[196]	train-rmse:0.377824+0.001285	test-rmse:0.377920+0.003887 
[197]	train-rmse:0.377803+0.001324	test-rmse:0.377899+0.003850 
[198]	train-rmse:0.377762+0.001306	test-rmse:0.377860+0.003860 
[199]	train-rmse:0.377761+0.001306	test-rmse:0.377860+0.003860 
[200]	train-rmse:0.377761+0.001306	test-rmse:0.377859+0.003860 
[201]	train-rmse:0.377760+0.001306	test-rmse:0.377859+0.003860 
[202]	train-rmse:0.377696+0.001269	test-rmse:0.377798+0.003890 
[203]	train-rmse:0.377654+0.001254	test-rmse:0.377758+0.003900 
[204]	train-rmse:0.377654+0.001254	test-rmse:0.377758+0.003900 
[205]	train-rmse:0.377653+0.001254	test-rmse:0.377757+0.003900 
[206]	train-rmse:0.377611+0.001239	test-rmse:0.377717+0.003922 
[207]	train-rmse:0.377590+0.001218	test-rmse:0.377696+0.003941 
[208]	train-rmse:0.377568+0.001214	test-rmse:0.377675+0.003937 
[209]	train-rmse:0.377568+0.001214	test-rmse:0.377675+0.003937 
[210]	train-rmse:0.377484+0.001175	test-rmse:0.377594+0.003974 
[211]	train-rmse:0.377463+0.001172	test-rmse:0.377573+0.003980 
[212]	train-rmse:0.377442+0.001169	test-rmse:0.377553+0.003977 
[213]	train-rmse:0.377441+0.001169	test-rmse:0.377552+0.003977 
[214]	train-rmse:0.377420+0.001208	test-rmse:0.377532+0.003939 
[215]	train-rmse:0.377399+0.001196	test-rmse:0.377512+0.003954 
[216]	train-rmse:0.377378+0.001194	test-rmse:0.377491+0.003961 
[217]	train-rmse:0.377336+0.001182	test-rmse:0.377451+0.003983 
[218]	train-rmse:0.377315+0.001179	test-rmse:0.377431+0.003979 
[219]	train-rmse:0.377314+0.001179	test-rmse:0.377431+0.003979 
[220]	train-rmse:0.377314+0.001179	test-rmse:0.377430+0.003979 
[221]	train-rmse:0.377292+0.001170	test-rmse:0.377410+0.003994 
[222]	train-rmse:0.377230+0.001204	test-rmse:0.377349+0.003960 
[223]	train-rmse:0.377167+0.001206	test-rmse:0.377289+0.003956 
[224]	train-rmse:0.377167+0.001206	test-rmse:0.377288+0.003956 
[225]	train-rmse:0.377125+0.001220	test-rmse:0.377247+0.003936 
[226]	train-rmse:0.377104+0.001218	test-rmse:0.377226+0.003943 
[227]	train-rmse:0.377083+0.001217	test-rmse:0.377205+0.003951 
[228]	train-rmse:0.377083+0.001217	test-rmse:0.377205+0.003951 
[229]	train-rmse:0.377062+0.001208	test-rmse:0.377186+0.003965 
[230]	train-rmse:0.377061+0.001208	test-rmse:0.377185+0.003965 
[231]	train-rmse:0.377040+0.001208	test-rmse:0.377164+0.003973 
[232]	train-rmse:0.376977+0.001232	test-rmse:0.377103+0.003946 
[233]	train-rmse:0.376977+0.001232	test-rmse:0.377103+0.003946 
[234]	train-rmse:0.376934+0.001199	test-rmse:0.377063+0.003979 
[235]	train-rmse:0.376934+0.001199	test-rmse:0.377062+0.003979 
[236]	train-rmse:0.376933+0.001199	test-rmse:0.377062+0.003979 
[237]	train-rmse:0.376933+0.001199	test-rmse:0.377061+0.003979 
[238]	train-rmse:0.376932+0.001199	test-rmse:0.377061+0.003979 
[239]	train-rmse:0.376870+0.001208	test-rmse:0.376999+0.003954 
[240]	train-rmse:0.376807+0.001194	test-rmse:0.376939+0.003972 
[241]	train-rmse:0.376786+0.001169	test-rmse:0.376918+0.003991 
[242]	train-rmse:0.376744+0.001143	test-rmse:0.376877+0.004016 
[243]	train-rmse:0.376723+0.001140	test-rmse:0.376857+0.004013 
[244]	train-rmse:0.376702+0.001177	test-rmse:0.376836+0.003975 
[245]	train-rmse:0.376681+0.001177	test-rmse:0.376815+0.003983 
[246]	train-rmse:0.376661+0.001174	test-rmse:0.376794+0.003979 
[247]	train-rmse:0.376619+0.001208	test-rmse:0.376753+0.003938 
[248]	train-rmse:0.376619+0.001208	test-rmse:0.376753+0.003938 
[249]	train-rmse:0.376577+0.001197	test-rmse:0.376713+0.003949 
[250]	train-rmse:0.376535+0.001208	test-rmse:0.376671+0.003929 
[251]	train-rmse:0.376493+0.001183	test-rmse:0.376631+0.003945 
[252]	train-rmse:0.376493+0.001183	test-rmse:0.376630+0.003945 
[253]	train-rmse:0.376430+0.001149	test-rmse:0.376571+0.003974 
[254]	train-rmse:0.376409+0.001126	test-rmse:0.376551+0.003993 
[255]	train-rmse:0.376409+0.001126	test-rmse:0.376550+0.003993 
[256]	train-rmse:0.376387+0.001115	test-rmse:0.376530+0.004008 
[257]	train-rmse:0.376346+0.001153	test-rmse:0.376490+0.003967 
[258]	train-rmse:0.376345+0.001153	test-rmse:0.376490+0.003967 
[259]	train-rmse:0.376325+0.001190	test-rmse:0.376469+0.003930 
[260]	train-rmse:0.376303+0.001180	test-rmse:0.376449+0.003945 
[261]	train-rmse:0.376240+0.001210	test-rmse:0.376388+0.003920 
[262]	train-rmse:0.376239+0.001210	test-rmse:0.376388+0.003920 
[263]	train-rmse:0.376177+0.001179	test-rmse:0.376328+0.003951 
[264]	train-rmse:0.376156+0.001182	test-rmse:0.376308+0.003949 
[265]	train-rmse:0.376135+0.001174	test-rmse:0.376287+0.003954 
[266]	train-rmse:0.376135+0.001174	test-rmse:0.376287+0.003954 
[267]	train-rmse:0.376113+0.001167	test-rmse:0.376267+0.003970 
[268]	train-rmse:0.376113+0.001167	test-rmse:0.376267+0.003970 
[269]	train-rmse:0.376071+0.001145	test-rmse:0.376225+0.003985 
[270]	train-rmse:0.376070+0.001145	test-rmse:0.376225+0.003985 
[271]	train-rmse:0.376049+0.001137	test-rmse:0.376204+0.003991 
[272]	train-rmse:0.376029+0.001131	test-rmse:0.376184+0.003997 
[273]	train-rmse:0.376028+0.001131	test-rmse:0.376183+0.003997 
[274]	train-rmse:0.376028+0.001131	test-rmse:0.376183+0.003997 
[275]	train-rmse:0.375986+0.001159	test-rmse:0.376143+0.003974 
[276]	train-rmse:0.375965+0.001162	test-rmse:0.376123+0.003972 
[277]	train-rmse:0.375944+0.001167	test-rmse:0.376102+0.003970 
[278]	train-rmse:0.375882+0.001200	test-rmse:0.376042+0.003946 
[279]	train-rmse:0.375861+0.001235	test-rmse:0.376021+0.003908 
[280]	train-rmse:0.375800+0.001237	test-rmse:0.375961+0.003892 
[281]	train-rmse:0.375778+0.001229	test-rmse:0.375941+0.003907 
[282]	train-rmse:0.375777+0.001229	test-rmse:0.375941+0.003907 
[283]	train-rmse:0.375757+0.001266	test-rmse:0.375919+0.003868 
[284]	train-rmse:0.375715+0.001234	test-rmse:0.375880+0.003901 
[285]	train-rmse:0.375714+0.001234	test-rmse:0.375880+0.003901 
[286]	train-rmse:0.375714+0.001234	test-rmse:0.375879+0.003901 
[287]	train-rmse:0.375713+0.001234	test-rmse:0.375879+0.003901 
[288]	train-rmse:0.375692+0.001237	test-rmse:0.375859+0.003899 
[289]	train-rmse:0.375692+0.001237	test-rmse:0.375858+0.003899 
[290]	train-rmse:0.375651+0.001249	test-rmse:0.375818+0.003878 
[291]	train-rmse:0.375629+0.001243	test-rmse:0.375798+0.003894 
[292]	train-rmse:0.375608+0.001234	test-rmse:0.375778+0.003899 
[293]	train-rmse:0.375566+0.001265	test-rmse:0.375738+0.003876 
[294]	train-rmse:0.375566+0.001265	test-rmse:0.375737+0.003876 
[295]	train-rmse:0.375545+0.001240	test-rmse:0.375717+0.003895 
[296]	train-rmse:0.375504+0.001252	test-rmse:0.375676+0.003874 
[297]	train-rmse:0.375483+0.001230	test-rmse:0.375656+0.003893 
[298]	train-rmse:0.375483+0.001230	test-rmse:0.375655+0.003893 
[299]	train-rmse:0.375442+0.001198	test-rmse:0.375614+0.003917 
[300]	train-rmse:0.375421+0.001191	test-rmse:0.375596+0.003932 
[301]	train-rmse:0.375421+0.001191	test-rmse:0.375595+0.003932 
[302]	train-rmse:0.375359+0.001208	test-rmse:0.375535+0.003910 
[303]	train-rmse:0.375338+0.001202	test-rmse:0.375515+0.003926 
[304]	train-rmse:0.375276+0.001213	test-rmse:0.375455+0.003925 
[305]	train-rmse:0.375255+0.001201	test-rmse:0.375434+0.003929 
[306]	train-rmse:0.375234+0.001201	test-rmse:0.375414+0.003926 
[307]	train-rmse:0.375214+0.001196	test-rmse:0.375395+0.003941 
[308]	train-rmse:0.375213+0.001196	test-rmse:0.375394+0.003941 
[309]	train-rmse:0.375172+0.001194	test-rmse:0.375355+0.003954 
[310]	train-rmse:0.375130+0.001158	test-rmse:0.375313+0.003978 
[311]	train-rmse:0.375109+0.001147	test-rmse:0.375293+0.003982 
[312]	train-rmse:0.375088+0.001143	test-rmse:0.375274+0.003998 
[313]	train-rmse:0.375047+0.001178	test-rmse:0.375235+0.003977 
[314]	train-rmse:0.375026+0.001178	test-rmse:0.375215+0.003974 
[315]	train-rmse:0.375006+0.001215	test-rmse:0.375194+0.003935 
[316]	train-rmse:0.374964+0.001180	test-rmse:0.375153+0.003958 
[317]	train-rmse:0.374943+0.001180	test-rmse:0.375133+0.003955 
[318]	train-rmse:0.374943+0.001180	test-rmse:0.375133+0.003955 
[319]	train-rmse:0.374943+0.001180	test-rmse:0.375132+0.003955 
[320]	train-rmse:0.374922+0.001169	test-rmse:0.375112+0.003960 
[321]	train-rmse:0.374880+0.001142	test-rmse:0.375072+0.003994 
[322]	train-rmse:0.374880+0.001142	test-rmse:0.375072+0.003994 
[323]	train-rmse:0.374879+0.001142	test-rmse:0.375072+0.003994 
[324]	train-rmse:0.374879+0.001142	test-rmse:0.375072+0.003994 
[325]	train-rmse:0.374858+0.001142	test-rmse:0.375051+0.003991 
[326]	train-rmse:0.374837+0.001119	test-rmse:0.375031+0.004010 
> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
> 
