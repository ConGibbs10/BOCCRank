> 
> # parse arguments
> args <- R.utils::commandArgs(trailingOnly = FALSE, asValues = TRUE)
> index <- as.integer(args$index)
> year <- as.character(args$year)
> nreps <- as.integer(args$nreps)
> seed <- as.integer(args$seed)
> 
> # get number of cores
> wkrs <- length(future::availableWorkers())
> 
> # set seed
> set.seed(seed)
> 
> # read and prepare the cluster features
> fp <- file.path('data-raw', 'features', year)
> clf <-
+   read_features(fp, pattern = 'paris.', recursive = FALSE)
> clf <- prepare_features(
+   clf,
+   feature_map = feature_map,
+   category = c('response', 'bio', 'net'),
+   type = 'list',
+   exclusions = c(
+     'mg2_pairs_count',
+     'mg2_not_pairs_count',
+     'mg2_portion_families_recovered',
+     'cluster_origin',
+     'bocc_origin',
+     'cluster_method',
+     'subcluster_method',
+     'year',
+     'IDs',
+     'go_sig_threshold',
+     'num_new_edges_on_any_node',
+     'HPO_ratio'
+   ),
+   verbose = TRUE
+ )
c('HPO_ratio', 'IDs', 'bocc_origin', 'cluster_method', 'cluster_origin', 'go_sig_threshold', 'max_norm_cell_type_comma_sep_string', 'max_norm_disease_comma_sep_string', 'mg2_not_pairs_count', 'mg2_pairs_count', 'mg2_portion_families_recovered', 'num_new_edges_on_any_node', 'sig_go_enrichment_fdr_corrected_p_vals', 'sig_go_enrichment_p_vals', 'sig_go_enrichment_terms', 'subcluster_method', 'year')

> 
> # prepare grid of parameters
> xgboost_params <- dials::parameters(
+   nrounds = dials::trees(),
+   eta = dials::learn_rate(),
+   gamma = dials::loss_reduction(),
+   dials::tree_depth(),
+   subsample = dials::sample_prop(),
+   rate_drop = dials::dropout(),
+   skip_drop = dials::dropout()
+ )
> xgboost_params <-
+   dials::grid_max_entropy(xgboost_params, size = nreps)
> 
> # tune the dart booster
> dart.param <-
+   list(
+     booster = "dart",
+     objective = "reg:logistic",
+     eval_metric = "rmse",
+     nthread = wkrs,
+     max_depth = xgboost_params$tree_depth[index],
+     eta = xgboost_params$eta[index],
+     gamma = xgboost_params$gamma[index],
+     subsample = xgboost_params$subsample[index],
+     rate_drop = xgboost_params$rate_drop[index],
+     skip_drop = xgboost_params$skip_drop[index]
+   )
> dart.cv <- xgboost::xgb.cv(
+   params = dart.param,
+   nrounds = xgboost_params$nrounds[index],
+   data = as.matrix(clf[, -1]),
+   nfold = 5,
+   label = clf$snowballing_pvalue,
+   early_stopping_rounds = 50
+ )
[1]	train-rmse:0.363766+0.001002	test-rmse:0.365504+0.003676 
Multiple eval metrics are present. Will use test_rmse for early stopping.
Will train until test_rmse hasn't improved in 50 rounds.

[2]	train-rmse:0.347169+0.000964	test-rmse:0.350635+0.003126 
[3]	train-rmse:0.336087+0.003297	test-rmse:0.340750+0.005927 
[4]	train-rmse:0.326827+0.004051	test-rmse:0.332339+0.004866 
[5]	train-rmse:0.318762+0.008944	test-rmse:0.325144+0.007941 
[6]	train-rmse:0.313300+0.008417	test-rmse:0.320355+0.008891 
[7]	train-rmse:0.310705+0.008316	test-rmse:0.318042+0.008987 
[8]	train-rmse:0.308578+0.008217	test-rmse:0.316203+0.008998 
[9]	train-rmse:0.304537+0.010599	test-rmse:0.312652+0.011782 
[10]	train-rmse:0.300652+0.012369	test-rmse:0.309153+0.012262 
[11]	train-rmse:0.297205+0.015136	test-rmse:0.306103+0.013871 
[12]	train-rmse:0.289130+0.018182	test-rmse:0.299443+0.016428 
[13]	train-rmse:0.285703+0.017819	test-rmse:0.296614+0.016494 
[14]	train-rmse:0.282458+0.017114	test-rmse:0.294151+0.015586 
[15]	train-rmse:0.275358+0.020907	test-rmse:0.288355+0.019084 
[16]	train-rmse:0.274655+0.020702	test-rmse:0.287766+0.018931 
[17]	train-rmse:0.271416+0.016246	test-rmse:0.284899+0.015099 
[18]	train-rmse:0.270801+0.016131	test-rmse:0.284399+0.015023 
[19]	train-rmse:0.270226+0.016073	test-rmse:0.283928+0.014980 
[20]	train-rmse:0.267746+0.017646	test-rmse:0.281940+0.016736 
[21]	train-rmse:0.267208+0.017541	test-rmse:0.281490+0.016654 
[22]	train-rmse:0.264883+0.019617	test-rmse:0.279614+0.018630 
[23]	train-rmse:0.264421+0.019508	test-rmse:0.279231+0.018547 
[24]	train-rmse:0.259468+0.016130	test-rmse:0.275321+0.016229 
[25]	train-rmse:0.257395+0.018380	test-rmse:0.273640+0.018243 
[26]	train-rmse:0.253183+0.019087	test-rmse:0.270601+0.018718 
[27]	train-rmse:0.249502+0.022266	test-rmse:0.267803+0.020959 
[28]	train-rmse:0.247050+0.021233	test-rmse:0.266023+0.019846 
[29]	train-rmse:0.246760+0.021159	test-rmse:0.265783+0.019783 
[30]	train-rmse:0.244488+0.020973	test-rmse:0.264284+0.019167 
[31]	train-rmse:0.240066+0.016684	test-rmse:0.260770+0.016336 
[32]	train-rmse:0.238408+0.018922	test-rmse:0.259690+0.017853 
[33]	train-rmse:0.236145+0.016178	test-rmse:0.257877+0.016021 
[34]	train-rmse:0.234612+0.018612	test-rmse:0.256792+0.017742 
[35]	train-rmse:0.230895+0.018198	test-rmse:0.253941+0.017594 
[36]	train-rmse:0.230720+0.018142	test-rmse:0.253803+0.017550 
[37]	train-rmse:0.230546+0.018084	test-rmse:0.253671+0.017506 
[38]	train-rmse:0.227547+0.020693	test-rmse:0.251469+0.019270 
[39]	train-rmse:0.225548+0.019869	test-rmse:0.249973+0.018169 
[40]	train-rmse:0.225401+0.019822	test-rmse:0.249856+0.018126 
[41]	train-rmse:0.225251+0.019771	test-rmse:0.249740+0.018088 
[42]	train-rmse:0.223438+0.019363	test-rmse:0.248371+0.018000 
[43]	train-rmse:0.221615+0.019061	test-rmse:0.247209+0.017331 
[44]	train-rmse:0.219551+0.016014	test-rmse:0.245681+0.015469 
[45]	train-rmse:0.217664+0.013638	test-rmse:0.244348+0.014140 
[46]	train-rmse:0.217548+0.013605	test-rmse:0.244254+0.014112 
[47]	train-rmse:0.214385+0.013435	test-rmse:0.241878+0.014204 
[48]	train-rmse:0.214282+0.013400	test-rmse:0.241791+0.014179 
[49]	train-rmse:0.214183+0.013372	test-rmse:0.241712+0.014159 
[50]	train-rmse:0.212628+0.013507	test-rmse:0.240521+0.014429 
[51]	train-rmse:0.211208+0.014162	test-rmse:0.239638+0.014832 
[52]	train-rmse:0.208190+0.014464	test-rmse:0.237727+0.014414 
[53]	train-rmse:0.208118+0.014434	test-rmse:0.237668+0.014392 
[54]	train-rmse:0.208042+0.014410	test-rmse:0.237607+0.014376 
[55]	train-rmse:0.206788+0.015613	test-rmse:0.236806+0.015008 
[56]	train-rmse:0.206717+0.015586	test-rmse:0.236752+0.014985 
[57]	train-rmse:0.204987+0.012970	test-rmse:0.235419+0.013449 
[58]	train-rmse:0.203364+0.010896	test-rmse:0.234314+0.012459 
[59]	train-rmse:0.201929+0.010572	test-rmse:0.233370+0.012321 
[60]	train-rmse:0.200391+0.008885	test-rmse:0.232325+0.011580 
[61]	train-rmse:0.197916+0.010561	test-rmse:0.230742+0.012249 
[62]	train-rmse:0.195686+0.012472	test-rmse:0.229381+0.012961 
[63]	train-rmse:0.195643+0.012453	test-rmse:0.229345+0.012951 
[64]	train-rmse:0.194186+0.010904	test-rmse:0.228497+0.011730 
[65]	train-rmse:0.193023+0.011623	test-rmse:0.227769+0.012008 
[66]	train-rmse:0.190796+0.012810	test-rmse:0.226287+0.013125 
[67]	train-rmse:0.187295+0.012578	test-rmse:0.223909+0.013435 
[68]	train-rmse:0.186311+0.013696	test-rmse:0.223332+0.013842 
[69]	train-rmse:0.185236+0.014270	test-rmse:0.222723+0.014445 
[70]	train-rmse:0.182983+0.013756	test-rmse:0.221592+0.013779 
[71]	train-rmse:0.181960+0.014260	test-rmse:0.221127+0.013977 
[72]	train-rmse:0.181142+0.015272	test-rmse:0.220611+0.014395 
[73]	train-rmse:0.180133+0.015755	test-rmse:0.220057+0.014929 
[74]	train-rmse:0.179257+0.016351	test-rmse:0.219517+0.015500 
[75]	train-rmse:0.179236+0.016331	test-rmse:0.219499+0.015487 
[76]	train-rmse:0.177166+0.016087	test-rmse:0.218626+0.014882 
[77]	train-rmse:0.175930+0.015243	test-rmse:0.217994+0.014067 
[78]	train-rmse:0.175162+0.016230	test-rmse:0.217649+0.014353 
[79]	train-rmse:0.175149+0.016214	test-rmse:0.217635+0.014343 
[80]	train-rmse:0.175133+0.016195	test-rmse:0.217624+0.014331 
[81]	train-rmse:0.175123+0.016180	test-rmse:0.217614+0.014319 
[82]	train-rmse:0.174332+0.016667	test-rmse:0.217090+0.014914 
[83]	train-rmse:0.172512+0.017063	test-rmse:0.216412+0.014499 
[84]	train-rmse:0.170221+0.014735	test-rmse:0.215028+0.012919 
[85]	train-rmse:0.169031+0.012880	test-rmse:0.214234+0.012131 
[86]	train-rmse:0.167495+0.013689	test-rmse:0.213398+0.012501 
[87]	train-rmse:0.166539+0.013413	test-rmse:0.213029+0.012014 
[88]	train-rmse:0.166533+0.013400	test-rmse:0.213018+0.012010 
[89]	train-rmse:0.165109+0.014639	test-rmse:0.212397+0.012682 
[90]	train-rmse:0.163211+0.012851	test-rmse:0.211297+0.012131 
[91]	train-rmse:0.163210+0.012839	test-rmse:0.211291+0.012129 
[92]	train-rmse:0.162175+0.011360	test-rmse:0.210677+0.011653 
[93]	train-rmse:0.159967+0.012158	test-rmse:0.209563+0.011643 
[94]	train-rmse:0.159030+0.010892	test-rmse:0.208998+0.011254 
[95]	train-rmse:0.157294+0.009539	test-rmse:0.208264+0.010572 
[96]	train-rmse:0.156584+0.009857	test-rmse:0.208005+0.010905 
[97]	train-rmse:0.155231+0.010125	test-rmse:0.207401+0.010840 
[98]	train-rmse:0.153878+0.011034	test-rmse:0.206912+0.010511 
[99]	train-rmse:0.153039+0.010013	test-rmse:0.206539+0.010302 
[100]	train-rmse:0.152253+0.009237	test-rmse:0.206086+0.010116 
[101]	train-rmse:0.152259+0.009231	test-rmse:0.206085+0.010114 
[102]	train-rmse:0.151557+0.009361	test-rmse:0.205855+0.009768 
[103]	train-rmse:0.150156+0.008700	test-rmse:0.205352+0.009385 
[104]	train-rmse:0.149636+0.009479	test-rmse:0.205277+0.009438 
[105]	train-rmse:0.149643+0.009473	test-rmse:0.205276+0.009437 
[106]	train-rmse:0.148453+0.009972	test-rmse:0.204958+0.009686 
[107]	train-rmse:0.148462+0.009967	test-rmse:0.204961+0.009684 
[108]	train-rmse:0.147895+0.010226	test-rmse:0.204752+0.009995 
[109]	train-rmse:0.146504+0.009843	test-rmse:0.204171+0.009567 
[110]	train-rmse:0.145147+0.009382	test-rmse:0.203701+0.009220 
[111]	train-rmse:0.145159+0.009378	test-rmse:0.203702+0.009220 
[112]	train-rmse:0.144128+0.010098	test-rmse:0.203386+0.009554 
[113]	train-rmse:0.142555+0.010145	test-rmse:0.202774+0.009770 
[114]	train-rmse:0.142024+0.010547	test-rmse:0.202575+0.009473 
[115]	train-rmse:0.140903+0.010313	test-rmse:0.202012+0.009127 
[116]	train-rmse:0.139095+0.009335	test-rmse:0.201365+0.008855 
[117]	train-rmse:0.138406+0.008359	test-rmse:0.201102+0.008771 
[118]	train-rmse:0.137905+0.008504	test-rmse:0.200927+0.009036 
[119]	train-rmse:0.137918+0.008501	test-rmse:0.200928+0.009034 
[120]	train-rmse:0.136988+0.008240	test-rmse:0.200485+0.008646 
[121]	train-rmse:0.136109+0.009161	test-rmse:0.200146+0.008550 
[122]	train-rmse:0.134596+0.008905	test-rmse:0.199611+0.008793 
[123]	train-rmse:0.134609+0.008900	test-rmse:0.199612+0.008793 
[124]	train-rmse:0.133664+0.008673	test-rmse:0.199352+0.008713 
[125]	train-rmse:0.133678+0.008670	test-rmse:0.199354+0.008711 
[126]	train-rmse:0.133692+0.008667	test-rmse:0.199355+0.008712 
[127]	train-rmse:0.133309+0.009203	test-rmse:0.199251+0.008544 
[128]	train-rmse:0.132879+0.009557	test-rmse:0.199049+0.008638 
[129]	train-rmse:0.132526+0.010046	test-rmse:0.198959+0.008494 
[130]	train-rmse:0.132165+0.010366	test-rmse:0.198819+0.008572 
[131]	train-rmse:0.131644+0.009811	test-rmse:0.198592+0.008528 
[132]	train-rmse:0.131658+0.009806	test-rmse:0.198593+0.008527 
[133]	train-rmse:0.131671+0.009803	test-rmse:0.198594+0.008527 
[134]	train-rmse:0.131316+0.010159	test-rmse:0.198542+0.008555 
[135]	train-rmse:0.131059+0.010502	test-rmse:0.198488+0.008464 
[136]	train-rmse:0.130548+0.009904	test-rmse:0.198271+0.008414 
[137]	train-rmse:0.130173+0.010302	test-rmse:0.198175+0.008465 
[138]	train-rmse:0.129685+0.009804	test-rmse:0.198041+0.008442 
[139]	train-rmse:0.128848+0.009774	test-rmse:0.197777+0.008470 
[140]	train-rmse:0.128353+0.009447	test-rmse:0.197664+0.008468 
[141]	train-rmse:0.127780+0.008691	test-rmse:0.197368+0.008417 
[142]	train-rmse:0.127795+0.008689	test-rmse:0.197370+0.008417 
[143]	train-rmse:0.127392+0.008215	test-rmse:0.197232+0.008407 
[144]	train-rmse:0.127409+0.008212	test-rmse:0.197233+0.008405 
[145]	train-rmse:0.127424+0.008210	test-rmse:0.197236+0.008404 
[146]	train-rmse:0.126616+0.008237	test-rmse:0.197034+0.008331 
[147]	train-rmse:0.126631+0.008235	test-rmse:0.197034+0.008332 
[148]	train-rmse:0.125431+0.007921	test-rmse:0.196684+0.008460 
[149]	train-rmse:0.125150+0.008345	test-rmse:0.196640+0.008377 
[150]	train-rmse:0.124500+0.008394	test-rmse:0.196355+0.008448 
[151]	train-rmse:0.124515+0.008391	test-rmse:0.196357+0.008449 
[152]	train-rmse:0.124276+0.008749	test-rmse:0.196354+0.008439 
[153]	train-rmse:0.123453+0.008064	test-rmse:0.196150+0.008456 
[154]	train-rmse:0.123468+0.008063	test-rmse:0.196151+0.008457 
[155]	train-rmse:0.123171+0.008528	test-rmse:0.196172+0.008492 
[156]	train-rmse:0.123185+0.008527	test-rmse:0.196174+0.008490 
[157]	train-rmse:0.122431+0.007851	test-rmse:0.195999+0.008591 
[158]	train-rmse:0.122002+0.007488	test-rmse:0.195874+0.008585 
[159]	train-rmse:0.121721+0.007963	test-rmse:0.195935+0.008691 
[160]	train-rmse:0.121737+0.007961	test-rmse:0.195935+0.008690 
[161]	train-rmse:0.121047+0.007967	test-rmse:0.195825+0.008734 
[162]	train-rmse:0.120612+0.007585	test-rmse:0.195625+0.008978 
[163]	train-rmse:0.120626+0.007584	test-rmse:0.195626+0.008978 
[164]	train-rmse:0.120321+0.007865	test-rmse:0.195642+0.008973 
[165]	train-rmse:0.120025+0.008328	test-rmse:0.195622+0.008934 
[166]	train-rmse:0.119708+0.008104	test-rmse:0.195568+0.008945 
[167]	train-rmse:0.119090+0.008390	test-rmse:0.195576+0.009166 
[168]	train-rmse:0.119105+0.008389	test-rmse:0.195579+0.009166 
[169]	train-rmse:0.119119+0.008387	test-rmse:0.195580+0.009165 
[170]	train-rmse:0.119133+0.008386	test-rmse:0.195580+0.009165 
[171]	train-rmse:0.118380+0.007688	test-rmse:0.195341+0.009417 
[172]	train-rmse:0.117909+0.008241	test-rmse:0.195339+0.009523 
[173]	train-rmse:0.117650+0.008462	test-rmse:0.195310+0.009533 
[174]	train-rmse:0.117664+0.008461	test-rmse:0.195311+0.009531 
[175]	train-rmse:0.117679+0.008460	test-rmse:0.195313+0.009531 
[176]	train-rmse:0.116748+0.008780	test-rmse:0.195263+0.009711 
[177]	train-rmse:0.116012+0.008194	test-rmse:0.195029+0.009871 
[178]	train-rmse:0.115621+0.007893	test-rmse:0.194958+0.009958 
[179]	train-rmse:0.115070+0.007997	test-rmse:0.194898+0.009971 
[180]	train-rmse:0.114528+0.007968	test-rmse:0.194803+0.009980 
[181]	train-rmse:0.114541+0.007967	test-rmse:0.194804+0.009980 
[182]	train-rmse:0.114556+0.007967	test-rmse:0.194804+0.009978 
[183]	train-rmse:0.114571+0.007966	test-rmse:0.194807+0.009977 
[184]	train-rmse:0.113910+0.007503	test-rmse:0.194597+0.010159 
[185]	train-rmse:0.113925+0.007501	test-rmse:0.194600+0.010157 
[186]	train-rmse:0.113337+0.007134	test-rmse:0.194414+0.010287 
[187]	train-rmse:0.113351+0.007135	test-rmse:0.194414+0.010287 
[188]	train-rmse:0.113173+0.007335	test-rmse:0.194416+0.010286 
[189]	train-rmse:0.112944+0.007654	test-rmse:0.194448+0.010341 
[190]	train-rmse:0.112671+0.007319	test-rmse:0.194380+0.010335 
[191]	train-rmse:0.112107+0.006812	test-rmse:0.194312+0.010358 
[192]	train-rmse:0.111650+0.007370	test-rmse:0.194398+0.010395 
[193]	train-rmse:0.111090+0.007396	test-rmse:0.194340+0.010395 
[194]	train-rmse:0.110787+0.007196	test-rmse:0.194355+0.010379 
[195]	train-rmse:0.110801+0.007196	test-rmse:0.194356+0.010378 
[196]	train-rmse:0.110621+0.007424	test-rmse:0.194319+0.010389 
[197]	train-rmse:0.110635+0.007423	test-rmse:0.194320+0.010388 
[198]	train-rmse:0.110422+0.007704	test-rmse:0.194338+0.010381 
[199]	train-rmse:0.110196+0.008017	test-rmse:0.194269+0.010401 
[200]	train-rmse:0.109652+0.008103	test-rmse:0.194201+0.010455 
[201]	train-rmse:0.109416+0.008427	test-rmse:0.194170+0.010466 
[202]	train-rmse:0.108978+0.008445	test-rmse:0.193987+0.010524 
[203]	train-rmse:0.108990+0.008444	test-rmse:0.193986+0.010523 
[204]	train-rmse:0.108726+0.008268	test-rmse:0.193990+0.010516 
[205]	train-rmse:0.108555+0.008169	test-rmse:0.194029+0.010472 
[206]	train-rmse:0.108071+0.008212	test-rmse:0.193970+0.010491 
[207]	train-rmse:0.107863+0.008527	test-rmse:0.193993+0.010483 
[208]	train-rmse:0.107612+0.008215	test-rmse:0.193915+0.010479 
[209]	train-rmse:0.107331+0.007899	test-rmse:0.193912+0.010479 
[210]	train-rmse:0.107107+0.008117	test-rmse:0.193962+0.010574 
[211]	train-rmse:0.106856+0.008384	test-rmse:0.193927+0.010507 
[212]	train-rmse:0.106743+0.008558	test-rmse:0.193925+0.010507 
[213]	train-rmse:0.106514+0.008398	test-rmse:0.193928+0.010507 
[214]	train-rmse:0.106090+0.008512	test-rmse:0.193867+0.010531 
[215]	train-rmse:0.105829+0.008313	test-rmse:0.193846+0.010553 
[216]	train-rmse:0.105403+0.008427	test-rmse:0.193724+0.010574 
[217]	train-rmse:0.105007+0.008593	test-rmse:0.193720+0.010577 
[218]	train-rmse:0.104697+0.008376	test-rmse:0.193705+0.010592 
[219]	train-rmse:0.104494+0.008266	test-rmse:0.193678+0.010603 
[220]	train-rmse:0.104284+0.008608	test-rmse:0.193660+0.010607 
[221]	train-rmse:0.103847+0.008616	test-rmse:0.193761+0.010735 
[222]	train-rmse:0.103350+0.008172	test-rmse:0.193727+0.010776 
[223]	train-rmse:0.102983+0.008567	test-rmse:0.193668+0.010637 
[224]	train-rmse:0.102766+0.008462	test-rmse:0.193651+0.010656 
[225]	train-rmse:0.102585+0.008753	test-rmse:0.193653+0.010654 
[226]	train-rmse:0.102439+0.008995	test-rmse:0.193630+0.010660 
[227]	train-rmse:0.102282+0.009122	test-rmse:0.193619+0.010641 
[228]	train-rmse:0.102146+0.009347	test-rmse:0.193615+0.010642 
[229]	train-rmse:0.101925+0.009255	test-rmse:0.193603+0.010654 
[230]	train-rmse:0.101606+0.009596	test-rmse:0.193591+0.010672 
[231]	train-rmse:0.101325+0.009247	test-rmse:0.193544+0.010670 
[232]	train-rmse:0.101093+0.009159	test-rmse:0.193479+0.010744 
[233]	train-rmse:0.100896+0.009328	test-rmse:0.193527+0.010832 
[234]	train-rmse:0.100738+0.009477	test-rmse:0.193559+0.010891 
[235]	train-rmse:0.100535+0.009415	test-rmse:0.193463+0.011002 
[236]	train-rmse:0.100195+0.009506	test-rmse:0.193411+0.011022 
[237]	train-rmse:0.100207+0.009505	test-rmse:0.193411+0.011020 
[238]	train-rmse:0.099998+0.009810	test-rmse:0.193388+0.011027 
[239]	train-rmse:0.099851+0.009952	test-rmse:0.193391+0.011034 
[240]	train-rmse:0.099617+0.009749	test-rmse:0.193358+0.011046 
[241]	train-rmse:0.098960+0.009245	test-rmse:0.193314+0.011009 
[242]	train-rmse:0.098600+0.009435	test-rmse:0.193199+0.011098 
[243]	train-rmse:0.098000+0.009062	test-rmse:0.193175+0.011090 
[244]	train-rmse:0.098013+0.009060	test-rmse:0.193175+0.011089 
[245]	train-rmse:0.098025+0.009060	test-rmse:0.193174+0.011089 
[246]	train-rmse:0.097781+0.008742	test-rmse:0.193167+0.011087 
[247]	train-rmse:0.097794+0.008741	test-rmse:0.193167+0.011087 
[248]	train-rmse:0.097806+0.008741	test-rmse:0.193169+0.011086 
[249]	train-rmse:0.097818+0.008740	test-rmse:0.193169+0.011084 
[250]	train-rmse:0.097196+0.008487	test-rmse:0.193054+0.011245 
[251]	train-rmse:0.097208+0.008487	test-rmse:0.193053+0.011245 
[252]	train-rmse:0.096914+0.008834	test-rmse:0.193123+0.011270 
[253]	train-rmse:0.096927+0.008834	test-rmse:0.193124+0.011269 
[254]	train-rmse:0.096745+0.008620	test-rmse:0.193128+0.011268 
[255]	train-rmse:0.096632+0.008739	test-rmse:0.193112+0.011236 
[256]	train-rmse:0.096644+0.008738	test-rmse:0.193111+0.011234 
[257]	train-rmse:0.096416+0.008488	test-rmse:0.193127+0.011234 
[258]	train-rmse:0.096174+0.008226	test-rmse:0.193089+0.011248 
[259]	train-rmse:0.096185+0.008225	test-rmse:0.193087+0.011247 
[260]	train-rmse:0.095726+0.007749	test-rmse:0.193074+0.011247 
[261]	train-rmse:0.095397+0.007832	test-rmse:0.193048+0.011317 
[262]	train-rmse:0.095410+0.007831	test-rmse:0.193047+0.011317 
[263]	train-rmse:0.095194+0.007614	test-rmse:0.193034+0.011321 
[264]	train-rmse:0.095206+0.007613	test-rmse:0.193033+0.011320 
[265]	train-rmse:0.095218+0.007612	test-rmse:0.193032+0.011319 
[266]	train-rmse:0.094895+0.008004	test-rmse:0.193062+0.011347 
[267]	train-rmse:0.094906+0.008004	test-rmse:0.193060+0.011345 
[268]	train-rmse:0.094751+0.007860	test-rmse:0.193113+0.011324 
[269]	train-rmse:0.094029+0.007726	test-rmse:0.193134+0.011226 
[270]	train-rmse:0.094040+0.007725	test-rmse:0.193134+0.011225 
[271]	train-rmse:0.094051+0.007724	test-rmse:0.193133+0.011224 
[272]	train-rmse:0.093714+0.007690	test-rmse:0.193137+0.011262 
[273]	train-rmse:0.093575+0.007857	test-rmse:0.193139+0.011269 
[274]	train-rmse:0.093266+0.007723	test-rmse:0.193136+0.011237 
[275]	train-rmse:0.092904+0.007766	test-rmse:0.193103+0.011249 
[276]	train-rmse:0.092618+0.007836	test-rmse:0.193144+0.011211 
[277]	train-rmse:0.092272+0.007523	test-rmse:0.193078+0.011264 
[278]	train-rmse:0.092130+0.007729	test-rmse:0.193093+0.011260 
[279]	train-rmse:0.091995+0.007884	test-rmse:0.193048+0.011180 
[280]	train-rmse:0.091616+0.007630	test-rmse:0.193050+0.011191 
[281]	train-rmse:0.091628+0.007629	test-rmse:0.193049+0.011191 
[282]	train-rmse:0.091639+0.007629	test-rmse:0.193048+0.011191 
[283]	train-rmse:0.091480+0.007546	test-rmse:0.193020+0.011223 
[284]	train-rmse:0.091491+0.007545	test-rmse:0.193019+0.011223 
[285]	train-rmse:0.091292+0.007464	test-rmse:0.193077+0.011153 
[286]	train-rmse:0.091117+0.007411	test-rmse:0.193091+0.011134 
[287]	train-rmse:0.090954+0.007284	test-rmse:0.193028+0.011157 
[288]	train-rmse:0.090845+0.007125	test-rmse:0.192977+0.011156 
[289]	train-rmse:0.090483+0.007035	test-rmse:0.193007+0.011238 
[290]	train-rmse:0.090090+0.006723	test-rmse:0.193020+0.011247 
[291]	train-rmse:0.089754+0.006646	test-rmse:0.193038+0.011243 
[292]	train-rmse:0.089569+0.006509	test-rmse:0.193033+0.011243 
[293]	train-rmse:0.089411+0.006293	test-rmse:0.193089+0.011243 
[294]	train-rmse:0.089422+0.006293	test-rmse:0.193089+0.011243 
[295]	train-rmse:0.088923+0.006139	test-rmse:0.193179+0.011250 
[296]	train-rmse:0.088934+0.006139	test-rmse:0.193178+0.011250 
[297]	train-rmse:0.088945+0.006139	test-rmse:0.193178+0.011249 
[298]	train-rmse:0.088891+0.006107	test-rmse:0.193183+0.011240 
[299]	train-rmse:0.088902+0.006107	test-rmse:0.193182+0.011240 
[300]	train-rmse:0.088637+0.006081	test-rmse:0.193169+0.011240 
[301]	train-rmse:0.088518+0.005991	test-rmse:0.193178+0.011235 
[302]	train-rmse:0.088419+0.006113	test-rmse:0.193215+0.011303 
[303]	train-rmse:0.088429+0.006113	test-rmse:0.193214+0.011302 
[304]	train-rmse:0.088291+0.006040	test-rmse:0.193159+0.011367 
[305]	train-rmse:0.087859+0.005654	test-rmse:0.193075+0.011413 
[306]	train-rmse:0.087650+0.005410	test-rmse:0.193047+0.011412 
[307]	train-rmse:0.087347+0.005371	test-rmse:0.193069+0.011394 
[308]	train-rmse:0.087150+0.005148	test-rmse:0.193046+0.011393 
[309]	train-rmse:0.087038+0.005298	test-rmse:0.193040+0.011383 
[310]	train-rmse:0.086885+0.005213	test-rmse:0.193006+0.011422 
[311]	train-rmse:0.086759+0.005386	test-rmse:0.192939+0.011300 
[312]	train-rmse:0.086692+0.005472	test-rmse:0.192958+0.011295 
[313]	train-rmse:0.086420+0.005470	test-rmse:0.193004+0.011333 
[314]	train-rmse:0.086279+0.005674	test-rmse:0.193025+0.011372 
[315]	train-rmse:0.086115+0.005522	test-rmse:0.193015+0.011375 
[316]	train-rmse:0.086125+0.005521	test-rmse:0.193014+0.011374 
[317]	train-rmse:0.086059+0.005629	test-rmse:0.192995+0.011340 
[318]	train-rmse:0.085656+0.005291	test-rmse:0.193002+0.011339 
[319]	train-rmse:0.085450+0.005339	test-rmse:0.193019+0.011305 
[320]	train-rmse:0.085338+0.005279	test-rmse:0.193038+0.011279 
[321]	train-rmse:0.085179+0.005517	test-rmse:0.193043+0.011289 
[322]	train-rmse:0.085029+0.005357	test-rmse:0.193085+0.011289 
[323]	train-rmse:0.085038+0.005356	test-rmse:0.193084+0.011288 
[324]	train-rmse:0.084887+0.005215	test-rmse:0.193099+0.011281 
[325]	train-rmse:0.084587+0.005405	test-rmse:0.193141+0.011375 
[326]	train-rmse:0.084319+0.005470	test-rmse:0.193116+0.011338 
[327]	train-rmse:0.084161+0.005608	test-rmse:0.193107+0.011339 
[328]	train-rmse:0.083994+0.005425	test-rmse:0.193109+0.011338 
[329]	train-rmse:0.083586+0.005528	test-rmse:0.193171+0.011414 
[330]	train-rmse:0.083418+0.005425	test-rmse:0.193190+0.011391 
[331]	train-rmse:0.083312+0.005528	test-rmse:0.193182+0.011392 
[332]	train-rmse:0.083201+0.005419	test-rmse:0.193166+0.011392 
[333]	train-rmse:0.082934+0.005220	test-rmse:0.193126+0.011434 
[334]	train-rmse:0.082862+0.005158	test-rmse:0.193075+0.011432 
[335]	train-rmse:0.082872+0.005158	test-rmse:0.193075+0.011431 
[336]	train-rmse:0.082730+0.004999	test-rmse:0.193053+0.011439 
[337]	train-rmse:0.082591+0.004857	test-rmse:0.193059+0.011435 
[338]	train-rmse:0.082349+0.004882	test-rmse:0.193065+0.011477 
[339]	train-rmse:0.082085+0.004947	test-rmse:0.193136+0.011539 
[340]	train-rmse:0.081758+0.005081	test-rmse:0.193189+0.011506 
[341]	train-rmse:0.081524+0.005080	test-rmse:0.193254+0.011491 
[342]	train-rmse:0.081429+0.005165	test-rmse:0.193214+0.011499 
[343]	train-rmse:0.081439+0.005165	test-rmse:0.193213+0.011498 
[344]	train-rmse:0.081448+0.005165	test-rmse:0.193213+0.011498 
[345]	train-rmse:0.081291+0.005091	test-rmse:0.193244+0.011458 
[346]	train-rmse:0.081033+0.005236	test-rmse:0.193297+0.011469 
[347]	train-rmse:0.080828+0.005072	test-rmse:0.193291+0.011468 
[348]	train-rmse:0.080632+0.005019	test-rmse:0.193295+0.011461 
[349]	train-rmse:0.080434+0.005097	test-rmse:0.193272+0.011500 
[350]	train-rmse:0.080200+0.004934	test-rmse:0.193249+0.011502 
[351]	train-rmse:0.080098+0.005107	test-rmse:0.193199+0.011413 
[352]	train-rmse:0.079840+0.005075	test-rmse:0.193224+0.011335 
[353]	train-rmse:0.079849+0.005074	test-rmse:0.193223+0.011334 
[354]	train-rmse:0.079405+0.005116	test-rmse:0.193244+0.011340 
[355]	train-rmse:0.079414+0.005116	test-rmse:0.193243+0.011339 
[356]	train-rmse:0.079424+0.005116	test-rmse:0.193241+0.011339 
[357]	train-rmse:0.079275+0.004939	test-rmse:0.193202+0.011355 
[358]	train-rmse:0.079152+0.004838	test-rmse:0.193235+0.011355 
[359]	train-rmse:0.079039+0.004702	test-rmse:0.193193+0.011373 
[360]	train-rmse:0.078790+0.004491	test-rmse:0.193232+0.011375 
[361]	train-rmse:0.078711+0.004642	test-rmse:0.193239+0.011387 
Stopping. Best iteration:
[311]	train-rmse:0.086759+0.005386	test-rmse:0.192939+0.011300

> hgrid <-
+   dplyr::bind_cols(xgboost_params[index, ], dart.cv$evaluation_log[dart.cv$best_iteration, ], best_iter = dart.cv$best_iteration)
> 
> # write result
> fname <- file.path('data-raw', 'tune', year, 'array', paste0('tune_', index, '.tsv'))
> readr::write_tsv(hgrid, file = fname)
